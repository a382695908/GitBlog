---
title: 梯度算法之批量梯度下降，随机梯度下降和小批量梯度下降
date: 2017-12-14 14:40:43
tags: [梯度下降]
categories: 技术篇
---

在机器学习领域，体梯度下降算法分为三种

- 批量梯度下降算法（BGD，Batch gradient descent algorithm）
- 随机梯度下降算法（SGD，Stochastic gradient descent algorithm）
- 小批量梯度下降算法（MBGD，Mini-batch gradient descent algorithm）

<!--More-->

# 批量梯度下降算法
BGD是最原始的梯度下降算法，每一次迭代使用全部的样本，即权重的迭代公式中(公式中用$\theta$代替$\theta_i$)，
$$
\jmath (\theta _0,\theta _1,...,\theta _n)=\sum_{i=0}^{m}( h_\theta(x_0,x_1,...,x_n)-y_i )^2        
$$
$$
\theta _i = \theta _i - \alpha \frac{\partial \jmath (\theta _1,\theta _2,...,\theta _n)}{\partial \theta _i}
$$
$$
公式(1)
$$

这里的m代表所有的样本，表示从第一个样本遍历到最后一个样本。

特点：

- 能达到全局最优解，易于并行实现
- 当样本数目很多时，训练过程缓慢

# 随机梯度下降算法
SGD的思想是更新每一个参数时都使用一个样本来进行更新，即公式（1）中m为1。每次更新参数都只使用一个样本，进行多次更新。这样在样本量很大的情况下，可能只用到其中的一部分样本就能得到最优解了。
但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。

特点：
- 训练速度快
- 准确度下降，并不是最优解，不易于并行实现

# 小批量梯度下降算法
MBGD的算法思想就是在更新每一参数时都使用一部分样本来进行更新，也就是公式（1）中的m的值大于1小于所有样本的数量。

相对于随机梯度下降，Mini-batch梯度下降降低了收敛波动性，即降低了参数更新的方差，使得更新更加稳定。相对于批量梯度下降，其提高了每次学习的速度。并且其不用担心内存瓶颈从而可以利用矩阵运算进行高效计算。一般而言每次更新随机选择[50,256]个样本进行学习，但是也要根据具体问题而选择，实践中可以进行多次试验，选择一个更新速度与更次次数都较适合的样本数。mini-batch梯度下降可以保证收敛性，常用于神经网络中。


# 补充
在样本量较小的情况下，可以使用批量梯度下降算法，样本量较大的情况或者线上，可以使用随机梯度下降算法或者小批量梯度下降算法。

在机器学习中的无约束优化算法，除了梯度下降以外，还有前面提到的最小二乘法，此外还有牛顿法和拟牛顿法。

梯度下降法和最小二乘法相比，梯度下降法需要选择步长，而最小二乘法不需要。梯度下降法是迭代求解，最小二乘法是计算解析解。如果样本量不算很大，且存在解析解，最小二乘法比起梯度下降法要有优势，计算速度很快。但是如果样本量很大，用最小二乘法由于需要求一个超级大的逆矩阵，这时就很难或者很慢才能求解解析解了，使用迭代的梯度下降法比较有优势。

梯度下降法和牛顿法/拟牛顿法相比，两者都是迭代求解，不过梯度下降法是梯度求解，而牛顿法/拟牛顿法是用二阶的海森矩阵的逆矩阵或伪逆矩阵求解。相对而言，使用牛顿法/拟牛顿法收敛更快。但是每次迭代的时间比梯度下降法长。

# sklearn中的SGD
sklearn官网上查了一下，并没有找到BGD和MBGD的相关文档，只是看到可SGD的，感兴趣的可以直接去官网看英文文档，点击SGD查看：[SGD](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)，这也有一个中文的 [SGD](http://sklearn.lzjqsdd.com/modules/sgd.html)

```
In [1]: from sklearn.linear_model import SGDClassifier

In [2]: X = [[0., 0.], [1., 1.]]

In [3]: y = [0, 1]

In [4]: clf = SGDClassifier(loss="hinge", penalty="l2")

In [5]: clf.fit(X, y)
Out[5]: 
SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,
       eta0=0.0, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=None, shuffle=True,
       verbose=0, warm_start=False)

In [6]:  clf.predict([[2., 2.]])
Out[6]: array([1])

In [7]: clf.coef_ 
Out[7]: array([[ 9.91080278,  9.91080278]])

In [8]: clf.intercept_ 
Out[8]: array([-9.97004991])
```
参考：

- https://www.cnblogs.com/pinard/p/5970503.html
- http://blog.csdn.net/uestc_c2_403/article/details/74910107