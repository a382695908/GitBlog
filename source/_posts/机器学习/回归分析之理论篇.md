---
title: 回归分析之理论篇
date: 2017-9-17 08:10:27
tags: [回归分析,正态分布]
categories: 技术篇
---
2015年的机器学习博客其实都是看《机器学习实战》这本书时学到的，说实话当时也是知其然，不知其所以然，以至于对其理解不深刻，好多细节和理论知识都搞的是乱七八糟，自从工作之后再去看一个算法，思考的比之前多了点，查看资料也比之前多了点，生怕理解错误，影响其他人，当然在理解的程度上还是不够深刻，这也是一个学习的过程吧，记录一下，欢迎指正。

CSDN链接：[点击阅读](http://blog.csdn.net/gamer_gyt/article/details/78008144)
<!--More-->
# 一：一些名词定义

## 1）指数分布族
指数分布族是指可以表示为指数形式的概率分布。
$$
f_X(x\mid\theta) = h(x) \exp \left (\eta(\theta) \cdot T(x) -A(\theta)\right )
$$
其中，η为自然参数(nature parameter)，T(x)是充分统计量（sufficient statistic）。当参数A，h，T都固定以后，就定义了一个以η为参数的函数族。

伯努利分布与高斯分布是两个典型的指数分布族

### 伯努利分布
又名两点分布或者0-1分布，是一个离散型概率分布。假设1的概率为p，0的概率为q，则
其概率质量函数为：
```
{\displaystyle f_{X}(x)=p^{x}(1-p)^{1-x}=\left\{{\begin{matrix}p&{\mbox{if }}x=1,\\q\ &{\mbox{if }}x=0.\\\end{matrix}}\right.}
```
其期望值为：
$$
{\displaystyle \operatorname {E} [X]=\sum _{i=0}^{1}x_{i}f_{X}(x)=0+p=p}
$$

其方差为：
$$
{\displaystyle \operatorname {var} [X]=\sum _{i=0}^{1}(x_{i}-E[X])^{2}f_{X}(x)=(0-p)^{2}(1-p)+(1-p)^{2}p=p(1-p)=pq}
$$


### 正态分布(高斯分布)
若随机变量X服从一个位置参数为 ${\displaystyle \mu }$ 、尺度参数为 ${\displaystyle \sigma } $ 的概率分布，记为：
$$
X \sim N(\mu,\sigma^2),
$$

其概率密度函数为:
```
f(x) = {1 \over \sigma\sqrt{2\pi} }\,e^{- {{(x-\mu )^2 \over 2\sigma^2}}}
```

正态分布的数学期望值或期望值$ {\displaystyle \mu } $ 等于位置参数，决定了分布的位置；其方差 $ {\displaystyle \sigma ^{2}} $ 的开平方或标准差$ {\displaystyle \sigma }$ 等于尺度参数，决定了分布的幅度。

### 标准正态分布：

如果$ {\displaystyle \mu =0} $ 并且 $ {\displaystyle \sigma =1} $ 则这个正态分布称为标准正态分布。简化为：
```
f(x) = \frac{1}{\sqrt{2\pi}} \, \exp\left(-\frac{x^2}{2} \right)
```
如下图所示：

![image](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Normal_distribution_pdf.png/650px-Normal_distribution_pdf.png)

正态分布中一些值得注意的量：

- 密度函数关于平均值对称
- 平均值与它的众数（statistical mode）以及中位数（median）同一数值。
- 函数曲线下68.268949%的面积在平均数左右的一个标准差范围内。
- 95.449974%的面积在平均数左右两个标准差 $ {\displaystyle 2\sigma } $ 的范围内。
- 99.730020%的面积在平均数左右三个标准差$ {\displaystyle 3\sigma } $ 的范围内。
- 99.993666%的面积在平均数左右四个标准差$ {\displaystyle 4\sigma } $ 的范围内。
- 函数曲线的反曲点（inflection point）为离平均数一个标准差距离的位置。

## 2）多重共线性和完全共线性

多重共线性：指线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真或难以估计准确。一般来说，由于经济数据的限制使得模型设计不当，导致设计矩阵中解释变量间存在普遍的相关关系。通俗点理解就是自变量里边有一些是打酱油的，可以由另外一些变量推导出来，当变量中存在大量的多重共线性变量就会导致模型误差很大，这个时候就需要从自变量中将“打酱油”的变量给剔除掉。

完全共线性：在多元回归中，一个自变量是一个或多个其他自变量的线性函数。

两者在某种特殊情况下是有交集的。

## 3）T检验
T检验又叫student T 检验，主要用于样本含量小，总标准差 $\sigma$ 未知的正太分布数据。T检验是用于小样本的两个平均值差异程度的检查方法，他是用T分布理论值来推断事件发生的概率，从而判断两个平均数的差异是否显著。
参考: http://blog.csdn.net/shulixu/article/details/53354206



## 4）关系
- 函数关系
> 确定性关系，y=3+2x
- 相关关系
>非确定性关系，比如说高中时数学成绩好的人，一般物理成绩也好，这是因为它们背后使用的都是数学逻辑，这种酒叫做非确定性关系。

## 5）虚拟变量
定义：
>又称虚设变量、名义变量或哑变量，用以反映质的属性的一个人工变量，是量化了的自变量，通常取值为0或1。（通常为离散变量，因子变量）

作用：
>引入哑变量可使线形回归模型变得更复杂，但对问题描述更简明，一个方程能达到两个方程的作用，而且接近现实。

设置：
>例如：体重（w）和身高（h），性别（s）的关系，但这里性别并非连续的或者数字可以表示的变量，你并不能拿 1表示男，2表示女，这里的性别是离散变量，只能为男或者女，所以这里就需要引入哑变量来处理。
性别（s） =》 isman（男1，非男0），iswoman （因为只有两种可能，所以这里只需要引入一个哑变量即可），同理假设这里有另外一个变量肤色（有黑，白，黄三种可能），那么这里只需引入两个哑变量即可（isblack，iswhite），因为不是这两种的话那肯定是黄色皮肤了。

例子：
针对上边所说的体重和身高，性别的关系。

构建模型：
- 1）加法模型
```
w = a + b * h + c * isman
```
针对数据样本而言，性别是确定的，所以 c * isman 的结果不是c就是0，所以在加法模型下，影响的是模型在y轴上的截距。这说明的是针对不同的性别而言，回归方程是平衡的，只不过是截距不一样。

- 2）乘法模型
```
w = a + b * h + c * isman * h + d * iswoman * h
```
同样针对数据样本而言，性别也是确定的，假设一个男性，isman 为1，iswoman 为0，则上述模型变成了 w = a + b*h + c * h =a + (b+c) * h，这个时候就是在y轴上的截距一样，而斜率不一致。

- 3）混合模型
```
w = a + b * h + c * isman + d * iswoman + e * isman * h + f * iswoman * h
```
假设一个针对一个性别为男的样本数据，该模型变可以变成 w = a + b*h + c + e * h = a +c + (b+e)*h，这个时候斜率和截距都是不一样的。

# 二：什么是回归（分析）
回归就是利用样本（已知数据），产生拟合方程，从而（对未知数据）进行预测。比如说我有一组随机变量X（X1，X2，X3...）和另外一组随机变量Y（Y1，Y2，Y3...）,那么研究变量X与Y之间的统计学方法就叫做回归分析。当然这里X和Y是单一对应的，所以这里是一元线性回归。

回归分为线性回归和非线性回归，其中一些非线性回归可以用线性回归的方法来进行分析的叫做==广义线性回归==，接下来我们来了解下每一种回归：

## 1）线性回归
线性回归可以分为一元线性回归和多元线性回归。当然线性回归中自变量的指数都是1，这里的线性并非真的是指用一条线将数据连起来，也可以是一个二维平面，三维平面等。

一元线性回归：自变量只有一个的回归，比如说北京二环的房子面积（Area）和房子总价（Money）的关系，随着面积（Area）的增大，房屋价格也是不断增长。这里的自变量只有面积，所以这里是一元线性回归。

多元线性回归：自变量大于等于两个，比如说北京二环的房子面积（Area），楼层（floor）和房屋价格（Money）的关系，这里自变量是两个，所以是二元线性回归，三元，多元同理。

## 2）非线性回归
有一类模型，其回归参数不是线性的，也不能通过转换的方法将其变为线性的参数，这类模型称为非线性回归模型。非线性回归可以分为一元回归和多元回归。非线性回归中至少有一个自变量的指数不为1。回归分析中，当研究的因果关系只涉及因变量和一个自变量时，叫做一元回归分析；当研究的因果关系涉及因变量和两个或两个以上自变量时，叫做多元回归分析。

## 3）广义线性回归
一些非线性回归可以用线性回归的方法来进行分析叫做广义线性回归。
典型的代表是Logistic回归。

## 4）如何衡量相关关系既判断适不适合使用线性回归模型？
使用相关系数（-1，1），绝对值越接近于1，相关系数越高，越适合使用线性回归模型（Rxy>0,代表正相关，Rxy<0,代表负相关）

$$
r_{XY} = \frac{ \sum (X_{i}-\bar{X})(Y_{i}-\bar{Y}) }{ \sqrt{ \sum (X_{i}-\bar{X})^2) \sum (Y_{i}-\bar{Y})^2) } }
$$

# 三：回归中困难点
## 1）选定变量
> 假设自变量特别多，有一些是和因变量相关的，有一些是和因变量不相关的，这里我们就需要筛选出有用的变量，如果筛选后变量还特别多的话，可以采用降维的方式进行变量缩减（可以参考之前的PCA降维的文章：http://blog.csdn.net/gamer_gyt/article/details/51418069 ，基本是整理《机器学习实战》这本书的笔记）

## 2）发现多重共线性
(1).方差扩大因子法( VIF)

>一般认为如果最大的VIF超过10，常常表示存在多重共线性。

(2).容差容忍定法

>如果容差（tolerance）<=0.1，常常表示存在多重共线性。

(3). 条件索引

>条件索引(condition index)>10，可以说明存在比较严重的共线性



## 3）过拟合与欠拟合问题
过拟合和欠拟合其实对每一个模型来讲都是存在的，过拟合就是模型过于符合训练数据的趋势，欠拟合就是模型对于训练数据和测试数据都表现出不好的情况。针对于欠拟合来讲，是很容易发现的，通常不被讨论。

在进行模型训练的时候，算法要进行不断的学习，模型在训练数据和测试数据上的错误都在不断下降，但是，如果学习的时间过长的话，模型在训练数据集上的表现将会继续下降，这是因为模型已经过拟合，并且学习到了训练数据集中不恰当的细节和噪音，同时，测试集上的错误率开始上升，也是模型泛化能力在下降。

这个完美的临界点就在于测试集中的错误率在上升时，此时训练集和测试集上都有良好的表现。通常有两种手段可以帮助你找到这个完美的临界点：重采样方法和验证集方法。

### 如何限制过拟合？
> 过拟合和欠拟合可以导致很差的模型表现。但是到目前为止大部分机器学习实际应用时的问题都是过拟合。
过拟合是个问题因为训练数据上的机器学习算法的评价方法与我们最关心的实际上的评价方法，也就是算法在位置数据上的表现是不一样的。
当评价机器学习算法时我们有两者重要的技巧来限制过拟合
使用重采样来评价模型效能
保留一个验证数据集
最流行的重采样技术是k折交叉验证。指的是在训练数据的子集上训练和测试模型k次，同时建立对于机器学习模型在未知数据上表现的评估。
验证集只是训练数据的子集，你把它保留到你进行机器学习算法的最后才使用。在训练数据上选择和调谐机器学习算法之后，我们在验证集上在对于模型进行评估，以便得到一些关于模型在未知数据上的表现的认知。

## 4）检验模型是否合理
验证目前主要采用如下三类办法：
1、拟合优度检验
主要有R^2，t检验，f检验等等
这三种检验为常规验证，只要在95%的置信度内满足即可说明拟合效果良好。
2、预测值和真实值比较
主要是差值和比值，一般差值和比值都不超过5%。
3、另外的办法
GEH方法最为常用。GEH是Geoffrey E. Havers于1970年左右提出的一种模型验证方法，其巧妙的运用一个拟定的公式和标准界定模型的拟合优劣。
GEH=(2(M-C)^2/(M+C))^(1/2)
其中M是预测值，C是实际观测值
如果GEH小于5，认为模型拟合效果良好，如果GEH在5-10之间，必须对数据不可靠需要进行检查，如果GEH大于10，说明数据存在问题的几率很高。
http://blog.sina.com.cn/s/blog_66188c300100hl45.html

## 5）线性回归的模型评判
- 误差平方和（残差平方和）

例如二维平面上的一点（x1，y1），经过线性回归模型预测其值为 y_1，那么预测模型的好与坏就是计算预测结果到直线的距离的大小，由于是一组数据，那么便是这一组数据的和。

点到直线的距离公式为： 
$$
 \frac{\left | A_{x_{0}}+B_{y_{0}} +C \right |}{\sqrt{A^2 + B^2 }}
$$
由于涉及到开方，在计算过程中十分不方便，所以这里转换为纵轴上的差值，即利用预测值与真实值的差进行累加求和，最小时即为最佳的线性回归模型，但是这里涉及到预测值与真实值的差可能为负数，所以这里用平方，所以最终的误差平方和为：
$$
RSS = \sum_{i=1}^{n}(y_{i}- \hat{y_{i}} )^2 = \sum_{i=1}^{n}[y_{i} - (\alpha +\beta x_{i})]^2
$$

- AIC准则（赤池信息准则）
$$
AIC=n ln (RSSp/n)+2p
$$
n为变量总个数，p为选出的变量个数，AIC越小越好

