{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/yilia/source/main.0cf68a.css","path":"main.0cf68a.css","modified":1,"renderable":1},{"_id":"themes/yilia/source/slider.e37972.js","path":"slider.e37972.js","modified":1,"renderable":1},{"_id":"themes/yilia/source/main.0cf68a.js","path":"main.0cf68a.js","modified":1,"renderable":1},{"_id":"themes/yilia/source/mobile.992cbe.js","path":"mobile.992cbe.js","modified":1,"renderable":1},{"_id":"source/assets/img/favicon.ico","path":"assets/img/favicon.ico","modified":1,"renderable":0},{"_id":"source/assets/img/gongzhonghao.jpg","path":"assets/img/gongzhonghao.jpg","modified":1,"renderable":0},{"_id":"source/assets/img/gzh.png","path":"assets/img/gzh.png","modified":1,"renderable":0},{"_id":"source/assets/img/head.jpg","path":"assets/img/head.jpg","modified":1,"renderable":0},{"_id":"themes/yilia/source/fonts/default-skin.b257fa.svg","path":"fonts/default-skin.b257fa.svg","modified":1,"renderable":1},{"_id":"themes/yilia/source/fonts/iconfont.16acc2.ttf","path":"fonts/iconfont.16acc2.ttf","modified":1,"renderable":1},{"_id":"themes/yilia/source/fonts/iconfont.45d7ee.svg","path":"fonts/iconfont.45d7ee.svg","modified":1,"renderable":1},{"_id":"themes/yilia/source/fonts/iconfont.8c627f.woff","path":"fonts/iconfont.8c627f.woff","modified":1,"renderable":1},{"_id":"themes/yilia/source/fonts/iconfont.b322fa.eot","path":"fonts/iconfont.b322fa.eot","modified":1,"renderable":1},{"_id":"themes/yilia/source/fonts/tooltip.4004ff.svg","path":"fonts/tooltip.4004ff.svg","modified":1,"renderable":1},{"_id":"themes/yilia/source/img/default-skin.png","path":"img/default-skin.png","modified":1,"renderable":1},{"_id":"themes/yilia/source/img/preloader.gif","path":"img/preloader.gif","modified":1,"renderable":1},{"_id":"themes/yilia/source/img/scrollbar_arrow.png","path":"img/scrollbar_arrow.png","modified":1,"renderable":1},{"_id":"source/assets/img/myqq.png","path":"assets/img/myqq.png","modified":1,"renderable":0},{"_id":"source/assets/img/weixin.jpeg","path":"assets/img/weixin.jpeg","modified":1,"renderable":0},{"_id":"source/assets/img/zhifubao.jpeg","path":"assets/img/zhifubao.jpeg","modified":1,"renderable":0},{"_id":"source/assets/img/myweixin.png","path":"assets/img/myweixin.png","modified":1,"renderable":0}],"Cache":[{"_id":"source/.DS_Store","hash":"66a6b63bccdb842047284d0e6c6fa9d9cadbc5bf","modified":1513652096787},{"_id":"themes/yilia/.babelrc","hash":"b1b76475ac17dc9e2fa50af96c9e31eea2d0f2b4","modified":1513652153363},{"_id":"themes/yilia/.editorconfig","hash":"da6d022b8f4d9c961e2f8f80677e92af8de0db4d","modified":1513652151539},{"_id":"themes/yilia/.eslintignore","hash":"df0a50b13cc00acb749226fee3cee6e0351fb1d9","modified":1513652149203},{"_id":"themes/yilia/.eslintrc.js","hash":"5696ae049de010ed3786768b0c359f14c05b5ec6","modified":1513652149251},{"_id":"themes/yilia/.gitattributes","hash":"e0f24dceeb1e6878a1dd9b01a2b9df1bc037a867","modified":1513652149159},{"_id":"themes/yilia/_config.yml","hash":"3f069f481b19e00ac70c78465596c75623e79454","modified":1513652415519},{"_id":"themes/yilia/package.json","hash":"367cb9579d35968a942c243ab248a5f5ebfaf462","modified":1513652153315},{"_id":"themes/yilia/webpack.config.js","hash":"05ba46a4ae744272f5312e684928910dccad3755","modified":1513652149295},{"_id":"source/photos/lazyload.min.js","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1513652096795},{"_id":"source/_posts/.DS_Store","hash":"f66001ae55987bdd3f940de36a95c44e86947d91","modified":1513652096787},{"_id":"source/assets/.DS_Store","hash":"8983992a301632f377142eb0de9b7125c7a7c97a","modified":1513652096791},{"_id":"source/photos/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1513652096795},{"_id":"source/photos/index.ejs","hash":"5f7ef5b9f701dff1dab353fcbbdaaa0e4989cb5d","modified":1513652096795},{"_id":"source/photos/ins.css","hash":"8c20f68a514bb8ff9a04207cf2f7d245710973c9","modified":1513652096795},{"_id":"source/photos/ins.js","hash":"30d1de36678c7b1eff68c8e37534cd34f91dd235","modified":1513652096795},{"_id":"source/photos/ins.json","hash":"9e35716e6ea22178abc95287afebaf2b2a16a840","modified":1513652096795},{"_id":"themes/yilia/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1513652149495},{"_id":"themes/yilia/.git/config","hash":"45dbf7cad960774e3ebd63f878005463a78e4c69","modified":1513652150391},{"_id":"themes/yilia/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1513652150811},{"_id":"themes/yilia/.git/index","hash":"c4e20173207a4ec1af69ee9bec34165220375b7e","modified":1513652185087},{"_id":"themes/yilia/.git/packed-refs","hash":"83644c3638dafa38c817265c9207f098dd8aeee6","modified":1513652149551},{"_id":"themes/yilia/languages/default.yml","hash":"3083f319b352d21d80fc5e20113ddf27889c9d11","modified":1513652157471},{"_id":"themes/yilia/languages/fr.yml","hash":"84ab164b37c6abf625473e9a0c18f6f815dd5fd9","modified":1513652157591},{"_id":"themes/yilia/languages/nl.yml","hash":"12ed59faba1fc4e8cdd1d42ab55ef518dde8039c","modified":1513652157671},{"_id":"themes/yilia/languages/no.yml","hash":"965a171e70347215ec726952e63f5b47930931ef","modified":1513652157719},{"_id":"themes/yilia/languages/ru.yml","hash":"4fda301bbd8b39f2c714e2c934eccc4b27c0a2b0","modified":1513652157247},{"_id":"themes/yilia/languages/zh-CN.yml","hash":"ca40697097ab0b3672a80b455d3f4081292d1eed","modified":1513652157047},{"_id":"themes/yilia/languages/zh-tw.yml","hash":"53ce3000c5f767759c7d2c4efcaa9049788599c3","modified":1513652156847},{"_id":"themes/yilia/layout/archive.ejs","hash":"2703b07cc8ac64ae46d1d263f4653013c7e1666b","modified":1513652153271},{"_id":"themes/yilia/layout/category.ejs","hash":"765426a9c8236828dc34759e604cc2c52292835a","modified":1513652153091},{"_id":"themes/yilia/layout/index.ejs","hash":"a35dc900203f9d8dd863ea4c1722198d6d457ec8","modified":1513652153179},{"_id":"themes/yilia/layout/layout.ejs","hash":"0a332bdbd3b86c231d690614687f5b97186b85d5","modified":1513652151631},{"_id":"themes/yilia/layout/page.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1513652153139},{"_id":"themes/yilia/layout/post.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1513652153227},{"_id":"themes/yilia/layout/tag.ejs","hash":"eaa7b4ccb2ca7befb90142e4e68995fb1ea68b2e","modified":1513652151583},{"_id":"themes/yilia/source/main.0cf68a.css","hash":"ddf6e2c6b953c2c59a3c271e6070010a4cc81cf9","modified":1513652151395},{"_id":"themes/yilia/source/slider.e37972.js","hash":"6dec4e220c89049037eebc44404abd8455d22ad7","modified":1513652151483},{"_id":"themes/yilia/source-src/css.ejs","hash":"94dbdb02ca11849e415d54fb28546a598f2cffb1","modified":1513652156483},{"_id":"themes/yilia/source-src/script.ejs","hash":"c21381e1317db7bb157f1d182b8c088cb7cba411","modified":1513652156715},{"_id":"themes/yilia/layout/_partial/toc.ejs","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1513652151863},{"_id":"themes/yilia/source/main.0cf68a.js","hash":"993fadeb5f6d296e9d997a49ee20dc97333ceab7","modified":1513652151351},{"_id":"themes/yilia/source/mobile.992cbe.js","hash":"01b35e71e37aa2849664eb5daf26daede2278398","modified":1513652151439},{"_id":"source/_posts/ELK/Elasticsearch-DSL部分集合.md","hash":"cc52f8936c83da24d1011a6dae7afe13b1952b83","modified":1513652096787},{"_id":"source/_posts/ELK/异常检测之指数平滑（利用elasticsearch来实现）.md","hash":"8f6b4f2451b4f13385c000da3b5183f26c35daf8","modified":1513652096787},{"_id":"source/_posts/夏未眠/这夏未眠-序.md","hash":"76399c1c260ecf3466e59e6172e6ef8e12ea4ab4","modified":1513652096787},{"_id":"source/_posts/夏未眠/这夏未眠-简介.md","hash":"3c0732081ffe855e6b8d491efc6c1f4e710079aa","modified":1513652096787},{"_id":"source/_posts/数据结构/数据结构算法之二叉树.md","hash":"0f3d6cb8e5a85069396a852bb63da3b2533372fd","modified":1513652096787},{"_id":"source/_posts/数据结构/数据结构算法之合并两个有序序列.md","hash":"1d5418f85135c819cdd25d23385616aaa3bab6fb","modified":1513652096787},{"_id":"source/_posts/数据结构/数据结构算法之排序.md","hash":"929e9b647ea08d3ba363e5547b61bdae2353fff6","modified":1513652096787},{"_id":"source/_posts/机器学习/MachingLearning中的距离和相似性计算以及python实现.md","hash":"6e086a73f81ed1115348043e78397c4d835a602d","modified":1513652096787},{"_id":"source/_posts/数据结构/数据结构算法之链表.md","hash":"31c629371f4e37bae08c26f15e8265fcf3ccfa7c","modified":1513652096787},{"_id":"source/_posts/机器学习/几种距离计算公式在数据挖掘中的应用场景分析.md","hash":"2386e74dde5bf09c11a3d9508af6fc4daf1033b6","modified":1513652096787},{"_id":"source/_posts/机器学习/回归分析之Sklearn实现电力预测.md","hash":"eb8347d04115511378f2635d443c4ec663bfa70e","modified":1513652096787},{"_id":"source/_posts/机器学习/回归分析之理论篇.md","hash":"53051dca3210ca9a3464da0d85b903e2af85732b","modified":1513652096787},{"_id":"source/_posts/机器学习/回归分析之线性回归（N元线性回归）.md","hash":"a4d3cb102058c359fc55eff4a22e3058f1b628c8","modified":1513652096787},{"_id":"source/_posts/机器学习/数据归一化和其在sklearn中的处理.md","hash":"e043191d50e5a86209897176f617e1c0498b5453","modified":1513652096787},{"_id":"source/_posts/机器学习/梯度算法之批量梯度下降，随机梯度下降和小批量梯度下降.md","hash":"d53ed383f5fddb71abfdaba694dda1be41045f0e","modified":1513652096787},{"_id":"source/_posts/机器学习/梯度算法之梯度上升和梯度下降.md","hash":"de751fb0df83b154ec4f88fc8229e42627509a5a","modified":1513652096791},{"_id":"source/_posts/随手记/Hexo-Yilia加入相册功能.md","hash":"856c477e7bc81ebb3dc65c0a726b05efb866d96d","modified":1513652096791},{"_id":"source/_posts/随手记/一切的闹闹哄哄，只是他在水帘洞躲避风沙那晚做的一个梦.md","hash":"eb726f0b85b19de4da789dfdb312550af39d16d3","modified":1513652096791},{"_id":"source/_posts/随手记/别了青春与流年，遇见下一个自己.md","hash":"f56b66b4de4ac1d15de0e6a632da299f2b1acb79","modified":1513652096791},{"_id":"source/assets/img/favicon.ico","hash":"292808ea6ec3f69bc01f10697fed6567bafed8f9","modified":1513652096791},{"_id":"source/assets/img/gongzhonghao.jpg","hash":"c973bbc42f022c18baa54432458093849fe6699a","modified":1513652096791},{"_id":"source/assets/img/gzh.png","hash":"41c9bc4e3009faee1a8404398c45eeca8b9508ca","modified":1513652096791},{"_id":"source/assets/img/head.jpg","hash":"22e476c6393281457ec0d2f39fd13a5ebffb21b2","modified":1513652096791},{"_id":"themes/yilia/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1513652149891},{"_id":"themes/yilia/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1513652149611},{"_id":"themes/yilia/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1513652150119},{"_id":"themes/yilia/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1513652149743},{"_id":"themes/yilia/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1513652149695},{"_id":"themes/yilia/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1513652149787},{"_id":"themes/yilia/.git/hooks/pre-rebase.sample","hash":"18be3eb275c1decd3614e139f5a311b75f1b0ab8","modified":1513652149651},{"_id":"themes/yilia/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1513652149935},{"_id":"themes/yilia/.git/hooks/prepare-commit-msg.sample","hash":"2b6275eda365cad50d167fe3a387c9bc9fedd54f","modified":1513652149987},{"_id":"themes/yilia/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1513652149831},{"_id":"themes/yilia/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1513652149339},{"_id":"themes/yilia/.git/logs/HEAD","hash":"72c97891247c040ebc4d93ddef86880c00f27185","modified":1513652150351},{"_id":"themes/yilia/layout/_partial/after-footer.ejs","hash":"b86b248720ad415ec1b5fee53fb583776c776f83","modified":1513652152319},{"_id":"themes/yilia/layout/_partial/archive-post.ejs","hash":"1f7d4819b7f67602c4a1b99871808d2160b60978","modified":1513652151999},{"_id":"themes/yilia/layout/_partial/archive.ejs","hash":"a6e94061ac55b9eb55275f87b608d62f6ea35659","modified":1513652153047},{"_id":"themes/yilia/layout/_partial/article.ejs","hash":"630c6ec866d056657d3d91e34b4c64eb993c0654","modified":1513652152375},{"_id":"themes/yilia/layout/_partial/aside.ejs","hash":"8edbd7993b9b061611a193533a664e2e85eae748","modified":1513652152223},{"_id":"themes/yilia/layout/_partial/baidu-analytics.ejs","hash":"f0e6e88f9f7eb08b8fe51449a8a3016273507924","modified":1513652151807},{"_id":"themes/yilia/layout/_partial/css.ejs","hash":"236f8a377b2e4e35754319c3029bcd4a4115431d","modified":1513652152175},{"_id":"themes/yilia/layout/_partial/footer.ejs","hash":"84630e328bc40cec4301cc57a7534c1a43a9e147","modified":1513652698943},{"_id":"themes/yilia/layout/_partial/google-analytics.ejs","hash":"f921e7f9223d7c95165e0f835f353b2938e40c45","modified":1513652151763},{"_id":"themes/yilia/layout/_partial/head.ejs","hash":"64f092186b5a744aa1603ce22bb1d44a34446add","modified":1513652152043},{"_id":"themes/yilia/layout/_partial/header.ejs","hash":"6387a93dad7c3d778eb91e3821852fbf6813880c","modified":1513652151719},{"_id":"themes/yilia/layout/_partial/left-col.ejs","hash":"183d7ca4ba8e7c80694ffdc8cf39957092238346","modified":1513652151675},{"_id":"themes/yilia/layout/_partial/mathjax.ejs","hash":"151a1ef2173ba7b6789d349f0f8da89616cc1394","modified":1513652151939},{"_id":"themes/yilia/layout/_partial/mobile-nav.ejs","hash":"7fbbfabf5e29525b24ada14613c21a26789132b4","modified":1513652153003},{"_id":"themes/yilia/layout/_partial/tools.ejs","hash":"c41341b9618e591538e1136a2d1637587c1bbd90","modified":1513652152087},{"_id":"themes/yilia/layout/_partial/viewer.ejs","hash":"e495790b2abe2290875817e42bd505bc611d3e26","modified":1513652151883},{"_id":"themes/yilia/source/fonts/default-skin.b257fa.svg","hash":"2ac727c9e092331d35cce95af209ccfac6d4c7c7","modified":1513652151015},{"_id":"themes/yilia/source/fonts/iconfont.16acc2.ttf","hash":"f342ac8bf4d937f42a7d6a0032ad267ab47eb7f2","modified":1513652151127},{"_id":"themes/yilia/source/fonts/iconfont.45d7ee.svg","hash":"f9304e5714d20861be7d8f4d36687e88e86b9e1b","modified":1513652151083},{"_id":"themes/yilia/source/fonts/iconfont.8c627f.woff","hash":"aa9672cb097f7fd73ae5a03bcd3d9d726935bc0a","modified":1513652151291},{"_id":"themes/yilia/source/fonts/iconfont.b322fa.eot","hash":"bc8c5e88f4994a852041b4d83f126d9c4d419b4a","modified":1513652151239},{"_id":"themes/yilia/source/fonts/tooltip.4004ff.svg","hash":"397fe4b1093bf9b62457dac48aa15dac06b54a3c","modified":1513652151179},{"_id":"themes/yilia/source/img/default-skin.png","hash":"ed95a8e40a2c3478c5915376acb8e5f33677f24d","modified":1513652150971},{"_id":"themes/yilia/source/img/preloader.gif","hash":"6342367c93c82da1b9c620e97c84a389cc43d96d","modified":1513652150879},{"_id":"themes/yilia/source/img/scrollbar_arrow.png","hash":"d64a33c4ddfbdb89deeb6f4e3d36eb84dc4777c0","modified":1513652150927},{"_id":"themes/yilia/source-src/css/_core.scss","hash":"24f347a2412abbf58318369152504da9538f8d3b","modified":1513652155471},{"_id":"themes/yilia/source-src/css/_function.scss","hash":"93a50dd19a93485712da1f8d0a1672482dd1eabc","modified":1513652155659},{"_id":"themes/yilia/source-src/css/archive.scss","hash":"7d27e22ac898e8fafec14549e940c73cbea1fba8","modified":1513652154011},{"_id":"themes/yilia/source-src/css/article-inner.scss","hash":"d79f2d35a06de83a2a226ca790b7a0a34789c115","modified":1513652154051},{"_id":"themes/yilia/source-src/css/article-main.scss","hash":"3fad68bd74260326f83090b0974dd80707e7bac7","modified":1513652154095},{"_id":"themes/yilia/source-src/css/article-nav.scss","hash":"43e507f2a48504079afd9471353337e23ca47470","modified":1513652154711},{"_id":"themes/yilia/source-src/css/article.scss","hash":"0f6d61af99ed4db87f8589db1feaea7747b55963","modified":1513652154367},{"_id":"themes/yilia/source-src/css/aside.scss","hash":"578a67464dd0f542197f7fcee158c991db058563","modified":1513652155115},{"_id":"themes/yilia/source-src/css/comment.scss","hash":"cafe3834017a3bf47420f37543725025225a2c89","modified":1513652155427},{"_id":"themes/yilia/source-src/css/fonts.scss","hash":"97b8fba41c914145710b90091f400b845879577f","modified":1513652155159},{"_id":"themes/yilia/source-src/css/footer.scss","hash":"7c995410b25baaf61dfc5e148e22ca60330abcd3","modified":1513652155383},{"_id":"themes/yilia/source-src/css/global.scss","hash":"b4cb4f45a55d4250cd9056f76dab2a3c0dabcec4","modified":1513652154143},{"_id":"themes/yilia/source-src/css/grid.scss","hash":"849a29fcd7150214fcf7b9715fa5dc71d1f9b896","modified":1513652155251},{"_id":"themes/yilia/source-src/css/highlight.scss","hash":"3719994c2c9393813cc1d42b657205c368a329cb","modified":1513652154635},{"_id":"themes/yilia/source-src/css/left.scss","hash":"0d30c0e7cdb831c3881a017006c782f2214ac195","modified":1513652155787},{"_id":"themes/yilia/source-src/css/main.scss","hash":"2f86a014af93583caba78a563d9549826bf28294","modified":1513652154231},{"_id":"themes/yilia/source-src/css/mobile-slider.scss","hash":"f053c609d84df0dd9eee1d11ddf0c19163a456be","modified":1513652154275},{"_id":"themes/yilia/source-src/css/mobile.scss","hash":"ace041d72f95b419f6a5e443191703c2b62007f4","modified":1513652155339},{"_id":"themes/yilia/source-src/css/page.scss","hash":"bf206bb7f7d0967bc8b7fdf01b7ffc99aff9ba88","modified":1513652155291},{"_id":"themes/yilia/source-src/css/reward.scss","hash":"80a4fcf9171d4a33235da96ac8a2b7dcabc45dfb","modified":1513652155899},{"_id":"themes/yilia/source-src/css/scroll.scss","hash":"9c8dfd1c76854ef063494ca76fac6360b391ed6d","modified":1513652154323},{"_id":"themes/yilia/source-src/css/share.scss","hash":"150c6425f6582e7ec78a873256ce49c9930e8805","modified":1513652155203},{"_id":"themes/yilia/source-src/css/social.scss","hash":"724162ccf3977e70a45d189abfaa20b6e2fba87b","modified":1513652155591},{"_id":"themes/yilia/source-src/css/tags-cloud.scss","hash":"c8aa84fca93862d3caae77c552873b8610f33327","modified":1513652154191},{"_id":"themes/yilia/source-src/css/tags.scss","hash":"ac67a3c7097849206244db9b0ba91daaba017ef5","modified":1513652155519},{"_id":"themes/yilia/source-src/css/tools.scss","hash":"1b1aa0908e58cf942b28e3881d07c5573c4129e1","modified":1513652156003},{"_id":"themes/yilia/source-src/css/tooltip.scss","hash":"53d5a554bc2f38e9bb3d26400a47767013c05216","modified":1513652156339},{"_id":"themes/yilia/source-src/js/anm.js","hash":"4a4c5d82b09a3063f91a434388e6aa064fd7fd98","modified":1513652153403},{"_id":"themes/yilia/source-src/js/Q.js","hash":"d011af172064b6c6e0c7051d8f9879373ddac113","modified":1513652153619},{"_id":"themes/yilia/source-src/js/aside.js","hash":"754f771264548a6c5a8ad842908e59ae4e7ed099","modified":1513652153963},{"_id":"themes/yilia/source-src/js/browser.js","hash":"04095b38cfd4316a23f8eb14b1ffaf95f78a4260","modified":1513652153863},{"_id":"themes/yilia/source-src/js/fix.js","hash":"d6782d53c992e712af39c84e804eccaf38830b94","modified":1513652153663},{"_id":"themes/yilia/source-src/js/main.js","hash":"3894e60827c817319e43c9ff3ed045fc3d7336ce","modified":1513652153515},{"_id":"themes/yilia/source-src/js/mobile.js","hash":"4d823b039fd296d24a454eae5a798b93c44560cb","modified":1513652153919},{"_id":"themes/yilia/source-src/js/report.js","hash":"4f1d9a18a936ce40b037f636a39127dd19175b6e","modified":1513652153707},{"_id":"themes/yilia/source-src/js/share.js","hash":"b090f82cf80cba7da764753906d9e2cc2acdf30d","modified":1513652153763},{"_id":"themes/yilia/source-src/js/slider.js","hash":"e846bcc5aac9c68b93f7b8de353df54d8d29f666","modified":1513652153571},{"_id":"themes/yilia/source-src/js/util.js","hash":"8456e9d6b19532742582c99b2fb9d09e146e1c58","modified":1513652153463},{"_id":"themes/yilia/source-src/js/viewer.js","hash":"2577deb6a9fe4f5436360b2ce9afcc7f9a7f0116","modified":1513652153807},{"_id":"source/assets/img/myqq.png","hash":"c8127e9005c9ef060a2a8253b05fabf553757e0c","modified":1513652096791},{"_id":"source/assets/img/weixin.jpeg","hash":"7c4ee260674f709476d2c6338ad1fdc969711ea0","modified":1513652096795},{"_id":"source/assets/img/zhifubao.jpeg","hash":"cde94ff388e7ef7d37f9a127e35db58b0a97f67c","modified":1513652096795},{"_id":"themes/yilia/layout/_partial/script.ejs","hash":"4cb685f07e89dd5175c2a576e73a1a957aec5637","modified":1513652152263},{"_id":"themes/yilia/.git/objects/pack/pack-81482136cf6b522e44e787dc9c868acfcb7c25a6.idx","hash":"192241e09a6504ede0484bccfdb070cc40a2907a","modified":1513652150723},{"_id":"themes/yilia/.git/refs/heads/master","hash":"4ed77da1a2617db0e77c3e3e190a1c79c16dfb9a","modified":1513652149439},{"_id":"themes/yilia/layout/_partial/post/category.ejs","hash":"0809a4829aabeb4e911a3ed04ec28db4df7dfe3f","modified":1513652152651},{"_id":"themes/yilia/layout/_partial/post/changyan.ejs","hash":"5f99b55980da64a723a8e14d5a7daba0d6504647","modified":1513652152959},{"_id":"themes/yilia/layout/_partial/post/date.ejs","hash":"ef71c4081e866a494367575c59610e7e6339ece0","modified":1513652152911},{"_id":"themes/yilia/layout/_partial/post/duoshuo.ejs","hash":"e8399025ed3b980aedb821c92855889f5f12fd5b","modified":1513652152823},{"_id":"themes/yilia/layout/_partial/post/gitment.ejs","hash":"e68bbac9ffb1ad27b56837c9abad6ed6bb7daa0c","modified":1513652152531},{"_id":"themes/yilia/layout/_partial/post/nav.ejs","hash":"1036c8e4e1a7bc935ba173744da735a0d6ed09cd","modified":1513652152431},{"_id":"themes/yilia/layout/_partial/post/share.ejs","hash":"5dccfbe165b23a101f1333cc65ed8efbd197453c","modified":1513652152867},{"_id":"themes/yilia/layout/_partial/post/tag.ejs","hash":"2e783e68755abb852760eb0e627a3fbb50a05a55","modified":1513652152479},{"_id":"themes/yilia/layout/_partial/post/title.ejs","hash":"2f275739b6f1193c123646a5a31f37d48644c667","modified":1513652152783},{"_id":"themes/yilia/layout/_partial/post/wangyiyun.ejs","hash":"ea41c462168d9697caef9485862e9cac718a12c1","modified":1513652152579},{"_id":"themes/yilia/source-src/css/core/_animation.scss","hash":"63a37f26276f9207405afe0f2d65339ce295bbaf","modified":1513652154411},{"_id":"themes/yilia/source-src/css/core/_media-queries.scss","hash":"491ab3378d5c11005ba65c607608bb36b368a9d5","modified":1513652154591},{"_id":"themes/yilia/source-src/css/core/_mixin.scss","hash":"3bba5c77bad5981eac859fe05c9561d580ba7fa9","modified":1513652154459},{"_id":"themes/yilia/source-src/css/core/_reset.scss","hash":"fab871fa93bd542e76a71a56428f2994a4aaf443","modified":1513652154547},{"_id":"themes/yilia/source-src/css/core/_variables.scss","hash":"fb511c505d1309249f21dc577d4ad2bad99a764f","modified":1513652154499},{"_id":"themes/yilia/source-src/css/fonts/iconfont.eot","hash":"bc8c5e88f4994a852041b4d83f126d9c4d419b4a","modified":1513652154923},{"_id":"themes/yilia/source-src/css/fonts/iconfont.svg","hash":"f9304e5714d20861be7d8f4d36687e88e86b9e1b","modified":1513652155027},{"_id":"themes/yilia/source-src/css/fonts/iconfont.ttf","hash":"f342ac8bf4d937f42a7d6a0032ad267ab47eb7f2","modified":1513652154983},{"_id":"themes/yilia/source-src/css/fonts/iconfont.woff","hash":"aa9672cb097f7fd73ae5a03bcd3d9d726935bc0a","modified":1513652155071},{"_id":"themes/yilia/source-src/css/img/checkered-pattern.png","hash":"049262fa0886989d750637b264bed34ab51c23c8","modified":1513652154879},{"_id":"themes/yilia/source-src/css/img/scrollbar_arrow.png","hash":"d64a33c4ddfbdb89deeb6f4e3d36eb84dc4777c0","modified":1513652154767},{"_id":"themes/yilia/source-src/css/img/tooltip.svg","hash":"397fe4b1093bf9b62457dac48aa15dac06b54a3c","modified":1513652154823},{"_id":"source/assets/img/myweixin.png","hash":"c38c97cd0a5563d54b7c6bb34ea4ae0663ab7174","modified":1513652096791},{"_id":"themes/yilia/.git/logs/refs/heads/master","hash":"72c97891247c040ebc4d93ddef86880c00f27185","modified":1513652150279},{"_id":"themes/yilia/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1513652149399},{"_id":"themes/yilia/.git/logs/refs/remotes/origin/HEAD","hash":"72c97891247c040ebc4d93ddef86880c00f27185","modified":1513652150199},{"_id":"themes/yilia/.git/objects/pack/pack-81482136cf6b522e44e787dc9c868acfcb7c25a6.pack","hash":"a3546a4602f8b7a5a61f5bdc6ff14c6523cf6441","modified":1513652150555},{"_id":"public/photos/lazyload.min.js","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1513652727180},{"_id":"public/photos/ins.css","hash":"8c20f68a514bb8ff9a04207cf2f7d245710973c9","modified":1513652727180},{"_id":"public/photos/ins.js","hash":"30d1de36678c7b1eff68c8e37534cd34f91dd235","modified":1513652727905},{"_id":"public/photos/ins.json","hash":"35bf12235f5ffb42fd2b7f47b1ac2bd86e70ab03","modified":1513652727943},{"_id":"public/atom.xml","hash":"d3020c51c3e6bed361bb6ac1abe0c8b3ea5f69df","modified":1513652727943},{"_id":"public/content.json","hash":"cfb73373feebe1c57ba42875ee786abc49721e7b","modified":1513652727987},{"_id":"public/sitemap.xml","hash":"814bac5d1f1d2ac21506ed435f887664db93aa58","modified":1513652727987},{"_id":"public/photos/index.html","hash":"bbcb63bfe8c670ec4bc651fcf5808cef8bf0cd1e","modified":1513652728006},{"_id":"public/2017/12/14/随手记/Hexo-Yilia加入相册功能/index.html","hash":"7806ca24cd5c3a3360982c54aca6828b93f26e1e","modified":1513652728006},{"_id":"public/2017/12/14/机器学习/梯度算法之批量梯度下降，随机梯度下降和小批量梯度下降/index.html","hash":"5c59b4e338796e4aeb6ff461c88f11c9c9d502e3","modified":1513652728007},{"_id":"public/2017/12/14/机器学习/梯度算法之梯度上升和梯度下降/index.html","hash":"3637cb297851e11912bdce913689614da57662ea","modified":1513652728007},{"_id":"public/2017/11/20/ELK/异常检测之指数平滑（利用elasticsearch来实现）/index.html","hash":"20f23d174ea21846ec1eaedde50d0694d29498f6","modified":1513652728007},{"_id":"public/2017/11/14/ELK/Elasticsearch-DSL部分集合/index.html","hash":"1196ee5aaae2dbc7564a1437d88b2c203d02a840","modified":1513652728007},{"_id":"public/2017/11/13/数据结构/数据结构算法之链表/index.html","hash":"94982858f4e2944f1ae4f328c693e3559344b284","modified":1513652728007},{"_id":"public/2017/11/13/数据结构/数据结构算法之合并两个有序序列/index.html","hash":"f88a9a39fa2910b432cd076825335ac747d17194","modified":1513652728008},{"_id":"public/2017/11/13/数据结构/数据结构算法之排序/index.html","hash":"90e1bed54e636a3bbb30f67a732b83bbd84ef469","modified":1513652728008},{"_id":"public/2017/11/13/数据结构/数据结构算法之二叉树/index.html","hash":"f9991613f0a147f7b3c293f280b500dfe4e68ded","modified":1513652728008},{"_id":"public/2017/11/07/机器学习/回归分析之Sklearn实现电力预测/index.html","hash":"1e80e2362748c6ecebe68401fb09872ababf700d","modified":1513652728009},{"_id":"public/2017/09/29/机器学习/回归分析之线性回归（N元线性回归）/index.html","hash":"17bbe8a6ad7c7ae13d67f6901d618a1fca01a969","modified":1513652728009},{"_id":"public/2017/09/20/机器学习/几种距离计算公式在数据挖掘中的应用场景分析/index.html","hash":"c27d11436ff173830f9a799e9e1400ad2e819634","modified":1513652728009},{"_id":"public/2017/09/17/机器学习/回归分析之理论篇/index.html","hash":"a0d584e6dd014af7b8b280b1ebcce365d6279c40","modified":1513652728009},{"_id":"public/2017/09/01/机器学习/数据归一化和其在sklearn中的处理/index.html","hash":"6a028feb90a7903a7063bc5bfc9eae2e7d7e2fa1","modified":1513652728010},{"_id":"public/2017/07/16/机器学习/MachingLearning中的距离和相似性计算以及python实现/index.html","hash":"c0b0b3843f6a8d205ed9066881b58b100c497a00","modified":1513652728010},{"_id":"public/2017/04/16/随手记/一切的闹闹哄哄，只是他在水帘洞躲避风沙那晚做的一个梦/index.html","hash":"82d811c10b0d89eb04739d92977bd26036a5f311","modified":1513652728010},{"_id":"public/2016/12/21/随手记/别了青春与流年，遇见下一个自己/index.html","hash":"ca8170931dfea729163345581751a8ccff0d8259","modified":1513652728010},{"_id":"public/2016/09/15/夏未眠/这夏未眠-简介/index.html","hash":"d2c9000523feb14d7e5ea504facf64bee75aabbf","modified":1513652728010},{"_id":"public/2016/09/15/夏未眠/这夏未眠-序/index.html","hash":"ff7fd217e32fe22726f3131f3ac7fd2eecda742a","modified":1513652728010},{"_id":"public/archives/index.html","hash":"f93a299203d85fe9fbf98fa4e166392a6bac8124","modified":1513652728011},{"_id":"public/archives/page/2/index.html","hash":"4b608d82e885526f43be02b59f0534b826d3f324","modified":1513652728011},{"_id":"public/archives/2016/index.html","hash":"65d496b9f7903ed5c3ddc741319f776fe415b879","modified":1513652728011},{"_id":"public/archives/2016/09/index.html","hash":"73f3366e05c1659358427989d9bff1046b8899b3","modified":1513652728011},{"_id":"public/archives/2016/12/index.html","hash":"2c24bd6e645ca0d71b9413e82fa1fb657b330515","modified":1513652728011},{"_id":"public/archives/2017/index.html","hash":"804853425a13523229d0b49cc48cbd088bb28355","modified":1513652728011},{"_id":"public/archives/2017/page/2/index.html","hash":"b5bde12c120ecd65e7c2a3a159740ec1470f98f0","modified":1513652728011},{"_id":"public/archives/2017/04/index.html","hash":"4c512a178aa51140046866757d7ce452c6430936","modified":1513652728011},{"_id":"public/archives/2017/07/index.html","hash":"e00a9d7f8f6b177d32965921f04df4af69552a81","modified":1513652728011},{"_id":"public/archives/2017/09/index.html","hash":"e1e247c3bc8f65da130dafdacc8ab863ddc4334c","modified":1513652728011},{"_id":"public/archives/2017/11/index.html","hash":"c3f931e1595335cb32e48093335bd21fd4ff4df1","modified":1513652728012},{"_id":"public/archives/2017/12/index.html","hash":"cea8777e11d4f2be371873ff91bf0c5921718863","modified":1513652728012},{"_id":"public/categories/技术篇/index.html","hash":"7272645489f093a3ed77b29700f2c5b6cf129af5","modified":1513652728012},{"_id":"public/categories/技术篇/page/2/index.html","hash":"2e288b5c4699ad91372f55c6179400cc2ed24de5","modified":1513652728012},{"_id":"public/categories/夏未眠/index.html","hash":"98f06b3837f164518fb21bdef9f876570a54443d","modified":1513652728012},{"_id":"public/categories/随手记/index.html","hash":"6ad7d097af5f825e01abbdb2a5fa9289f152fe3b","modified":1513652728012},{"_id":"public/index.html","hash":"c3dd99d3067ed1324b472a9b5036b7bf2004aad2","modified":1513652728012},{"_id":"public/page/2/index.html","hash":"d564a19b2d0327c372ae699ebffcf41b2b461648","modified":1513652728012},{"_id":"public/tags/ELK/index.html","hash":"0a2785f38a93d086d6500bba67c8b10900ab72f6","modified":1513652728012},{"_id":"public/tags/ES/index.html","hash":"9fd1152b7f40ab439a7b52cd791fb801bda8d858","modified":1513652728013},{"_id":"public/tags/异常检测/index.html","hash":"4d7b90faa43718b47ea534e70513f72bd6b113be","modified":1513652728013},{"_id":"public/tags/夏未眠/index.html","hash":"3238c067189838614677e23f197848c3de10dcda","modified":1513652728013},{"_id":"public/tags/数据结构/index.html","hash":"b28687268eba0edcbfa59b7200c798e2308de454","modified":1513652728013},{"_id":"public/tags/距离计算/index.html","hash":"aed9adbbe741d0ce49d47d29219e34dc93a76ffa","modified":1513652728013},{"_id":"public/tags/回归分析/index.html","hash":"d703cfb40e6fded10bf27a8167c1bb4046d68b7a","modified":1513652728013},{"_id":"public/tags/sklearn/index.html","hash":"399a50900d9af98bca6b2c75799ed94b470f4fb1","modified":1513652728014},{"_id":"public/tags/正态分布/index.html","hash":"1cfdffc0c0d7e7365febf13b79bf825868f74911","modified":1513652728014},{"_id":"public/tags/数据归一化/index.html","hash":"e3619969b93fdacf3d567c85a54246084137cd81","modified":1513652728014},{"_id":"public/tags/梯度下降/index.html","hash":"ac95f6c365e07781fb3c52967f5926edcb8f1a40","modified":1513652728014},{"_id":"public/tags/hexo/index.html","hash":"ec549556afdc028599ece9abf0945ffaf59c9728","modified":1513652728014},{"_id":"public/tags/随手记/index.html","hash":"5913c0391b6fa5f2564183139ad319b38a7a5a20","modified":1513652728014},{"_id":"public/assets/img/favicon.ico","hash":"292808ea6ec3f69bc01f10697fed6567bafed8f9","modified":1513652728023},{"_id":"public/assets/img/gzh.png","hash":"41c9bc4e3009faee1a8404398c45eeca8b9508ca","modified":1513652728023},{"_id":"public/assets/img/gongzhonghao.jpg","hash":"c973bbc42f022c18baa54432458093849fe6699a","modified":1513652728023},{"_id":"public/assets/img/head.jpg","hash":"22e476c6393281457ec0d2f39fd13a5ebffb21b2","modified":1513652728023},{"_id":"public/fonts/default-skin.b257fa.svg","hash":"2ac727c9e092331d35cce95af209ccfac6d4c7c7","modified":1513652728023},{"_id":"public/fonts/iconfont.16acc2.ttf","hash":"f342ac8bf4d937f42a7d6a0032ad267ab47eb7f2","modified":1513652728023},{"_id":"public/fonts/iconfont.45d7ee.svg","hash":"f9304e5714d20861be7d8f4d36687e88e86b9e1b","modified":1513652728023},{"_id":"public/fonts/iconfont.b322fa.eot","hash":"bc8c5e88f4994a852041b4d83f126d9c4d419b4a","modified":1513652728023},{"_id":"public/fonts/iconfont.8c627f.woff","hash":"aa9672cb097f7fd73ae5a03bcd3d9d726935bc0a","modified":1513652728024},{"_id":"public/fonts/tooltip.4004ff.svg","hash":"397fe4b1093bf9b62457dac48aa15dac06b54a3c","modified":1513652728024},{"_id":"public/img/default-skin.png","hash":"ed95a8e40a2c3478c5915376acb8e5f33677f24d","modified":1513652728024},{"_id":"public/img/preloader.gif","hash":"6342367c93c82da1b9c620e97c84a389cc43d96d","modified":1513652728024},{"_id":"public/img/scrollbar_arrow.png","hash":"d64a33c4ddfbdb89deeb6f4e3d36eb84dc4777c0","modified":1513652728024},{"_id":"public/assets/img/myqq.png","hash":"c8127e9005c9ef060a2a8253b05fabf553757e0c","modified":1513652728029},{"_id":"public/assets/img/weixin.jpeg","hash":"7c4ee260674f709476d2c6338ad1fdc969711ea0","modified":1513652728031},{"_id":"public/assets/img/zhifubao.jpeg","hash":"cde94ff388e7ef7d37f9a127e35db58b0a97f67c","modified":1513652728031},{"_id":"public/main.0cf68a.css","hash":"ddf6e2c6b953c2c59a3c271e6070010a4cc81cf9","modified":1513652728034},{"_id":"public/main.0cf68a.js","hash":"993fadeb5f6d296e9d997a49ee20dc97333ceab7","modified":1513652728034},{"_id":"public/slider.e37972.js","hash":"6dec4e220c89049037eebc44404abd8455d22ad7","modified":1513652728034},{"_id":"public/mobile.992cbe.js","hash":"01b35e71e37aa2849664eb5daf26daede2278398","modified":1513652728034},{"_id":"public/assets/img/myweixin.png","hash":"c38c97cd0a5563d54b7c6bb34ea4ae0663ab7174","modified":1513652728038}],"Category":[{"name":"技术篇","_id":"cjbd1r25r0007kxuwyjk4r7hv"},{"name":"夏未眠","_id":"cjbd1r263000gkxuwj2mhft7c"},{"name":"随手记","_id":"cjbd1r26v001ikxuwrsxf5aaq"}],"Data":[],"Page":[{"_content":"","source":"photos/lazyload.min.js","raw":"","date":"2017-12-19T02:54:56.795Z","updated":"2017-12-19T02:54:56.795Z","path":"photos/lazyload.min.js","layout":"false","title":"","comments":1,"_id":"cjbd1r2480000kxuww83ykxmh","content":"","site":{"data":{}},"excerpt":"","more":""},{"layout":"post","slug":"photos","title":"相册","noDate":"true","comments":0,"_content":"<link rel=\"stylesheet\" href=\"./ins.css\">\n<div class=\"photos-btn-wrap\">\n\t<!--<a class=\"photos-btn active\" href=\"javascript:void(0)\">相册</a>\n\t<a class=\"photos-btn\" target=\"_blank\" href=\"http://litten.me/gallery/\">摄影</a> -->\n</div>\n<div class=\"instagram itemscope\">\n\t<a href=\"https://www.instagram.com/litten225/\" target=\"_blank\" class=\"open-ins\">图片来自Thinkgamer，正在加载中…</a>\n</div>\n<script>\n  (function() {\n    var loadScript = function(path) {\n      var $script = document.createElement('script')\n      document.getElementsByTagName('body')[0].appendChild($script)\n      $script.setAttribute('src', path)\n    }\n    setTimeout(function() {\n      loadScript('./ins.js')\n    }, 0)\n  })()\n</script>\n","source":"photos/index.ejs","raw":"---\nlayout: post\nslug: \"photos\"\ntitle: \"相册\"\nnoDate: \"true\"\ncomments: \"false\"\n---\n<link rel=\"stylesheet\" href=\"./ins.css\">\n<div class=\"photos-btn-wrap\">\n\t<!--<a class=\"photos-btn active\" href=\"javascript:void(0)\">相册</a>\n\t<a class=\"photos-btn\" target=\"_blank\" href=\"http://litten.me/gallery/\">摄影</a> -->\n</div>\n<div class=\"instagram itemscope\">\n\t<a href=\"https://www.instagram.com/litten225/\" target=\"_blank\" class=\"open-ins\">图片来自Thinkgamer，正在加载中…</a>\n</div>\n<script>\n  (function() {\n    var loadScript = function(path) {\n      var $script = document.createElement('script')\n      document.getElementsByTagName('body')[0].appendChild($script)\n      $script.setAttribute('src', path)\n    }\n    setTimeout(function() {\n      loadScript('./ins.js')\n    }, 0)\n  })()\n</script>\n","date":"2017-12-19T02:54:56.795Z","updated":"2017-12-19T02:54:56.795Z","path":"photos/index.html","_id":"cjbd1r24s0001kxuw4jsxnevg","content":"<link rel=\"stylesheet\" href=\"./ins.css\">\n<div class=\"photos-btn-wrap\">\n\t<!--<a class=\"photos-btn active\" href=\"javascript:void(0)\">相册</a>\n\t<a class=\"photos-btn\" target=\"_blank\" href=\"http://litten.me/gallery/\">摄影</a> -->\n</div>\n<div class=\"instagram itemscope\">\n\t<a href=\"https://www.instagram.com/litten225/\" target=\"_blank\" class=\"open-ins\">图片来自Thinkgamer，正在加载中…</a>\n</div>\n<script>\n  (function() {\n    var loadScript = function(path) {\n      var $script = document.createElement('script')\n      document.getElementsByTagName('body')[0].appendChild($script)\n      $script.setAttribute('src', path)\n    }\n    setTimeout(function() {\n      loadScript('./ins.js')\n    }, 0)\n  })()\n</script>\n","site":{"data":{}},"excerpt":"","more":"<link rel=\"stylesheet\" href=\"./ins.css\">\n<div class=\"photos-btn-wrap\">\n\t<!--<a class=\"photos-btn active\" href=\"javascript:void(0)\">相册</a>\n\t<a class=\"photos-btn\" target=\"_blank\" href=\"http://litten.me/gallery/\">摄影</a> -->\n</div>\n<div class=\"instagram itemscope\">\n\t<a href=\"https://www.instagram.com/litten225/\" target=\"_blank\" class=\"open-ins\">图片来自Thinkgamer，正在加载中…</a>\n</div>\n<script>\n  (function() {\n    var loadScript = function(path) {\n      var $script = document.createElement('script')\n      document.getElementsByTagName('body')[0].appendChild($script)\n      $script.setAttribute('src', path)\n    }\n    setTimeout(function() {\n      loadScript('./ins.js')\n    }, 0)\n  })()\n</script>\n"},{"_content":"#post-instagram{\n\tpadding: 30px;\n}\n#post-instagram .article-entry{\n\tpadding-right: 0;\n}\n.instagram{\n\tposition: relative;\n\tmin-height: 500px;\n}\n.instagram img {\n\twidth: 100%;\n}\n.instagram .year {\n\tfont-size: 16px;\n}\n.instagram .open-ins{\n\tpadding: 10px 0;\n\tcolor: #cdcdcd;\n}\n.instagram .open-ins:hover{\n\tcolor: #657b83;\n}\n.instagram .year{\n\tdisplay: inline;\n}\n.instagram .thumb {\n\twidth: 25%;\n\theight: 0;\n\tpadding-bottom: 25%;\n\tposition: relative;\n\tdisplay: inline-block;\n\ttext-align: center;\n\tbackground: #ededed;\n}\n.instagram .thumb a {\n\tposition: relative;\n}\n.instagram .album h1 em{\n\tfont-style: normal;\n\tfont-size: 14px;\n\tmargin-left: 10px;\n}\n.instagram .album ul{\n\tdisplay: flex;\n\tflex-wrap: wrap;\n\tclear: both;\n\twidth: 100%;\n\ttext-align: left;\n}\n.instagram .album li{\n\tlist-style: none;\n\tdisplay: inline-block;\n\tbox-sizing: border-box;\n\tpadding: 0 5px;\n\tmargin-bottom: -10px;\n\theight: 0;\n\twidth: 25%;\n\tposition: relative;\n\tpadding-bottom: 25%;\n}\n.instagram .album li:before{\n\tdisplay: none;\n}\n.instagram .album div.img-box{\n\tposition: absolute;\n\twidth: 90%;\n\theight: 90%;\n\t-webkit-box-shadow: 0 1px 0 rgba(255,255,255,0.4), 0 1px 0 1px rgba(255,255,255,0.1);\n\t-moz-box-shadow: 0 1px 0 rgba(255,255,255,0.4), 0 1px 0 1px rgba(255,255,255,0.1);\n\tbox-shadow: 0 1px 0 rgba(255,255,255,0.4), 0 1px 0 1px rgba(255,255,255,0.1);\n}\n.instagram .album div.img-box img{\n\twidth: 100%;\n\theight: 100%;\n    position: absolute;\n    z-index: 2;\n}\n.instagram .album div.img-box .img-bg{\n\tposition: absolute;\n\ttop: 0;\n\tleft: 0;\n\tbottom: 0px;\n\twidth: 100%;\n\tmargin: -5px;\n\tpadding: 5px;\n\t-webkit-box-shadow: 0 0 0 1px rgba(0,0,0,.04), 0 1px 5px rgba(0,0,0,0.1);\n\t-moz-box-shadow: 0 0 0 1px rgba(0,0,0,.04), 0 1px 5px rgba(0,0,0,0.1);\n\tbox-shadow: 0 0 0 1px rgba(0,0,0,.04), 0 1px 5px rgba(0,0,0,0.1);\n\t-webkit-transition: all 0.15s ease-out 0.1s;\n\t-moz-transition: all 0.15s ease-out 0.1s;\n\t-o-transition: all 0.15s ease-out 0.1s;\n\ttransition: all 0.15s ease-out 0.1s;\n\topacity: 0.2;\n\tcursor: pointer;\n\tdisplay: block;\n\tz-index: 3;\n}\n.instagram .album div.img-box .icon {\n    font-size: 14px;\n    position: absolute;\n    left: 50%;\n    top: 50%;\n    margin-left: -7px;\n    margin-top: -7px;\n    color: #999;\n    z-index: 1;\n}\n.instagram .album div.img-box .img-bg:hover{\n\topacity: 0;\n}\n.photos-btn-wrap {\n\tborder-bottom: 1px solid #e5e5e5;\n\tmargin-bottom: 20px;\n}\n.photos-btn {\n\tfont-size: 16px;\n\tcolor: #333;\n\tmargin-bottom: -4px;\n\tpadding: 5px 8px 3px;\n}\n.photos-btn.active {\n\tcolor: #08c;\n\tborder: 1px solid #e5e5e5;\n\tborder-bottom: 5px solid #fff;\n}\n\n@media screen and (max-width:600px) {\n\t.instagram .thumb {\n\t\twidth: 50%;\n\t\tpadding-bottom: 50%;\n\t}\n\t.instagram .album li {\n\t\twidth: 100%;\n\t\tposition: relative;\n\t\tpadding-bottom: 100%;\n\t\ttext-align: center;\n\t}\n\t.instagram .album div.img-box{\n\t\tmargin: 0;\n\t\twidth: 90%;\n\t\theight: 90%;\n\t}\n}","source":"photos/ins.css","raw":"#post-instagram{\n\tpadding: 30px;\n}\n#post-instagram .article-entry{\n\tpadding-right: 0;\n}\n.instagram{\n\tposition: relative;\n\tmin-height: 500px;\n}\n.instagram img {\n\twidth: 100%;\n}\n.instagram .year {\n\tfont-size: 16px;\n}\n.instagram .open-ins{\n\tpadding: 10px 0;\n\tcolor: #cdcdcd;\n}\n.instagram .open-ins:hover{\n\tcolor: #657b83;\n}\n.instagram .year{\n\tdisplay: inline;\n}\n.instagram .thumb {\n\twidth: 25%;\n\theight: 0;\n\tpadding-bottom: 25%;\n\tposition: relative;\n\tdisplay: inline-block;\n\ttext-align: center;\n\tbackground: #ededed;\n}\n.instagram .thumb a {\n\tposition: relative;\n}\n.instagram .album h1 em{\n\tfont-style: normal;\n\tfont-size: 14px;\n\tmargin-left: 10px;\n}\n.instagram .album ul{\n\tdisplay: flex;\n\tflex-wrap: wrap;\n\tclear: both;\n\twidth: 100%;\n\ttext-align: left;\n}\n.instagram .album li{\n\tlist-style: none;\n\tdisplay: inline-block;\n\tbox-sizing: border-box;\n\tpadding: 0 5px;\n\tmargin-bottom: -10px;\n\theight: 0;\n\twidth: 25%;\n\tposition: relative;\n\tpadding-bottom: 25%;\n}\n.instagram .album li:before{\n\tdisplay: none;\n}\n.instagram .album div.img-box{\n\tposition: absolute;\n\twidth: 90%;\n\theight: 90%;\n\t-webkit-box-shadow: 0 1px 0 rgba(255,255,255,0.4), 0 1px 0 1px rgba(255,255,255,0.1);\n\t-moz-box-shadow: 0 1px 0 rgba(255,255,255,0.4), 0 1px 0 1px rgba(255,255,255,0.1);\n\tbox-shadow: 0 1px 0 rgba(255,255,255,0.4), 0 1px 0 1px rgba(255,255,255,0.1);\n}\n.instagram .album div.img-box img{\n\twidth: 100%;\n\theight: 100%;\n    position: absolute;\n    z-index: 2;\n}\n.instagram .album div.img-box .img-bg{\n\tposition: absolute;\n\ttop: 0;\n\tleft: 0;\n\tbottom: 0px;\n\twidth: 100%;\n\tmargin: -5px;\n\tpadding: 5px;\n\t-webkit-box-shadow: 0 0 0 1px rgba(0,0,0,.04), 0 1px 5px rgba(0,0,0,0.1);\n\t-moz-box-shadow: 0 0 0 1px rgba(0,0,0,.04), 0 1px 5px rgba(0,0,0,0.1);\n\tbox-shadow: 0 0 0 1px rgba(0,0,0,.04), 0 1px 5px rgba(0,0,0,0.1);\n\t-webkit-transition: all 0.15s ease-out 0.1s;\n\t-moz-transition: all 0.15s ease-out 0.1s;\n\t-o-transition: all 0.15s ease-out 0.1s;\n\ttransition: all 0.15s ease-out 0.1s;\n\topacity: 0.2;\n\tcursor: pointer;\n\tdisplay: block;\n\tz-index: 3;\n}\n.instagram .album div.img-box .icon {\n    font-size: 14px;\n    position: absolute;\n    left: 50%;\n    top: 50%;\n    margin-left: -7px;\n    margin-top: -7px;\n    color: #999;\n    z-index: 1;\n}\n.instagram .album div.img-box .img-bg:hover{\n\topacity: 0;\n}\n.photos-btn-wrap {\n\tborder-bottom: 1px solid #e5e5e5;\n\tmargin-bottom: 20px;\n}\n.photos-btn {\n\tfont-size: 16px;\n\tcolor: #333;\n\tmargin-bottom: -4px;\n\tpadding: 5px 8px 3px;\n}\n.photos-btn.active {\n\tcolor: #08c;\n\tborder: 1px solid #e5e5e5;\n\tborder-bottom: 5px solid #fff;\n}\n\n@media screen and (max-width:600px) {\n\t.instagram .thumb {\n\t\twidth: 50%;\n\t\tpadding-bottom: 50%;\n\t}\n\t.instagram .album li {\n\t\twidth: 100%;\n\t\tposition: relative;\n\t\tpadding-bottom: 100%;\n\t\ttext-align: center;\n\t}\n\t.instagram .album div.img-box{\n\t\tmargin: 0;\n\t\twidth: 90%;\n\t\theight: 90%;\n\t}\n}","date":"2017-12-19T02:54:56.795Z","updated":"2017-12-19T02:54:56.795Z","path":"photos/ins.css","layout":"false","title":"","comments":1,"_id":"cjbd1r24v0002kxuws963yy3c","content":"#post-instagram{\n\tpadding: 30px;\n}\n#post-instagram .article-entry{\n\tpadding-right: 0;\n}\n.instagram{\n\tposition: relative;\n\tmin-height: 500px;\n}\n.instagram img {\n\twidth: 100%;\n}\n.instagram .year {\n\tfont-size: 16px;\n}\n.instagram .open-ins{\n\tpadding: 10px 0;\n\tcolor: #cdcdcd;\n}\n.instagram .open-ins:hover{\n\tcolor: #657b83;\n}\n.instagram .year{\n\tdisplay: inline;\n}\n.instagram .thumb {\n\twidth: 25%;\n\theight: 0;\n\tpadding-bottom: 25%;\n\tposition: relative;\n\tdisplay: inline-block;\n\ttext-align: center;\n\tbackground: #ededed;\n}\n.instagram .thumb a {\n\tposition: relative;\n}\n.instagram .album h1 em{\n\tfont-style: normal;\n\tfont-size: 14px;\n\tmargin-left: 10px;\n}\n.instagram .album ul{\n\tdisplay: flex;\n\tflex-wrap: wrap;\n\tclear: both;\n\twidth: 100%;\n\ttext-align: left;\n}\n.instagram .album li{\n\tlist-style: none;\n\tdisplay: inline-block;\n\tbox-sizing: border-box;\n\tpadding: 0 5px;\n\tmargin-bottom: -10px;\n\theight: 0;\n\twidth: 25%;\n\tposition: relative;\n\tpadding-bottom: 25%;\n}\n.instagram .album li:before{\n\tdisplay: none;\n}\n.instagram .album div.img-box{\n\tposition: absolute;\n\twidth: 90%;\n\theight: 90%;\n\t-webkit-box-shadow: 0 1px 0 rgba(255,255,255,0.4), 0 1px 0 1px rgba(255,255,255,0.1);\n\t-moz-box-shadow: 0 1px 0 rgba(255,255,255,0.4), 0 1px 0 1px rgba(255,255,255,0.1);\n\tbox-shadow: 0 1px 0 rgba(255,255,255,0.4), 0 1px 0 1px rgba(255,255,255,0.1);\n}\n.instagram .album div.img-box img{\n\twidth: 100%;\n\theight: 100%;\n    position: absolute;\n    z-index: 2;\n}\n.instagram .album div.img-box .img-bg{\n\tposition: absolute;\n\ttop: 0;\n\tleft: 0;\n\tbottom: 0px;\n\twidth: 100%;\n\tmargin: -5px;\n\tpadding: 5px;\n\t-webkit-box-shadow: 0 0 0 1px rgba(0,0,0,.04), 0 1px 5px rgba(0,0,0,0.1);\n\t-moz-box-shadow: 0 0 0 1px rgba(0,0,0,.04), 0 1px 5px rgba(0,0,0,0.1);\n\tbox-shadow: 0 0 0 1px rgba(0,0,0,.04), 0 1px 5px rgba(0,0,0,0.1);\n\t-webkit-transition: all 0.15s ease-out 0.1s;\n\t-moz-transition: all 0.15s ease-out 0.1s;\n\t-o-transition: all 0.15s ease-out 0.1s;\n\ttransition: all 0.15s ease-out 0.1s;\n\topacity: 0.2;\n\tcursor: pointer;\n\tdisplay: block;\n\tz-index: 3;\n}\n.instagram .album div.img-box .icon {\n    font-size: 14px;\n    position: absolute;\n    left: 50%;\n    top: 50%;\n    margin-left: -7px;\n    margin-top: -7px;\n    color: #999;\n    z-index: 1;\n}\n.instagram .album div.img-box .img-bg:hover{\n\topacity: 0;\n}\n.photos-btn-wrap {\n\tborder-bottom: 1px solid #e5e5e5;\n\tmargin-bottom: 20px;\n}\n.photos-btn {\n\tfont-size: 16px;\n\tcolor: #333;\n\tmargin-bottom: -4px;\n\tpadding: 5px 8px 3px;\n}\n.photos-btn.active {\n\tcolor: #08c;\n\tborder: 1px solid #e5e5e5;\n\tborder-bottom: 5px solid #fff;\n}\n\n@media screen and (max-width:600px) {\n\t.instagram .thumb {\n\t\twidth: 50%;\n\t\tpadding-bottom: 50%;\n\t}\n\t.instagram .album li {\n\t\twidth: 100%;\n\t\tposition: relative;\n\t\tpadding-bottom: 100%;\n\t\ttext-align: center;\n\t}\n\t.instagram .album div.img-box{\n\t\tmargin: 0;\n\t\twidth: 90%;\n\t\theight: 90%;\n\t}\n}","site":{"data":{}},"excerpt":"","more":"#post-instagram{\n\tpadding: 30px;\n}\n#post-instagram .article-entry{\n\tpadding-right: 0;\n}\n.instagram{\n\tposition: relative;\n\tmin-height: 500px;\n}\n.instagram img {\n\twidth: 100%;\n}\n.instagram .year {\n\tfont-size: 16px;\n}\n.instagram .open-ins{\n\tpadding: 10px 0;\n\tcolor: #cdcdcd;\n}\n.instagram .open-ins:hover{\n\tcolor: #657b83;\n}\n.instagram .year{\n\tdisplay: inline;\n}\n.instagram .thumb {\n\twidth: 25%;\n\theight: 0;\n\tpadding-bottom: 25%;\n\tposition: relative;\n\tdisplay: inline-block;\n\ttext-align: center;\n\tbackground: #ededed;\n}\n.instagram .thumb a {\n\tposition: relative;\n}\n.instagram .album h1 em{\n\tfont-style: normal;\n\tfont-size: 14px;\n\tmargin-left: 10px;\n}\n.instagram .album ul{\n\tdisplay: flex;\n\tflex-wrap: wrap;\n\tclear: both;\n\twidth: 100%;\n\ttext-align: left;\n}\n.instagram .album li{\n\tlist-style: none;\n\tdisplay: inline-block;\n\tbox-sizing: border-box;\n\tpadding: 0 5px;\n\tmargin-bottom: -10px;\n\theight: 0;\n\twidth: 25%;\n\tposition: relative;\n\tpadding-bottom: 25%;\n}\n.instagram .album li:before{\n\tdisplay: none;\n}\n.instagram .album div.img-box{\n\tposition: absolute;\n\twidth: 90%;\n\theight: 90%;\n\t-webkit-box-shadow: 0 1px 0 rgba(255,255,255,0.4), 0 1px 0 1px rgba(255,255,255,0.1);\n\t-moz-box-shadow: 0 1px 0 rgba(255,255,255,0.4), 0 1px 0 1px rgba(255,255,255,0.1);\n\tbox-shadow: 0 1px 0 rgba(255,255,255,0.4), 0 1px 0 1px rgba(255,255,255,0.1);\n}\n.instagram .album div.img-box img{\n\twidth: 100%;\n\theight: 100%;\n    position: absolute;\n    z-index: 2;\n}\n.instagram .album div.img-box .img-bg{\n\tposition: absolute;\n\ttop: 0;\n\tleft: 0;\n\tbottom: 0px;\n\twidth: 100%;\n\tmargin: -5px;\n\tpadding: 5px;\n\t-webkit-box-shadow: 0 0 0 1px rgba(0,0,0,.04), 0 1px 5px rgba(0,0,0,0.1);\n\t-moz-box-shadow: 0 0 0 1px rgba(0,0,0,.04), 0 1px 5px rgba(0,0,0,0.1);\n\tbox-shadow: 0 0 0 1px rgba(0,0,0,.04), 0 1px 5px rgba(0,0,0,0.1);\n\t-webkit-transition: all 0.15s ease-out 0.1s;\n\t-moz-transition: all 0.15s ease-out 0.1s;\n\t-o-transition: all 0.15s ease-out 0.1s;\n\ttransition: all 0.15s ease-out 0.1s;\n\topacity: 0.2;\n\tcursor: pointer;\n\tdisplay: block;\n\tz-index: 3;\n}\n.instagram .album div.img-box .icon {\n    font-size: 14px;\n    position: absolute;\n    left: 50%;\n    top: 50%;\n    margin-left: -7px;\n    margin-top: -7px;\n    color: #999;\n    z-index: 1;\n}\n.instagram .album div.img-box .img-bg:hover{\n\topacity: 0;\n}\n.photos-btn-wrap {\n\tborder-bottom: 1px solid #e5e5e5;\n\tmargin-bottom: 20px;\n}\n.photos-btn {\n\tfont-size: 16px;\n\tcolor: #333;\n\tmargin-bottom: -4px;\n\tpadding: 5px 8px 3px;\n}\n.photos-btn.active {\n\tcolor: #08c;\n\tborder: 1px solid #e5e5e5;\n\tborder-bottom: 5px solid #fff;\n}\n\n@media screen and (max-width:600px) {\n\t.instagram .thumb {\n\t\twidth: 50%;\n\t\tpadding-bottom: 50%;\n\t}\n\t.instagram .album li {\n\t\twidth: 100%;\n\t\tposition: relative;\n\t\tpadding-bottom: 100%;\n\t\ttext-align: center;\n\t}\n\t.instagram .album div.img-box{\n\t\tmargin: 0;\n\t\twidth: 90%;\n\t\theight: 90%;\n\t}\n}"},{"_content":"/******/\n(function(modules) { // webpackBootstrap\n  /******/ // The module cache\n  /******/\n  var installedModules = {};\n  /******/\n  /******/ // The require function\n  /******/\n  function __webpack_require__(moduleId) {\n    /******/\n    /******/ // Check if module is in cache\n    /******/\n    if (installedModules[moduleId])\n    /******/\n      return installedModules[moduleId].exports;\n    /******/\n    /******/ // Create a new module (and put it into the cache)\n    /******/\n    var module = installedModules[moduleId] = {\n      /******/\n      exports: {},\n      /******/\n      id: moduleId,\n      /******/\n      loaded: false\n        /******/\n    };\n    /******/\n    /******/ // Execute the module function\n    /******/\n    modules[moduleId].call(module.exports, module, module.exports, __webpack_require__);\n    /******/\n    /******/ // Flag the module as loaded\n    /******/\n    module.loaded = true;\n    /******/\n    /******/ // Return the exports of the module\n    /******/\n    return module.exports;\n    /******/\n  }\n  /******/\n  /******/\n  /******/ // expose the modules object (__webpack_modules__)\n  /******/\n  __webpack_require__.m = modules;\n  /******/\n  /******/ // expose the module cache\n  /******/\n  __webpack_require__.c = installedModules;\n  /******/\n  /******/ // __webpack_public_path__\n  /******/\n  __webpack_require__.p = \"/dist/\";\n  /******/\n  /******/ // Load entry module and return exports\n  /******/\n  return __webpack_require__(0);\n  /******/\n})\n/************************************************************************/\n/******/\n([\n  /* 0 */\n  /***/\n  function(module, exports, __webpack_require__) {\n\n    'use strict';\n\n    __webpack_require__(1);\n\n    var _view = __webpack_require__(2);\n\n    var _view2 = _interopRequireDefault(_view);\n\n    function _interopRequireDefault(obj) {\n      return obj && obj.__esModule ? obj : {\n        default: obj\n      };\n    }\n\n    /**\n     * @name impush-client \n     * @description 这个项目让我发家致富…\n     * @date 2016-12-1\n     */\n\n    var _collection = [];\n    var _count = 0;\n    var searchData;\n\n    function addMask(elem) {\n      var rect = elem.getBoundingClientRect();\n      var style = getComputedStyle(elem, null);\n\n      var mask = document.createElement('i');\n      mask.className = 'icon-film';\n      mask.style.color = '#fff';\n      mask.style.fontSize = '26px';\n      mask.style.position = 'absolute';\n      mask.style.right = '10px';\n      mask.style.bottom = '10px';\n      mask.style.zIndex = 1;\n      elem.parentNode.appendChild(mask);\n    }\n\n    var createVideoIncon = function createVideoIncon() {\n      var $videoImg = document.querySelectorAll('.thumb a[data-type=\"video\"]');\n      for (var i = 0, len = $videoImg.length; i < len; i++) {\n        addMask($videoImg[i]);\n      }\n    };\n    var render = function render(res) {\n      var ulTmpl = \"\";\n      for (var j = 0, len2 = res.list.length; j < len2; j++) {\n        var data = res.list[j].arr;\n        var liTmpl = \"\";\n        for (var i = 0, len = data.link.length; i < len; i++) {\n          // var minSrc = 'https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/min_photos/' + data.link[i] + '.min.jpg';\n          var minSrc = 'https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/min_photos/' + data.link[i];\n          var src = 'https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/photos/' + data.link[i];\n          var type = data.type[i];\n          var target = src + (type === 'video' ? '.mp4' : '.jpg');\n          // src += '.jpg';\n\n          liTmpl += '<figure class=\"thumb\" itemprop=\"associatedMedia\" itemscope=\"\" itemtype=\"http://schema.org/ImageObject\">\\\n                <a href=\"' + src + '\" itemprop=\"contentUrl\" data-size=\"640x640\" data-type=\"' + type + '\" data-target=\"' + target + '\">\\\n                  <img class=\"reward-img\" data-type=\"' + type + '\" data-src=\"' + minSrc + '\" src=\"/assets/img/head.jpg\" itemprop=\"thumbnail\" onload=\"lzld(this)\">\\\n                </a>\\\n                <figcaption style=\"display:none\" itemprop=\"caption description\">' + data.text[i] + '</figcaption>\\\n            </figure>';\n        }\n        ulTmpl = ulTmpl + '<section class=\"archives album\"><h1 class=\"year\">' + data.year + '<em>' + data.month + '月</em></h1>\\\n        <ul class=\"img-box-ul\">' + liTmpl + '</ul>\\\n        </section>';\n      }\n      document.querySelector('.instagram').innerHTML = '<div class=\"photos\" itemscope=\"\" itemtype=\"http://schema.org/ImageGallery\">' + ulTmpl + '</div>';\n      createVideoIncon();\n      _view2.default.init();\n    };\n\n    var replacer = function replacer(str) {\n      var arr = str.split(\"/\");\n      return \"/assets/ins/\" + arr[arr.length - 1];\n    };\n\n    var ctrler = function ctrler(data) {\n      var imgObj = {};\n      for (var i = 0, len = data.length; i < len; i++) {\n        var y = data[i].y;\n        var m = data[i].m;\n        var src = replacer(data[i].src);\n        var text = data[i].text;\n        var key = y + \"\" + ((m + \"\").length == 1 ? \"0\" + m : m);\n        if (imgObj[key]) {\n          imgObj[key].srclist.push(src);\n          imgObj[key].text.push(text);\n        } else {\n          imgObj[key] = {\n            year: y,\n            month: m,\n            srclist: [src],\n            text: [text]\n          };\n        }\n      }\n      render(imgObj);\n    };\n\n    function loadData(success) {\n      if (!searchData) {\n        var xhr = new XMLHttpRequest();\n        xhr.open('GET', './ins.json?t=' + +new Date(), true);\n\n        xhr.onload = function() {\n          if (this.status >= 200 && this.status < 300) {\n            var res = JSON.parse(this.response);\n            searchData = res;\n            success(searchData);\n          } else {\n            console.error(this.statusText);\n          }\n        };\n\n        xhr.onerror = function() {\n          console.error(this.statusText);\n        };\n\n        xhr.send();\n      } else {\n        success(searchData);\n      }\n    }\n\n    var Ins = {\n      init: function init() {\n        loadData(function(data) {\n          render(data);\n        });\n      }\n    };\n\n    Ins.init();\n\n    // export default impush;\n\n    /***/\n  },\n  /* 1 */\n  /***/\n  function(module, exports, __webpack_require__) {\n\n    /* WEBPACK VAR INJECTION */\n    (function(global) {\n      'use strict';\n\n      var inViewport = __webpack_require__(3);\n      var lazyAttrs = ['data-src'];\n\n      global.lzld = lazyload();\n\n      // Provide libs using getAttribute early to get the good src\n      // and not the fake data-src\n      replaceGetAttribute('Image');\n      replaceGetAttribute('IFrame');\n\n      function registerLazyAttr(attr) {\n        if (indexOf.call(lazyAttrs, attr) === -1) {\n          lazyAttrs.push(attr);\n        }\n      }\n\n      function lazyload(opts) {\n        opts = merge({\n          'offset': 333,\n          'src': 'data-src',\n          'container': false\n        }, opts || {});\n\n        if (typeof opts.src === 'string') {\n          registerLazyAttr(opts.src);\n        }\n\n        var elts = [];\n\n        function show(elt) {\n          var src = findRealSrc(elt);\n\n          if (src) {\n            elt.src = src;\n          }\n\n          elt.setAttribute('data-lzled', true);\n          elts[indexOf.call(elts, elt)] = null;\n        }\n\n        function findRealSrc(elt) {\n          if (typeof opts.src === 'function') {\n            return opts.src(elt);\n          }\n\n          return elt.getAttribute(opts.src);\n        }\n\n        function register(elt) {\n          elt.onload = null;\n          elt.removeAttribute('onload');\n          elt.onerror = null;\n          elt.removeAttribute('onerror');\n\n          if (indexOf.call(elts, elt) === -1) {\n            inViewport(elt, opts, show);\n          }\n        }\n\n        return register;\n      }\n\n      function replaceGetAttribute(elementName) {\n        var fullname = 'HTML' + elementName + 'Element';\n        if (fullname in global === false) {\n          return;\n        }\n\n        var original = global[fullname].prototype.getAttribute;\n        global[fullname].prototype.getAttribute = function(name) {\n          if (name === 'src') {\n            var realSrc;\n            for (var i = 0, max = lazyAttrs.length; i < max; i++) {\n              realSrc = original.call(this, lazyAttrs[i]);\n              if (realSrc) {\n                break;\n              }\n            }\n\n            return realSrc || original.call(this, name);\n          }\n\n          // our own lazyloader will go through theses lines\n          // because we use getAttribute(opts.src)\n          return original.call(this, name);\n        };\n      }\n\n      function merge(defaults, opts) {\n        for (var name in defaults) {\n          if (opts[name] === undefined) {\n            opts[name] = defaults[name];\n          }\n        }\n\n        return opts;\n      }\n\n      // http://webreflection.blogspot.fr/2011/06/partial-polyfills.html\n      function indexOf(value) {\n        for (var i = this.length; i-- && this[i] !== value;) {}\n        return i;\n      }\n\n      module.exports = lazyload;\n\n      // export default impush;\n      /* WEBPACK VAR INJECTION */\n    }.call(exports, (function() {\n      return this;\n    }())))\n\n    /***/\n  },\n  /* 2 */\n  /***/\n  function(module, exports) {\n\n    'use strict';\n\n    var initPhotoSwipeFromDOM = function initPhotoSwipeFromDOM(gallerySelector) {\n\n      // parse slide data (url, title, size ...) from DOM elements \n      // (children of gallerySelector)\n      var parseThumbnailElements = function parseThumbnailElements(el) {\n        el = el.parentNode.parentNode;\n        var thumbElements = el.getElementsByClassName('thumb'),\n          numNodes = thumbElements.length,\n          items = [],\n          figureEl,\n          linkEl,\n          size,\n          type,\n          // video or not\n          target,\n          item;\n\n        for (var i = 0; i < numNodes; i++) {\n\n          figureEl = thumbElements[i]; // \n\n          // include only element nodes \n          if (figureEl.nodeType !== 1) {\n            continue;\n          }\n\n          linkEl = figureEl.children[0]; // \n\n          size = linkEl.getAttribute('data-size').split('x');\n          type = linkEl.getAttribute('data-type');\n          target = linkEl.getAttribute('data-target');\n          // create slide object\n          item = {\n            src: linkEl.getAttribute('href'),\n            w: parseInt(size[0], 10),\n            h: parseInt(size[1], 10)\n          };\n\n          if (figureEl.children.length > 1) {\n            item.title = figureEl.children[1].innerHTML;\n          }\n\n          if (linkEl.children.length > 0) {\n            item.msrc = linkEl.children[0].getAttribute('src');\n            item.type = type;\n            item.target = target;\n            item.html = '<video src=\"' + target + '\" controls=\"controls\" autoplay=\"autoplay\"></video>';\n            if (type === 'video') {\n              //item.src = null;\n            }\n          }\n\n          item.el = figureEl; // save link to element for getThumbBoundsFn\n          items.push(item);\n        }\n\n        return items;\n      };\n\n      // find nearest parent element\n      var closest = function closest(el, fn) {\n        return el && (fn(el) ? el : closest(el.parentNode, fn));\n      };\n\n      // triggers when user clicks on thumbnail\n      var onThumbnailsClick = function onThumbnailsClick(e) {\n        e = e || window.event;\n        e.preventDefault ? e.preventDefault() : e.returnValue = false;\n\n        var eTarget = e.target || e.srcElement;\n\n        // find root element of slide\n        var clickedListItem = closest(eTarget, function(el) {\n          return el.tagName && el.tagName.toUpperCase() === 'FIGURE';\n        });\n\n        if (!clickedListItem) {\n          return;\n        }\n\n        // find index of clicked item by looping through all child nodes\n        // alternatively, you may define index via data- attribute\n        var clickedGallery = clickedListItem.parentNode,\n\n          // childNodes = clickedListItem.parentNode.childNodes,\n          // numChildNodes = childNodes.length,\n          childNodes = document.getElementsByClassName('thumb'),\n          numChildNodes = childNodes.length,\n          nodeIndex = 0,\n          index;\n\n        for (var i = 0; i < numChildNodes; i++) {\n          if (childNodes[i].nodeType !== 1) {\n            continue;\n          }\n\n          if (childNodes[i] === clickedListItem) {\n            index = nodeIndex;\n            break;\n          }\n          nodeIndex++;\n        }\n\n        if (index >= 0) {\n          // open PhotoSwipe if valid index found\n          openPhotoSwipe(index, clickedGallery);\n        }\n        return false;\n      };\n\n      // parse picture index and gallery index from URL (#&pid=1&gid=2)\n      var photoswipeParseHash = function photoswipeParseHash() {\n        var hash = window.location.hash.substring(1),\n          params = {};\n\n        if (hash.length < 5) {\n          return params;\n        }\n\n        var vars = hash.split('&');\n        for (var i = 0; i < vars.length; i++) {\n          if (!vars[i]) {\n            continue;\n          }\n          var pair = vars[i].split('=');\n          if (pair.length < 2) {\n            continue;\n          }\n          params[pair[0]] = pair[1];\n        }\n\n        if (params.gid) {\n          params.gid = parseInt(params.gid, 10);\n        }\n\n        return params;\n      };\n\n      var openPhotoSwipe = function openPhotoSwipe(index, galleryElement, disableAnimation, fromURL) {\n        var pswpElement = document.querySelectorAll('.pswp')[0],\n          gallery,\n          options,\n          items;\n\n        items = parseThumbnailElements(galleryElement);\n        // define options (if needed)\n        options = {\n\n          // define gallery index (for URL)\n          galleryUID: galleryElement.getAttribute('data-pswp-uid'),\n\n          getThumbBoundsFn: function getThumbBoundsFn(index) {\n            // See Options -> getThumbBoundsFn section of documentation for more info\n            var thumbnail = items[index].el.getElementsByTagName('img')[0],\n              // find thumbnail\n              pageYScroll = window.pageYOffset || document.documentElement.scrollTop,\n              rect = thumbnail.getBoundingClientRect();\n\n            return {\n              x: rect.left,\n              y: rect.top + pageYScroll,\n              w: rect.width\n            };\n          }\n\n        };\n\n        // PhotoSwipe opened from URL\n        if (fromURL) {\n          if (options.galleryPIDs) {\n            // parse real index when custom PIDs are used \n            // http://photoswipe.com/documentation/faq.html#custom-pid-in-url\n            for (var j = 0; j < items.length; j++) {\n              if (items[j].pid == index) {\n                options.index = j;\n                break;\n              }\n            }\n          } else {\n            // in URL indexes start from 1\n            options.index = parseInt(index, 10) - 1;\n          }\n        } else {\n          options.index = parseInt(index, 10);\n        }\n\n        // exit if index not found\n        if (isNaN(options.index)) {\n          return;\n        }\n\n        if (disableAnimation) {\n          options.showAnimationDuration = 0;\n        }\n\n        // Pass data to PhotoSwipe and initialize it\n        gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, options);\n        gallery.init();\n\n        var $tempVideo;\n        var stopVideoHandle = function stopVideoHandle() {\n          if ($tempVideo) {\n            $tempVideo.remove();\n            $tempVideo = null;\n          }\n        };\n        var changeHandle = function changeHandle() {\n          var item = gallery.currItem;\n          stopVideoHandle();\n          if (item.type === 'video') {\n            var $ctn = item.container;\n            var style = $ctn.getElementsByClassName('pswp__img')[0].style;\n            var $video = document.createElement('video');\n            $video.setAttribute('autoplay', 'autoplay');\n            $video.setAttribute('controls', 'controls');\n            $video.setAttribute('src', item.target);\n            $video.style.width = style.width;\n            $video.style.height = style.height;\n            $video.style.position = 'absolute';\n            $video.style.zIndex = 2;\n            $tempVideo = $video;\n            $ctn.appendChild($video);\n          }\n        };\n        gallery.listen('initialZoomIn', changeHandle);\n        gallery.listen('afterChange', changeHandle);\n        gallery.listen('initialZoomOut', stopVideoHandle);\n      };\n\n      // loop through all gallery elements and bind events\n      var galleryElements = document.querySelectorAll(gallerySelector);\n      for (var i = 0, l = galleryElements.length; i < l; i++) {\n        galleryElements[i].setAttribute('data-pswp-uid', i + 1);\n        galleryElements[i].onclick = onThumbnailsClick;\n      }\n\n      // Parse URL and open gallery if it contains #&pid=3&gid=1\n      var hashData = photoswipeParseHash();\n      if (hashData.pid && hashData.gid) {\n        openPhotoSwipe(hashData.pid, galleryElements[hashData.gid - 1], true, true);\n      }\n    };\n\n    var Viewer = function() {\n      function init() {\n        initPhotoSwipeFromDOM('.photos');\n      }\n      return {\n        init: init\n      };\n    }();\n\n    module.exports = Viewer;\n\n    /***/\n  },\n  /* 3 */\n  /***/\n  function(module, exports) {\n\n    /* WEBPACK VAR INJECTION */\n    (function(global) {\n      module.exports = inViewport;\n\n      var instances = [];\n      var supportsMutationObserver = typeof global.MutationObserver === 'function';\n\n      function inViewport(elt, params, cb) {\n        var opts = {\n          container: global.document.body,\n          offset: 0\n        };\n\n        if (params === undefined || typeof params === 'function') {\n          cb = params;\n          params = {};\n        }\n\n        var container = opts.container = params.container || opts.container;\n        var offset = opts.offset = params.offset || opts.offset;\n\n        for (var i = 0; i < instances.length; i++) {\n          if (instances[i].container === container) {\n            return instances[i].isInViewport(elt, offset, cb);\n          }\n        }\n\n        return instances[\n          instances.push(createInViewport(container)) - 1\n        ].isInViewport(elt, offset, cb);\n      }\n\n      function addEvent(el, type, fn) {\n        if (el.attachEvent) {\n          el.attachEvent('on' + type, fn);\n        } else {\n          el.addEventListener(type, fn, false);\n        }\n      }\n\n      function debounce(func, wait, immediate) {\n        var timeout;\n        return function() {\n          var context = this,\n            args = arguments;\n          var callNow = immediate && !timeout;\n          clearTimeout(timeout);\n          timeout = setTimeout(later, wait);\n          if (callNow) func.apply(context, args);\n\n          function later() {\n            timeout = null;\n            if (!immediate) func.apply(context, args);\n          }\n        };\n      }\n\n      // https://github.com/jquery/sizzle/blob/3136f48b90e3edc84cbaaa6f6f7734ef03775a07/sizzle.js#L708\n      var contains = function() {\n        if (!global.document) {\n          return true;\n        }\n        return global.document.documentElement.compareDocumentPosition ?\n          function(a, b) {\n            return !!(a.compareDocumentPosition(b) & 16);\n          } :\n          global.document.documentElement.contains ?\n          function(a, b) {\n            return a !== b && (a.contains ? a.contains(b) : false);\n          } :\n          function(a, b) {\n            while (b = b.parentNode) {\n              if (b === a) {\n                return true;\n              }\n            }\n            return false;\n          };\n      }\n\n      function createInViewport(container) {\n        var watches = createWatches();\n\n        var scrollContainer = container === global.document.body ? global : container;\n        var debouncedCheck = debounce(watches.checkAll(watchInViewport), 15);\n\n        addEvent(scrollContainer, 'scroll', debouncedCheck);\n\n        if (scrollContainer === global) {\n          addEvent(global, 'resize', debouncedCheck);\n        }\n\n        if (supportsMutationObserver) {\n          observeDOM(watches, container, debouncedCheck);\n        }\n\n        // failsafe check, every 200ms we check for visible images\n        // usecase: a hidden parent containing eleements\n        // when the parent becomes visible, we have no event that the children\n        // became visible\n        setInterval(debouncedCheck, 150);\n\n        function isInViewport(elt, offset, cb) {\n          if (!cb) {\n            return isVisible(elt, offset);\n          }\n\n          var remote = createRemote(elt, offset, cb);\n          remote.watch();\n          return remote;\n        }\n\n        function createRemote(elt, offset, cb) {\n          function watch() {\n            watches.add(elt, offset, cb);\n          }\n\n          function dispose() {\n            watches.remove(elt);\n          }\n\n          return {\n            watch: watch,\n            dispose: dispose\n          };\n        }\n\n        function watchInViewport(elt, offset, cb) {\n          if (isVisible(elt, offset)) {\n            watches.remove(elt);\n            cb(elt);\n          }\n        }\n\n        function isVisible(elt, offset) {\n          if (!contains(global.document.documentElement, elt) || !contains(global.document.documentElement, container)) {\n            return false;\n          }\n\n          // Check if the element is visible\n          // https://github.com/jquery/jquery/blob/740e190223d19a114d5373758127285d14d6b71e/src/css/hiddenVisibleSelectors.js\n          if (!elt.offsetWidth || !elt.offsetHeight) {\n            return false;\n          }\n\n          var eltRect = elt.getBoundingClientRect();\n          var viewport = {};\n\n          if (container === global.document.body) {\n            viewport = {\n              top: -offset,\n              left: -offset,\n              right: global.document.documentElement.clientWidth + offset,\n              bottom: global.document.documentElement.clientHeight + offset\n            };\n          } else {\n            var containerRect = container.getBoundingClientRect();\n            viewport = {\n              top: containerRect.top - offset,\n              left: containerRect.left - offset,\n              right: containerRect.right + offset,\n              bottom: containerRect.bottom + offset\n            };\n          }\n\n          // The element must overlap with the visible part of the viewport\n          var visible =\n            (\n              (eltRect.right > viewport.left) &&\n              (eltRect.left < viewport.right) &&\n              (eltRect.bottom > viewport.top) &&\n              (eltRect.top < viewport.bottom)\n            );\n\n          return visible;\n        }\n\n        return {\n          container: container,\n          isInViewport: isInViewport\n        };\n      }\n\n      function createWatches() {\n        var watches = [];\n\n        function add(elt, offset, cb) {\n          if (!isWatched(elt)) {\n            watches.push([elt, offset, cb]);\n          }\n        }\n\n        function remove(elt) {\n          var pos = indexOf(elt);\n          if (pos !== -1) {\n            watches.splice(pos, 1);\n          }\n        }\n\n        function indexOf(elt) {\n          for (var i = watches.length - 1; i >= 0; i--) {\n            if (watches[i][0] === elt) {\n              return i;\n            }\n          }\n          return -1;\n        }\n\n        function isWatched(elt) {\n          return indexOf(elt) !== -1;\n        }\n\n        function checkAll(cb) {\n          return function() {\n            for (var i = watches.length - 1; i >= 0; i--) {\n              cb.apply(this, watches[i]);\n            }\n          };\n        }\n\n        return {\n          add: add,\n          remove: remove,\n          isWatched: isWatched,\n          checkAll: checkAll\n        };\n      }\n\n      function observeDOM(watches, container, cb) {\n        var observer = new MutationObserver(watch);\n        var filter = Array.prototype.filter;\n        var concat = Array.prototype.concat;\n\n        observer.observe(container, {\n          childList: true,\n          subtree: true,\n          // changes like style/width/height/display will be catched\n          attributes: true\n        });\n\n        function watch(mutations) {\n          // some new DOM nodes where previously watched\n          // we should check their positions\n          if (mutations.some(knownNodes) === true) {\n            setTimeout(cb, 0);\n          }\n        }\n\n        function knownNodes(mutation) {\n          var nodes = concat.call([],\n            Array.prototype.slice.call(mutation.addedNodes),\n            mutation.target\n          );\n          return filter.call(nodes, watches.isWatched).length > 0;\n        }\n      }\n\n      /* WEBPACK VAR INJECTION */\n    }.call(exports, (function() {\n      return this;\n    }())))\n\n    /***/\n  }\n  /******/\n]);","source":"photos/ins.js","raw":"/******/\n(function(modules) { // webpackBootstrap\n  /******/ // The module cache\n  /******/\n  var installedModules = {};\n  /******/\n  /******/ // The require function\n  /******/\n  function __webpack_require__(moduleId) {\n    /******/\n    /******/ // Check if module is in cache\n    /******/\n    if (installedModules[moduleId])\n    /******/\n      return installedModules[moduleId].exports;\n    /******/\n    /******/ // Create a new module (and put it into the cache)\n    /******/\n    var module = installedModules[moduleId] = {\n      /******/\n      exports: {},\n      /******/\n      id: moduleId,\n      /******/\n      loaded: false\n        /******/\n    };\n    /******/\n    /******/ // Execute the module function\n    /******/\n    modules[moduleId].call(module.exports, module, module.exports, __webpack_require__);\n    /******/\n    /******/ // Flag the module as loaded\n    /******/\n    module.loaded = true;\n    /******/\n    /******/ // Return the exports of the module\n    /******/\n    return module.exports;\n    /******/\n  }\n  /******/\n  /******/\n  /******/ // expose the modules object (__webpack_modules__)\n  /******/\n  __webpack_require__.m = modules;\n  /******/\n  /******/ // expose the module cache\n  /******/\n  __webpack_require__.c = installedModules;\n  /******/\n  /******/ // __webpack_public_path__\n  /******/\n  __webpack_require__.p = \"/dist/\";\n  /******/\n  /******/ // Load entry module and return exports\n  /******/\n  return __webpack_require__(0);\n  /******/\n})\n/************************************************************************/\n/******/\n([\n  /* 0 */\n  /***/\n  function(module, exports, __webpack_require__) {\n\n    'use strict';\n\n    __webpack_require__(1);\n\n    var _view = __webpack_require__(2);\n\n    var _view2 = _interopRequireDefault(_view);\n\n    function _interopRequireDefault(obj) {\n      return obj && obj.__esModule ? obj : {\n        default: obj\n      };\n    }\n\n    /**\n     * @name impush-client \n     * @description 这个项目让我发家致富…\n     * @date 2016-12-1\n     */\n\n    var _collection = [];\n    var _count = 0;\n    var searchData;\n\n    function addMask(elem) {\n      var rect = elem.getBoundingClientRect();\n      var style = getComputedStyle(elem, null);\n\n      var mask = document.createElement('i');\n      mask.className = 'icon-film';\n      mask.style.color = '#fff';\n      mask.style.fontSize = '26px';\n      mask.style.position = 'absolute';\n      mask.style.right = '10px';\n      mask.style.bottom = '10px';\n      mask.style.zIndex = 1;\n      elem.parentNode.appendChild(mask);\n    }\n\n    var createVideoIncon = function createVideoIncon() {\n      var $videoImg = document.querySelectorAll('.thumb a[data-type=\"video\"]');\n      for (var i = 0, len = $videoImg.length; i < len; i++) {\n        addMask($videoImg[i]);\n      }\n    };\n    var render = function render(res) {\n      var ulTmpl = \"\";\n      for (var j = 0, len2 = res.list.length; j < len2; j++) {\n        var data = res.list[j].arr;\n        var liTmpl = \"\";\n        for (var i = 0, len = data.link.length; i < len; i++) {\n          // var minSrc = 'https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/min_photos/' + data.link[i] + '.min.jpg';\n          var minSrc = 'https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/min_photos/' + data.link[i];\n          var src = 'https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/photos/' + data.link[i];\n          var type = data.type[i];\n          var target = src + (type === 'video' ? '.mp4' : '.jpg');\n          // src += '.jpg';\n\n          liTmpl += '<figure class=\"thumb\" itemprop=\"associatedMedia\" itemscope=\"\" itemtype=\"http://schema.org/ImageObject\">\\\n                <a href=\"' + src + '\" itemprop=\"contentUrl\" data-size=\"640x640\" data-type=\"' + type + '\" data-target=\"' + target + '\">\\\n                  <img class=\"reward-img\" data-type=\"' + type + '\" data-src=\"' + minSrc + '\" src=\"/assets/img/head.jpg\" itemprop=\"thumbnail\" onload=\"lzld(this)\">\\\n                </a>\\\n                <figcaption style=\"display:none\" itemprop=\"caption description\">' + data.text[i] + '</figcaption>\\\n            </figure>';\n        }\n        ulTmpl = ulTmpl + '<section class=\"archives album\"><h1 class=\"year\">' + data.year + '<em>' + data.month + '月</em></h1>\\\n        <ul class=\"img-box-ul\">' + liTmpl + '</ul>\\\n        </section>';\n      }\n      document.querySelector('.instagram').innerHTML = '<div class=\"photos\" itemscope=\"\" itemtype=\"http://schema.org/ImageGallery\">' + ulTmpl + '</div>';\n      createVideoIncon();\n      _view2.default.init();\n    };\n\n    var replacer = function replacer(str) {\n      var arr = str.split(\"/\");\n      return \"/assets/ins/\" + arr[arr.length - 1];\n    };\n\n    var ctrler = function ctrler(data) {\n      var imgObj = {};\n      for (var i = 0, len = data.length; i < len; i++) {\n        var y = data[i].y;\n        var m = data[i].m;\n        var src = replacer(data[i].src);\n        var text = data[i].text;\n        var key = y + \"\" + ((m + \"\").length == 1 ? \"0\" + m : m);\n        if (imgObj[key]) {\n          imgObj[key].srclist.push(src);\n          imgObj[key].text.push(text);\n        } else {\n          imgObj[key] = {\n            year: y,\n            month: m,\n            srclist: [src],\n            text: [text]\n          };\n        }\n      }\n      render(imgObj);\n    };\n\n    function loadData(success) {\n      if (!searchData) {\n        var xhr = new XMLHttpRequest();\n        xhr.open('GET', './ins.json?t=' + +new Date(), true);\n\n        xhr.onload = function() {\n          if (this.status >= 200 && this.status < 300) {\n            var res = JSON.parse(this.response);\n            searchData = res;\n            success(searchData);\n          } else {\n            console.error(this.statusText);\n          }\n        };\n\n        xhr.onerror = function() {\n          console.error(this.statusText);\n        };\n\n        xhr.send();\n      } else {\n        success(searchData);\n      }\n    }\n\n    var Ins = {\n      init: function init() {\n        loadData(function(data) {\n          render(data);\n        });\n      }\n    };\n\n    Ins.init();\n\n    // export default impush;\n\n    /***/\n  },\n  /* 1 */\n  /***/\n  function(module, exports, __webpack_require__) {\n\n    /* WEBPACK VAR INJECTION */\n    (function(global) {\n      'use strict';\n\n      var inViewport = __webpack_require__(3);\n      var lazyAttrs = ['data-src'];\n\n      global.lzld = lazyload();\n\n      // Provide libs using getAttribute early to get the good src\n      // and not the fake data-src\n      replaceGetAttribute('Image');\n      replaceGetAttribute('IFrame');\n\n      function registerLazyAttr(attr) {\n        if (indexOf.call(lazyAttrs, attr) === -1) {\n          lazyAttrs.push(attr);\n        }\n      }\n\n      function lazyload(opts) {\n        opts = merge({\n          'offset': 333,\n          'src': 'data-src',\n          'container': false\n        }, opts || {});\n\n        if (typeof opts.src === 'string') {\n          registerLazyAttr(opts.src);\n        }\n\n        var elts = [];\n\n        function show(elt) {\n          var src = findRealSrc(elt);\n\n          if (src) {\n            elt.src = src;\n          }\n\n          elt.setAttribute('data-lzled', true);\n          elts[indexOf.call(elts, elt)] = null;\n        }\n\n        function findRealSrc(elt) {\n          if (typeof opts.src === 'function') {\n            return opts.src(elt);\n          }\n\n          return elt.getAttribute(opts.src);\n        }\n\n        function register(elt) {\n          elt.onload = null;\n          elt.removeAttribute('onload');\n          elt.onerror = null;\n          elt.removeAttribute('onerror');\n\n          if (indexOf.call(elts, elt) === -1) {\n            inViewport(elt, opts, show);\n          }\n        }\n\n        return register;\n      }\n\n      function replaceGetAttribute(elementName) {\n        var fullname = 'HTML' + elementName + 'Element';\n        if (fullname in global === false) {\n          return;\n        }\n\n        var original = global[fullname].prototype.getAttribute;\n        global[fullname].prototype.getAttribute = function(name) {\n          if (name === 'src') {\n            var realSrc;\n            for (var i = 0, max = lazyAttrs.length; i < max; i++) {\n              realSrc = original.call(this, lazyAttrs[i]);\n              if (realSrc) {\n                break;\n              }\n            }\n\n            return realSrc || original.call(this, name);\n          }\n\n          // our own lazyloader will go through theses lines\n          // because we use getAttribute(opts.src)\n          return original.call(this, name);\n        };\n      }\n\n      function merge(defaults, opts) {\n        for (var name in defaults) {\n          if (opts[name] === undefined) {\n            opts[name] = defaults[name];\n          }\n        }\n\n        return opts;\n      }\n\n      // http://webreflection.blogspot.fr/2011/06/partial-polyfills.html\n      function indexOf(value) {\n        for (var i = this.length; i-- && this[i] !== value;) {}\n        return i;\n      }\n\n      module.exports = lazyload;\n\n      // export default impush;\n      /* WEBPACK VAR INJECTION */\n    }.call(exports, (function() {\n      return this;\n    }())))\n\n    /***/\n  },\n  /* 2 */\n  /***/\n  function(module, exports) {\n\n    'use strict';\n\n    var initPhotoSwipeFromDOM = function initPhotoSwipeFromDOM(gallerySelector) {\n\n      // parse slide data (url, title, size ...) from DOM elements \n      // (children of gallerySelector)\n      var parseThumbnailElements = function parseThumbnailElements(el) {\n        el = el.parentNode.parentNode;\n        var thumbElements = el.getElementsByClassName('thumb'),\n          numNodes = thumbElements.length,\n          items = [],\n          figureEl,\n          linkEl,\n          size,\n          type,\n          // video or not\n          target,\n          item;\n\n        for (var i = 0; i < numNodes; i++) {\n\n          figureEl = thumbElements[i]; // \n\n          // include only element nodes \n          if (figureEl.nodeType !== 1) {\n            continue;\n          }\n\n          linkEl = figureEl.children[0]; // \n\n          size = linkEl.getAttribute('data-size').split('x');\n          type = linkEl.getAttribute('data-type');\n          target = linkEl.getAttribute('data-target');\n          // create slide object\n          item = {\n            src: linkEl.getAttribute('href'),\n            w: parseInt(size[0], 10),\n            h: parseInt(size[1], 10)\n          };\n\n          if (figureEl.children.length > 1) {\n            item.title = figureEl.children[1].innerHTML;\n          }\n\n          if (linkEl.children.length > 0) {\n            item.msrc = linkEl.children[0].getAttribute('src');\n            item.type = type;\n            item.target = target;\n            item.html = '<video src=\"' + target + '\" controls=\"controls\" autoplay=\"autoplay\"></video>';\n            if (type === 'video') {\n              //item.src = null;\n            }\n          }\n\n          item.el = figureEl; // save link to element for getThumbBoundsFn\n          items.push(item);\n        }\n\n        return items;\n      };\n\n      // find nearest parent element\n      var closest = function closest(el, fn) {\n        return el && (fn(el) ? el : closest(el.parentNode, fn));\n      };\n\n      // triggers when user clicks on thumbnail\n      var onThumbnailsClick = function onThumbnailsClick(e) {\n        e = e || window.event;\n        e.preventDefault ? e.preventDefault() : e.returnValue = false;\n\n        var eTarget = e.target || e.srcElement;\n\n        // find root element of slide\n        var clickedListItem = closest(eTarget, function(el) {\n          return el.tagName && el.tagName.toUpperCase() === 'FIGURE';\n        });\n\n        if (!clickedListItem) {\n          return;\n        }\n\n        // find index of clicked item by looping through all child nodes\n        // alternatively, you may define index via data- attribute\n        var clickedGallery = clickedListItem.parentNode,\n\n          // childNodes = clickedListItem.parentNode.childNodes,\n          // numChildNodes = childNodes.length,\n          childNodes = document.getElementsByClassName('thumb'),\n          numChildNodes = childNodes.length,\n          nodeIndex = 0,\n          index;\n\n        for (var i = 0; i < numChildNodes; i++) {\n          if (childNodes[i].nodeType !== 1) {\n            continue;\n          }\n\n          if (childNodes[i] === clickedListItem) {\n            index = nodeIndex;\n            break;\n          }\n          nodeIndex++;\n        }\n\n        if (index >= 0) {\n          // open PhotoSwipe if valid index found\n          openPhotoSwipe(index, clickedGallery);\n        }\n        return false;\n      };\n\n      // parse picture index and gallery index from URL (#&pid=1&gid=2)\n      var photoswipeParseHash = function photoswipeParseHash() {\n        var hash = window.location.hash.substring(1),\n          params = {};\n\n        if (hash.length < 5) {\n          return params;\n        }\n\n        var vars = hash.split('&');\n        for (var i = 0; i < vars.length; i++) {\n          if (!vars[i]) {\n            continue;\n          }\n          var pair = vars[i].split('=');\n          if (pair.length < 2) {\n            continue;\n          }\n          params[pair[0]] = pair[1];\n        }\n\n        if (params.gid) {\n          params.gid = parseInt(params.gid, 10);\n        }\n\n        return params;\n      };\n\n      var openPhotoSwipe = function openPhotoSwipe(index, galleryElement, disableAnimation, fromURL) {\n        var pswpElement = document.querySelectorAll('.pswp')[0],\n          gallery,\n          options,\n          items;\n\n        items = parseThumbnailElements(galleryElement);\n        // define options (if needed)\n        options = {\n\n          // define gallery index (for URL)\n          galleryUID: galleryElement.getAttribute('data-pswp-uid'),\n\n          getThumbBoundsFn: function getThumbBoundsFn(index) {\n            // See Options -> getThumbBoundsFn section of documentation for more info\n            var thumbnail = items[index].el.getElementsByTagName('img')[0],\n              // find thumbnail\n              pageYScroll = window.pageYOffset || document.documentElement.scrollTop,\n              rect = thumbnail.getBoundingClientRect();\n\n            return {\n              x: rect.left,\n              y: rect.top + pageYScroll,\n              w: rect.width\n            };\n          }\n\n        };\n\n        // PhotoSwipe opened from URL\n        if (fromURL) {\n          if (options.galleryPIDs) {\n            // parse real index when custom PIDs are used \n            // http://photoswipe.com/documentation/faq.html#custom-pid-in-url\n            for (var j = 0; j < items.length; j++) {\n              if (items[j].pid == index) {\n                options.index = j;\n                break;\n              }\n            }\n          } else {\n            // in URL indexes start from 1\n            options.index = parseInt(index, 10) - 1;\n          }\n        } else {\n          options.index = parseInt(index, 10);\n        }\n\n        // exit if index not found\n        if (isNaN(options.index)) {\n          return;\n        }\n\n        if (disableAnimation) {\n          options.showAnimationDuration = 0;\n        }\n\n        // Pass data to PhotoSwipe and initialize it\n        gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, options);\n        gallery.init();\n\n        var $tempVideo;\n        var stopVideoHandle = function stopVideoHandle() {\n          if ($tempVideo) {\n            $tempVideo.remove();\n            $tempVideo = null;\n          }\n        };\n        var changeHandle = function changeHandle() {\n          var item = gallery.currItem;\n          stopVideoHandle();\n          if (item.type === 'video') {\n            var $ctn = item.container;\n            var style = $ctn.getElementsByClassName('pswp__img')[0].style;\n            var $video = document.createElement('video');\n            $video.setAttribute('autoplay', 'autoplay');\n            $video.setAttribute('controls', 'controls');\n            $video.setAttribute('src', item.target);\n            $video.style.width = style.width;\n            $video.style.height = style.height;\n            $video.style.position = 'absolute';\n            $video.style.zIndex = 2;\n            $tempVideo = $video;\n            $ctn.appendChild($video);\n          }\n        };\n        gallery.listen('initialZoomIn', changeHandle);\n        gallery.listen('afterChange', changeHandle);\n        gallery.listen('initialZoomOut', stopVideoHandle);\n      };\n\n      // loop through all gallery elements and bind events\n      var galleryElements = document.querySelectorAll(gallerySelector);\n      for (var i = 0, l = galleryElements.length; i < l; i++) {\n        galleryElements[i].setAttribute('data-pswp-uid', i + 1);\n        galleryElements[i].onclick = onThumbnailsClick;\n      }\n\n      // Parse URL and open gallery if it contains #&pid=3&gid=1\n      var hashData = photoswipeParseHash();\n      if (hashData.pid && hashData.gid) {\n        openPhotoSwipe(hashData.pid, galleryElements[hashData.gid - 1], true, true);\n      }\n    };\n\n    var Viewer = function() {\n      function init() {\n        initPhotoSwipeFromDOM('.photos');\n      }\n      return {\n        init: init\n      };\n    }();\n\n    module.exports = Viewer;\n\n    /***/\n  },\n  /* 3 */\n  /***/\n  function(module, exports) {\n\n    /* WEBPACK VAR INJECTION */\n    (function(global) {\n      module.exports = inViewport;\n\n      var instances = [];\n      var supportsMutationObserver = typeof global.MutationObserver === 'function';\n\n      function inViewport(elt, params, cb) {\n        var opts = {\n          container: global.document.body,\n          offset: 0\n        };\n\n        if (params === undefined || typeof params === 'function') {\n          cb = params;\n          params = {};\n        }\n\n        var container = opts.container = params.container || opts.container;\n        var offset = opts.offset = params.offset || opts.offset;\n\n        for (var i = 0; i < instances.length; i++) {\n          if (instances[i].container === container) {\n            return instances[i].isInViewport(elt, offset, cb);\n          }\n        }\n\n        return instances[\n          instances.push(createInViewport(container)) - 1\n        ].isInViewport(elt, offset, cb);\n      }\n\n      function addEvent(el, type, fn) {\n        if (el.attachEvent) {\n          el.attachEvent('on' + type, fn);\n        } else {\n          el.addEventListener(type, fn, false);\n        }\n      }\n\n      function debounce(func, wait, immediate) {\n        var timeout;\n        return function() {\n          var context = this,\n            args = arguments;\n          var callNow = immediate && !timeout;\n          clearTimeout(timeout);\n          timeout = setTimeout(later, wait);\n          if (callNow) func.apply(context, args);\n\n          function later() {\n            timeout = null;\n            if (!immediate) func.apply(context, args);\n          }\n        };\n      }\n\n      // https://github.com/jquery/sizzle/blob/3136f48b90e3edc84cbaaa6f6f7734ef03775a07/sizzle.js#L708\n      var contains = function() {\n        if (!global.document) {\n          return true;\n        }\n        return global.document.documentElement.compareDocumentPosition ?\n          function(a, b) {\n            return !!(a.compareDocumentPosition(b) & 16);\n          } :\n          global.document.documentElement.contains ?\n          function(a, b) {\n            return a !== b && (a.contains ? a.contains(b) : false);\n          } :\n          function(a, b) {\n            while (b = b.parentNode) {\n              if (b === a) {\n                return true;\n              }\n            }\n            return false;\n          };\n      }\n\n      function createInViewport(container) {\n        var watches = createWatches();\n\n        var scrollContainer = container === global.document.body ? global : container;\n        var debouncedCheck = debounce(watches.checkAll(watchInViewport), 15);\n\n        addEvent(scrollContainer, 'scroll', debouncedCheck);\n\n        if (scrollContainer === global) {\n          addEvent(global, 'resize', debouncedCheck);\n        }\n\n        if (supportsMutationObserver) {\n          observeDOM(watches, container, debouncedCheck);\n        }\n\n        // failsafe check, every 200ms we check for visible images\n        // usecase: a hidden parent containing eleements\n        // when the parent becomes visible, we have no event that the children\n        // became visible\n        setInterval(debouncedCheck, 150);\n\n        function isInViewport(elt, offset, cb) {\n          if (!cb) {\n            return isVisible(elt, offset);\n          }\n\n          var remote = createRemote(elt, offset, cb);\n          remote.watch();\n          return remote;\n        }\n\n        function createRemote(elt, offset, cb) {\n          function watch() {\n            watches.add(elt, offset, cb);\n          }\n\n          function dispose() {\n            watches.remove(elt);\n          }\n\n          return {\n            watch: watch,\n            dispose: dispose\n          };\n        }\n\n        function watchInViewport(elt, offset, cb) {\n          if (isVisible(elt, offset)) {\n            watches.remove(elt);\n            cb(elt);\n          }\n        }\n\n        function isVisible(elt, offset) {\n          if (!contains(global.document.documentElement, elt) || !contains(global.document.documentElement, container)) {\n            return false;\n          }\n\n          // Check if the element is visible\n          // https://github.com/jquery/jquery/blob/740e190223d19a114d5373758127285d14d6b71e/src/css/hiddenVisibleSelectors.js\n          if (!elt.offsetWidth || !elt.offsetHeight) {\n            return false;\n          }\n\n          var eltRect = elt.getBoundingClientRect();\n          var viewport = {};\n\n          if (container === global.document.body) {\n            viewport = {\n              top: -offset,\n              left: -offset,\n              right: global.document.documentElement.clientWidth + offset,\n              bottom: global.document.documentElement.clientHeight + offset\n            };\n          } else {\n            var containerRect = container.getBoundingClientRect();\n            viewport = {\n              top: containerRect.top - offset,\n              left: containerRect.left - offset,\n              right: containerRect.right + offset,\n              bottom: containerRect.bottom + offset\n            };\n          }\n\n          // The element must overlap with the visible part of the viewport\n          var visible =\n            (\n              (eltRect.right > viewport.left) &&\n              (eltRect.left < viewport.right) &&\n              (eltRect.bottom > viewport.top) &&\n              (eltRect.top < viewport.bottom)\n            );\n\n          return visible;\n        }\n\n        return {\n          container: container,\n          isInViewport: isInViewport\n        };\n      }\n\n      function createWatches() {\n        var watches = [];\n\n        function add(elt, offset, cb) {\n          if (!isWatched(elt)) {\n            watches.push([elt, offset, cb]);\n          }\n        }\n\n        function remove(elt) {\n          var pos = indexOf(elt);\n          if (pos !== -1) {\n            watches.splice(pos, 1);\n          }\n        }\n\n        function indexOf(elt) {\n          for (var i = watches.length - 1; i >= 0; i--) {\n            if (watches[i][0] === elt) {\n              return i;\n            }\n          }\n          return -1;\n        }\n\n        function isWatched(elt) {\n          return indexOf(elt) !== -1;\n        }\n\n        function checkAll(cb) {\n          return function() {\n            for (var i = watches.length - 1; i >= 0; i--) {\n              cb.apply(this, watches[i]);\n            }\n          };\n        }\n\n        return {\n          add: add,\n          remove: remove,\n          isWatched: isWatched,\n          checkAll: checkAll\n        };\n      }\n\n      function observeDOM(watches, container, cb) {\n        var observer = new MutationObserver(watch);\n        var filter = Array.prototype.filter;\n        var concat = Array.prototype.concat;\n\n        observer.observe(container, {\n          childList: true,\n          subtree: true,\n          // changes like style/width/height/display will be catched\n          attributes: true\n        });\n\n        function watch(mutations) {\n          // some new DOM nodes where previously watched\n          // we should check their positions\n          if (mutations.some(knownNodes) === true) {\n            setTimeout(cb, 0);\n          }\n        }\n\n        function knownNodes(mutation) {\n          var nodes = concat.call([],\n            Array.prototype.slice.call(mutation.addedNodes),\n            mutation.target\n          );\n          return filter.call(nodes, watches.isWatched).length > 0;\n        }\n      }\n\n      /* WEBPACK VAR INJECTION */\n    }.call(exports, (function() {\n      return this;\n    }())))\n\n    /***/\n  }\n  /******/\n]);","date":"2017-12-19T02:54:56.795Z","updated":"2017-12-19T02:54:56.795Z","path":"photos/ins.js","layout":"false","title":"","comments":1,"_id":"cjbd1r24w0003kxuw36yj44k9","content":"/******/\n(function(modules) { // webpackBootstrap\n  /******/ // The module cache\n  /******/\n  var installedModules = {};\n  /******/\n  /******/ // The require function\n  /******/\n  function __webpack_require__(moduleId) {\n    /******/\n    /******/ // Check if module is in cache\n    /******/\n    if (installedModules[moduleId])\n    /******/\n      return installedModules[moduleId].exports;\n    /******/\n    /******/ // Create a new module (and put it into the cache)\n    /******/\n    var module = installedModules[moduleId] = {\n      /******/\n      exports: {},\n      /******/\n      id: moduleId,\n      /******/\n      loaded: false\n        /******/\n    };\n    /******/\n    /******/ // Execute the module function\n    /******/\n    modules[moduleId].call(module.exports, module, module.exports, __webpack_require__);\n    /******/\n    /******/ // Flag the module as loaded\n    /******/\n    module.loaded = true;\n    /******/\n    /******/ // Return the exports of the module\n    /******/\n    return module.exports;\n    /******/\n  }\n  /******/\n  /******/\n  /******/ // expose the modules object (__webpack_modules__)\n  /******/\n  __webpack_require__.m = modules;\n  /******/\n  /******/ // expose the module cache\n  /******/\n  __webpack_require__.c = installedModules;\n  /******/\n  /******/ // __webpack_public_path__\n  /******/\n  __webpack_require__.p = \"/dist/\";\n  /******/\n  /******/ // Load entry module and return exports\n  /******/\n  return __webpack_require__(0);\n  /******/\n})\n/************************************************************************/\n/******/\n([\n  /* 0 */\n  /***/\n  function(module, exports, __webpack_require__) {\n\n    'use strict';\n\n    __webpack_require__(1);\n\n    var _view = __webpack_require__(2);\n\n    var _view2 = _interopRequireDefault(_view);\n\n    function _interopRequireDefault(obj) {\n      return obj && obj.__esModule ? obj : {\n        default: obj\n      };\n    }\n\n    /**\n     * @name impush-client \n     * @description 这个项目让我发家致富…\n     * @date 2016-12-1\n     */\n\n    var _collection = [];\n    var _count = 0;\n    var searchData;\n\n    function addMask(elem) {\n      var rect = elem.getBoundingClientRect();\n      var style = getComputedStyle(elem, null);\n\n      var mask = document.createElement('i');\n      mask.className = 'icon-film';\n      mask.style.color = '#fff';\n      mask.style.fontSize = '26px';\n      mask.style.position = 'absolute';\n      mask.style.right = '10px';\n      mask.style.bottom = '10px';\n      mask.style.zIndex = 1;\n      elem.parentNode.appendChild(mask);\n    }\n\n    var createVideoIncon = function createVideoIncon() {\n      var $videoImg = document.querySelectorAll('.thumb a[data-type=\"video\"]');\n      for (var i = 0, len = $videoImg.length; i < len; i++) {\n        addMask($videoImg[i]);\n      }\n    };\n    var render = function render(res) {\n      var ulTmpl = \"\";\n      for (var j = 0, len2 = res.list.length; j < len2; j++) {\n        var data = res.list[j].arr;\n        var liTmpl = \"\";\n        for (var i = 0, len = data.link.length; i < len; i++) {\n          // var minSrc = 'https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/min_photos/' + data.link[i] + '.min.jpg';\n          var minSrc = 'https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/min_photos/' + data.link[i];\n          var src = 'https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/photos/' + data.link[i];\n          var type = data.type[i];\n          var target = src + (type === 'video' ? '.mp4' : '.jpg');\n          // src += '.jpg';\n\n          liTmpl += '<figure class=\"thumb\" itemprop=\"associatedMedia\" itemscope=\"\" itemtype=\"http://schema.org/ImageObject\">\\\n                <a href=\"' + src + '\" itemprop=\"contentUrl\" data-size=\"640x640\" data-type=\"' + type + '\" data-target=\"' + target + '\">\\\n                  <img class=\"reward-img\" data-type=\"' + type + '\" data-src=\"' + minSrc + '\" src=\"/assets/img/head.jpg\" itemprop=\"thumbnail\" onload=\"lzld(this)\">\\\n                </a>\\\n                <figcaption style=\"display:none\" itemprop=\"caption description\">' + data.text[i] + '</figcaption>\\\n            </figure>';\n        }\n        ulTmpl = ulTmpl + '<section class=\"archives album\"><h1 class=\"year\">' + data.year + '<em>' + data.month + '月</em></h1>\\\n        <ul class=\"img-box-ul\">' + liTmpl + '</ul>\\\n        </section>';\n      }\n      document.querySelector('.instagram').innerHTML = '<div class=\"photos\" itemscope=\"\" itemtype=\"http://schema.org/ImageGallery\">' + ulTmpl + '</div>';\n      createVideoIncon();\n      _view2.default.init();\n    };\n\n    var replacer = function replacer(str) {\n      var arr = str.split(\"/\");\n      return \"/assets/ins/\" + arr[arr.length - 1];\n    };\n\n    var ctrler = function ctrler(data) {\n      var imgObj = {};\n      for (var i = 0, len = data.length; i < len; i++) {\n        var y = data[i].y;\n        var m = data[i].m;\n        var src = replacer(data[i].src);\n        var text = data[i].text;\n        var key = y + \"\" + ((m + \"\").length == 1 ? \"0\" + m : m);\n        if (imgObj[key]) {\n          imgObj[key].srclist.push(src);\n          imgObj[key].text.push(text);\n        } else {\n          imgObj[key] = {\n            year: y,\n            month: m,\n            srclist: [src],\n            text: [text]\n          };\n        }\n      }\n      render(imgObj);\n    };\n\n    function loadData(success) {\n      if (!searchData) {\n        var xhr = new XMLHttpRequest();\n        xhr.open('GET', './ins.json?t=' + +new Date(), true);\n\n        xhr.onload = function() {\n          if (this.status >= 200 && this.status < 300) {\n            var res = JSON.parse(this.response);\n            searchData = res;\n            success(searchData);\n          } else {\n            console.error(this.statusText);\n          }\n        };\n\n        xhr.onerror = function() {\n          console.error(this.statusText);\n        };\n\n        xhr.send();\n      } else {\n        success(searchData);\n      }\n    }\n\n    var Ins = {\n      init: function init() {\n        loadData(function(data) {\n          render(data);\n        });\n      }\n    };\n\n    Ins.init();\n\n    // export default impush;\n\n    /***/\n  },\n  /* 1 */\n  /***/\n  function(module, exports, __webpack_require__) {\n\n    /* WEBPACK VAR INJECTION */\n    (function(global) {\n      'use strict';\n\n      var inViewport = __webpack_require__(3);\n      var lazyAttrs = ['data-src'];\n\n      global.lzld = lazyload();\n\n      // Provide libs using getAttribute early to get the good src\n      // and not the fake data-src\n      replaceGetAttribute('Image');\n      replaceGetAttribute('IFrame');\n\n      function registerLazyAttr(attr) {\n        if (indexOf.call(lazyAttrs, attr) === -1) {\n          lazyAttrs.push(attr);\n        }\n      }\n\n      function lazyload(opts) {\n        opts = merge({\n          'offset': 333,\n          'src': 'data-src',\n          'container': false\n        }, opts || {});\n\n        if (typeof opts.src === 'string') {\n          registerLazyAttr(opts.src);\n        }\n\n        var elts = [];\n\n        function show(elt) {\n          var src = findRealSrc(elt);\n\n          if (src) {\n            elt.src = src;\n          }\n\n          elt.setAttribute('data-lzled', true);\n          elts[indexOf.call(elts, elt)] = null;\n        }\n\n        function findRealSrc(elt) {\n          if (typeof opts.src === 'function') {\n            return opts.src(elt);\n          }\n\n          return elt.getAttribute(opts.src);\n        }\n\n        function register(elt) {\n          elt.onload = null;\n          elt.removeAttribute('onload');\n          elt.onerror = null;\n          elt.removeAttribute('onerror');\n\n          if (indexOf.call(elts, elt) === -1) {\n            inViewport(elt, opts, show);\n          }\n        }\n\n        return register;\n      }\n\n      function replaceGetAttribute(elementName) {\n        var fullname = 'HTML' + elementName + 'Element';\n        if (fullname in global === false) {\n          return;\n        }\n\n        var original = global[fullname].prototype.getAttribute;\n        global[fullname].prototype.getAttribute = function(name) {\n          if (name === 'src') {\n            var realSrc;\n            for (var i = 0, max = lazyAttrs.length; i < max; i++) {\n              realSrc = original.call(this, lazyAttrs[i]);\n              if (realSrc) {\n                break;\n              }\n            }\n\n            return realSrc || original.call(this, name);\n          }\n\n          // our own lazyloader will go through theses lines\n          // because we use getAttribute(opts.src)\n          return original.call(this, name);\n        };\n      }\n\n      function merge(defaults, opts) {\n        for (var name in defaults) {\n          if (opts[name] === undefined) {\n            opts[name] = defaults[name];\n          }\n        }\n\n        return opts;\n      }\n\n      // http://webreflection.blogspot.fr/2011/06/partial-polyfills.html\n      function indexOf(value) {\n        for (var i = this.length; i-- && this[i] !== value;) {}\n        return i;\n      }\n\n      module.exports = lazyload;\n\n      // export default impush;\n      /* WEBPACK VAR INJECTION */\n    }.call(exports, (function() {\n      return this;\n    }())))\n\n    /***/\n  },\n  /* 2 */\n  /***/\n  function(module, exports) {\n\n    'use strict';\n\n    var initPhotoSwipeFromDOM = function initPhotoSwipeFromDOM(gallerySelector) {\n\n      // parse slide data (url, title, size ...) from DOM elements \n      // (children of gallerySelector)\n      var parseThumbnailElements = function parseThumbnailElements(el) {\n        el = el.parentNode.parentNode;\n        var thumbElements = el.getElementsByClassName('thumb'),\n          numNodes = thumbElements.length,\n          items = [],\n          figureEl,\n          linkEl,\n          size,\n          type,\n          // video or not\n          target,\n          item;\n\n        for (var i = 0; i < numNodes; i++) {\n\n          figureEl = thumbElements[i]; // \n\n          // include only element nodes \n          if (figureEl.nodeType !== 1) {\n            continue;\n          }\n\n          linkEl = figureEl.children[0]; // \n\n          size = linkEl.getAttribute('data-size').split('x');\n          type = linkEl.getAttribute('data-type');\n          target = linkEl.getAttribute('data-target');\n          // create slide object\n          item = {\n            src: linkEl.getAttribute('href'),\n            w: parseInt(size[0], 10),\n            h: parseInt(size[1], 10)\n          };\n\n          if (figureEl.children.length > 1) {\n            item.title = figureEl.children[1].innerHTML;\n          }\n\n          if (linkEl.children.length > 0) {\n            item.msrc = linkEl.children[0].getAttribute('src');\n            item.type = type;\n            item.target = target;\n            item.html = '<video src=\"' + target + '\" controls=\"controls\" autoplay=\"autoplay\"></video>';\n            if (type === 'video') {\n              //item.src = null;\n            }\n          }\n\n          item.el = figureEl; // save link to element for getThumbBoundsFn\n          items.push(item);\n        }\n\n        return items;\n      };\n\n      // find nearest parent element\n      var closest = function closest(el, fn) {\n        return el && (fn(el) ? el : closest(el.parentNode, fn));\n      };\n\n      // triggers when user clicks on thumbnail\n      var onThumbnailsClick = function onThumbnailsClick(e) {\n        e = e || window.event;\n        e.preventDefault ? e.preventDefault() : e.returnValue = false;\n\n        var eTarget = e.target || e.srcElement;\n\n        // find root element of slide\n        var clickedListItem = closest(eTarget, function(el) {\n          return el.tagName && el.tagName.toUpperCase() === 'FIGURE';\n        });\n\n        if (!clickedListItem) {\n          return;\n        }\n\n        // find index of clicked item by looping through all child nodes\n        // alternatively, you may define index via data- attribute\n        var clickedGallery = clickedListItem.parentNode,\n\n          // childNodes = clickedListItem.parentNode.childNodes,\n          // numChildNodes = childNodes.length,\n          childNodes = document.getElementsByClassName('thumb'),\n          numChildNodes = childNodes.length,\n          nodeIndex = 0,\n          index;\n\n        for (var i = 0; i < numChildNodes; i++) {\n          if (childNodes[i].nodeType !== 1) {\n            continue;\n          }\n\n          if (childNodes[i] === clickedListItem) {\n            index = nodeIndex;\n            break;\n          }\n          nodeIndex++;\n        }\n\n        if (index >= 0) {\n          // open PhotoSwipe if valid index found\n          openPhotoSwipe(index, clickedGallery);\n        }\n        return false;\n      };\n\n      // parse picture index and gallery index from URL (#&pid=1&gid=2)\n      var photoswipeParseHash = function photoswipeParseHash() {\n        var hash = window.location.hash.substring(1),\n          params = {};\n\n        if (hash.length < 5) {\n          return params;\n        }\n\n        var vars = hash.split('&');\n        for (var i = 0; i < vars.length; i++) {\n          if (!vars[i]) {\n            continue;\n          }\n          var pair = vars[i].split('=');\n          if (pair.length < 2) {\n            continue;\n          }\n          params[pair[0]] = pair[1];\n        }\n\n        if (params.gid) {\n          params.gid = parseInt(params.gid, 10);\n        }\n\n        return params;\n      };\n\n      var openPhotoSwipe = function openPhotoSwipe(index, galleryElement, disableAnimation, fromURL) {\n        var pswpElement = document.querySelectorAll('.pswp')[0],\n          gallery,\n          options,\n          items;\n\n        items = parseThumbnailElements(galleryElement);\n        // define options (if needed)\n        options = {\n\n          // define gallery index (for URL)\n          galleryUID: galleryElement.getAttribute('data-pswp-uid'),\n\n          getThumbBoundsFn: function getThumbBoundsFn(index) {\n            // See Options -> getThumbBoundsFn section of documentation for more info\n            var thumbnail = items[index].el.getElementsByTagName('img')[0],\n              // find thumbnail\n              pageYScroll = window.pageYOffset || document.documentElement.scrollTop,\n              rect = thumbnail.getBoundingClientRect();\n\n            return {\n              x: rect.left,\n              y: rect.top + pageYScroll,\n              w: rect.width\n            };\n          }\n\n        };\n\n        // PhotoSwipe opened from URL\n        if (fromURL) {\n          if (options.galleryPIDs) {\n            // parse real index when custom PIDs are used \n            // http://photoswipe.com/documentation/faq.html#custom-pid-in-url\n            for (var j = 0; j < items.length; j++) {\n              if (items[j].pid == index) {\n                options.index = j;\n                break;\n              }\n            }\n          } else {\n            // in URL indexes start from 1\n            options.index = parseInt(index, 10) - 1;\n          }\n        } else {\n          options.index = parseInt(index, 10);\n        }\n\n        // exit if index not found\n        if (isNaN(options.index)) {\n          return;\n        }\n\n        if (disableAnimation) {\n          options.showAnimationDuration = 0;\n        }\n\n        // Pass data to PhotoSwipe and initialize it\n        gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, options);\n        gallery.init();\n\n        var $tempVideo;\n        var stopVideoHandle = function stopVideoHandle() {\n          if ($tempVideo) {\n            $tempVideo.remove();\n            $tempVideo = null;\n          }\n        };\n        var changeHandle = function changeHandle() {\n          var item = gallery.currItem;\n          stopVideoHandle();\n          if (item.type === 'video') {\n            var $ctn = item.container;\n            var style = $ctn.getElementsByClassName('pswp__img')[0].style;\n            var $video = document.createElement('video');\n            $video.setAttribute('autoplay', 'autoplay');\n            $video.setAttribute('controls', 'controls');\n            $video.setAttribute('src', item.target);\n            $video.style.width = style.width;\n            $video.style.height = style.height;\n            $video.style.position = 'absolute';\n            $video.style.zIndex = 2;\n            $tempVideo = $video;\n            $ctn.appendChild($video);\n          }\n        };\n        gallery.listen('initialZoomIn', changeHandle);\n        gallery.listen('afterChange', changeHandle);\n        gallery.listen('initialZoomOut', stopVideoHandle);\n      };\n\n      // loop through all gallery elements and bind events\n      var galleryElements = document.querySelectorAll(gallerySelector);\n      for (var i = 0, l = galleryElements.length; i < l; i++) {\n        galleryElements[i].setAttribute('data-pswp-uid', i + 1);\n        galleryElements[i].onclick = onThumbnailsClick;\n      }\n\n      // Parse URL and open gallery if it contains #&pid=3&gid=1\n      var hashData = photoswipeParseHash();\n      if (hashData.pid && hashData.gid) {\n        openPhotoSwipe(hashData.pid, galleryElements[hashData.gid - 1], true, true);\n      }\n    };\n\n    var Viewer = function() {\n      function init() {\n        initPhotoSwipeFromDOM('.photos');\n      }\n      return {\n        init: init\n      };\n    }();\n\n    module.exports = Viewer;\n\n    /***/\n  },\n  /* 3 */\n  /***/\n  function(module, exports) {\n\n    /* WEBPACK VAR INJECTION */\n    (function(global) {\n      module.exports = inViewport;\n\n      var instances = [];\n      var supportsMutationObserver = typeof global.MutationObserver === 'function';\n\n      function inViewport(elt, params, cb) {\n        var opts = {\n          container: global.document.body,\n          offset: 0\n        };\n\n        if (params === undefined || typeof params === 'function') {\n          cb = params;\n          params = {};\n        }\n\n        var container = opts.container = params.container || opts.container;\n        var offset = opts.offset = params.offset || opts.offset;\n\n        for (var i = 0; i < instances.length; i++) {\n          if (instances[i].container === container) {\n            return instances[i].isInViewport(elt, offset, cb);\n          }\n        }\n\n        return instances[\n          instances.push(createInViewport(container)) - 1\n        ].isInViewport(elt, offset, cb);\n      }\n\n      function addEvent(el, type, fn) {\n        if (el.attachEvent) {\n          el.attachEvent('on' + type, fn);\n        } else {\n          el.addEventListener(type, fn, false);\n        }\n      }\n\n      function debounce(func, wait, immediate) {\n        var timeout;\n        return function() {\n          var context = this,\n            args = arguments;\n          var callNow = immediate && !timeout;\n          clearTimeout(timeout);\n          timeout = setTimeout(later, wait);\n          if (callNow) func.apply(context, args);\n\n          function later() {\n            timeout = null;\n            if (!immediate) func.apply(context, args);\n          }\n        };\n      }\n\n      // https://github.com/jquery/sizzle/blob/3136f48b90e3edc84cbaaa6f6f7734ef03775a07/sizzle.js#L708\n      var contains = function() {\n        if (!global.document) {\n          return true;\n        }\n        return global.document.documentElement.compareDocumentPosition ?\n          function(a, b) {\n            return !!(a.compareDocumentPosition(b) & 16);\n          } :\n          global.document.documentElement.contains ?\n          function(a, b) {\n            return a !== b && (a.contains ? a.contains(b) : false);\n          } :\n          function(a, b) {\n            while (b = b.parentNode) {\n              if (b === a) {\n                return true;\n              }\n            }\n            return false;\n          };\n      }\n\n      function createInViewport(container) {\n        var watches = createWatches();\n\n        var scrollContainer = container === global.document.body ? global : container;\n        var debouncedCheck = debounce(watches.checkAll(watchInViewport), 15);\n\n        addEvent(scrollContainer, 'scroll', debouncedCheck);\n\n        if (scrollContainer === global) {\n          addEvent(global, 'resize', debouncedCheck);\n        }\n\n        if (supportsMutationObserver) {\n          observeDOM(watches, container, debouncedCheck);\n        }\n\n        // failsafe check, every 200ms we check for visible images\n        // usecase: a hidden parent containing eleements\n        // when the parent becomes visible, we have no event that the children\n        // became visible\n        setInterval(debouncedCheck, 150);\n\n        function isInViewport(elt, offset, cb) {\n          if (!cb) {\n            return isVisible(elt, offset);\n          }\n\n          var remote = createRemote(elt, offset, cb);\n          remote.watch();\n          return remote;\n        }\n\n        function createRemote(elt, offset, cb) {\n          function watch() {\n            watches.add(elt, offset, cb);\n          }\n\n          function dispose() {\n            watches.remove(elt);\n          }\n\n          return {\n            watch: watch,\n            dispose: dispose\n          };\n        }\n\n        function watchInViewport(elt, offset, cb) {\n          if (isVisible(elt, offset)) {\n            watches.remove(elt);\n            cb(elt);\n          }\n        }\n\n        function isVisible(elt, offset) {\n          if (!contains(global.document.documentElement, elt) || !contains(global.document.documentElement, container)) {\n            return false;\n          }\n\n          // Check if the element is visible\n          // https://github.com/jquery/jquery/blob/740e190223d19a114d5373758127285d14d6b71e/src/css/hiddenVisibleSelectors.js\n          if (!elt.offsetWidth || !elt.offsetHeight) {\n            return false;\n          }\n\n          var eltRect = elt.getBoundingClientRect();\n          var viewport = {};\n\n          if (container === global.document.body) {\n            viewport = {\n              top: -offset,\n              left: -offset,\n              right: global.document.documentElement.clientWidth + offset,\n              bottom: global.document.documentElement.clientHeight + offset\n            };\n          } else {\n            var containerRect = container.getBoundingClientRect();\n            viewport = {\n              top: containerRect.top - offset,\n              left: containerRect.left - offset,\n              right: containerRect.right + offset,\n              bottom: containerRect.bottom + offset\n            };\n          }\n\n          // The element must overlap with the visible part of the viewport\n          var visible =\n            (\n              (eltRect.right > viewport.left) &&\n              (eltRect.left < viewport.right) &&\n              (eltRect.bottom > viewport.top) &&\n              (eltRect.top < viewport.bottom)\n            );\n\n          return visible;\n        }\n\n        return {\n          container: container,\n          isInViewport: isInViewport\n        };\n      }\n\n      function createWatches() {\n        var watches = [];\n\n        function add(elt, offset, cb) {\n          if (!isWatched(elt)) {\n            watches.push([elt, offset, cb]);\n          }\n        }\n\n        function remove(elt) {\n          var pos = indexOf(elt);\n          if (pos !== -1) {\n            watches.splice(pos, 1);\n          }\n        }\n\n        function indexOf(elt) {\n          for (var i = watches.length - 1; i >= 0; i--) {\n            if (watches[i][0] === elt) {\n              return i;\n            }\n          }\n          return -1;\n        }\n\n        function isWatched(elt) {\n          return indexOf(elt) !== -1;\n        }\n\n        function checkAll(cb) {\n          return function() {\n            for (var i = watches.length - 1; i >= 0; i--) {\n              cb.apply(this, watches[i]);\n            }\n          };\n        }\n\n        return {\n          add: add,\n          remove: remove,\n          isWatched: isWatched,\n          checkAll: checkAll\n        };\n      }\n\n      function observeDOM(watches, container, cb) {\n        var observer = new MutationObserver(watch);\n        var filter = Array.prototype.filter;\n        var concat = Array.prototype.concat;\n\n        observer.observe(container, {\n          childList: true,\n          subtree: true,\n          // changes like style/width/height/display will be catched\n          attributes: true\n        });\n\n        function watch(mutations) {\n          // some new DOM nodes where previously watched\n          // we should check their positions\n          if (mutations.some(knownNodes) === true) {\n            setTimeout(cb, 0);\n          }\n        }\n\n        function knownNodes(mutation) {\n          var nodes = concat.call([],\n            Array.prototype.slice.call(mutation.addedNodes),\n            mutation.target\n          );\n          return filter.call(nodes, watches.isWatched).length > 0;\n        }\n      }\n\n      /* WEBPACK VAR INJECTION */\n    }.call(exports, (function() {\n      return this;\n    }())))\n\n    /***/\n  }\n  /******/\n]);","site":{"data":{}},"excerpt":"","more":"/******/\n(function(modules) { // webpackBootstrap\n  /******/ // The module cache\n  /******/\n  var installedModules = {};\n  /******/\n  /******/ // The require function\n  /******/\n  function __webpack_require__(moduleId) {\n    /******/\n    /******/ // Check if module is in cache\n    /******/\n    if (installedModules[moduleId])\n    /******/\n      return installedModules[moduleId].exports;\n    /******/\n    /******/ // Create a new module (and put it into the cache)\n    /******/\n    var module = installedModules[moduleId] = {\n      /******/\n      exports: {},\n      /******/\n      id: moduleId,\n      /******/\n      loaded: false\n        /******/\n    };\n    /******/\n    /******/ // Execute the module function\n    /******/\n    modules[moduleId].call(module.exports, module, module.exports, __webpack_require__);\n    /******/\n    /******/ // Flag the module as loaded\n    /******/\n    module.loaded = true;\n    /******/\n    /******/ // Return the exports of the module\n    /******/\n    return module.exports;\n    /******/\n  }\n  /******/\n  /******/\n  /******/ // expose the modules object (__webpack_modules__)\n  /******/\n  __webpack_require__.m = modules;\n  /******/\n  /******/ // expose the module cache\n  /******/\n  __webpack_require__.c = installedModules;\n  /******/\n  /******/ // __webpack_public_path__\n  /******/\n  __webpack_require__.p = \"/dist/\";\n  /******/\n  /******/ // Load entry module and return exports\n  /******/\n  return __webpack_require__(0);\n  /******/\n})\n/************************************************************************/\n/******/\n([\n  /* 0 */\n  /***/\n  function(module, exports, __webpack_require__) {\n\n    'use strict';\n\n    __webpack_require__(1);\n\n    var _view = __webpack_require__(2);\n\n    var _view2 = _interopRequireDefault(_view);\n\n    function _interopRequireDefault(obj) {\n      return obj && obj.__esModule ? obj : {\n        default: obj\n      };\n    }\n\n    /**\n     * @name impush-client \n     * @description 这个项目让我发家致富…\n     * @date 2016-12-1\n     */\n\n    var _collection = [];\n    var _count = 0;\n    var searchData;\n\n    function addMask(elem) {\n      var rect = elem.getBoundingClientRect();\n      var style = getComputedStyle(elem, null);\n\n      var mask = document.createElement('i');\n      mask.className = 'icon-film';\n      mask.style.color = '#fff';\n      mask.style.fontSize = '26px';\n      mask.style.position = 'absolute';\n      mask.style.right = '10px';\n      mask.style.bottom = '10px';\n      mask.style.zIndex = 1;\n      elem.parentNode.appendChild(mask);\n    }\n\n    var createVideoIncon = function createVideoIncon() {\n      var $videoImg = document.querySelectorAll('.thumb a[data-type=\"video\"]');\n      for (var i = 0, len = $videoImg.length; i < len; i++) {\n        addMask($videoImg[i]);\n      }\n    };\n    var render = function render(res) {\n      var ulTmpl = \"\";\n      for (var j = 0, len2 = res.list.length; j < len2; j++) {\n        var data = res.list[j].arr;\n        var liTmpl = \"\";\n        for (var i = 0, len = data.link.length; i < len; i++) {\n          // var minSrc = 'https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/min_photos/' + data.link[i] + '.min.jpg';\n          var minSrc = 'https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/min_photos/' + data.link[i];\n          var src = 'https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/photos/' + data.link[i];\n          var type = data.type[i];\n          var target = src + (type === 'video' ? '.mp4' : '.jpg');\n          // src += '.jpg';\n\n          liTmpl += '<figure class=\"thumb\" itemprop=\"associatedMedia\" itemscope=\"\" itemtype=\"http://schema.org/ImageObject\">\\\n                <a href=\"' + src + '\" itemprop=\"contentUrl\" data-size=\"640x640\" data-type=\"' + type + '\" data-target=\"' + target + '\">\\\n                  <img class=\"reward-img\" data-type=\"' + type + '\" data-src=\"' + minSrc + '\" src=\"/assets/img/head.jpg\" itemprop=\"thumbnail\" onload=\"lzld(this)\">\\\n                </a>\\\n                <figcaption style=\"display:none\" itemprop=\"caption description\">' + data.text[i] + '</figcaption>\\\n            </figure>';\n        }\n        ulTmpl = ulTmpl + '<section class=\"archives album\"><h1 class=\"year\">' + data.year + '<em>' + data.month + '月</em></h1>\\\n        <ul class=\"img-box-ul\">' + liTmpl + '</ul>\\\n        </section>';\n      }\n      document.querySelector('.instagram').innerHTML = '<div class=\"photos\" itemscope=\"\" itemtype=\"http://schema.org/ImageGallery\">' + ulTmpl + '</div>';\n      createVideoIncon();\n      _view2.default.init();\n    };\n\n    var replacer = function replacer(str) {\n      var arr = str.split(\"/\");\n      return \"/assets/ins/\" + arr[arr.length - 1];\n    };\n\n    var ctrler = function ctrler(data) {\n      var imgObj = {};\n      for (var i = 0, len = data.length; i < len; i++) {\n        var y = data[i].y;\n        var m = data[i].m;\n        var src = replacer(data[i].src);\n        var text = data[i].text;\n        var key = y + \"\" + ((m + \"\").length == 1 ? \"0\" + m : m);\n        if (imgObj[key]) {\n          imgObj[key].srclist.push(src);\n          imgObj[key].text.push(text);\n        } else {\n          imgObj[key] = {\n            year: y,\n            month: m,\n            srclist: [src],\n            text: [text]\n          };\n        }\n      }\n      render(imgObj);\n    };\n\n    function loadData(success) {\n      if (!searchData) {\n        var xhr = new XMLHttpRequest();\n        xhr.open('GET', './ins.json?t=' + +new Date(), true);\n\n        xhr.onload = function() {\n          if (this.status >= 200 && this.status < 300) {\n            var res = JSON.parse(this.response);\n            searchData = res;\n            success(searchData);\n          } else {\n            console.error(this.statusText);\n          }\n        };\n\n        xhr.onerror = function() {\n          console.error(this.statusText);\n        };\n\n        xhr.send();\n      } else {\n        success(searchData);\n      }\n    }\n\n    var Ins = {\n      init: function init() {\n        loadData(function(data) {\n          render(data);\n        });\n      }\n    };\n\n    Ins.init();\n\n    // export default impush;\n\n    /***/\n  },\n  /* 1 */\n  /***/\n  function(module, exports, __webpack_require__) {\n\n    /* WEBPACK VAR INJECTION */\n    (function(global) {\n      'use strict';\n\n      var inViewport = __webpack_require__(3);\n      var lazyAttrs = ['data-src'];\n\n      global.lzld = lazyload();\n\n      // Provide libs using getAttribute early to get the good src\n      // and not the fake data-src\n      replaceGetAttribute('Image');\n      replaceGetAttribute('IFrame');\n\n      function registerLazyAttr(attr) {\n        if (indexOf.call(lazyAttrs, attr) === -1) {\n          lazyAttrs.push(attr);\n        }\n      }\n\n      function lazyload(opts) {\n        opts = merge({\n          'offset': 333,\n          'src': 'data-src',\n          'container': false\n        }, opts || {});\n\n        if (typeof opts.src === 'string') {\n          registerLazyAttr(opts.src);\n        }\n\n        var elts = [];\n\n        function show(elt) {\n          var src = findRealSrc(elt);\n\n          if (src) {\n            elt.src = src;\n          }\n\n          elt.setAttribute('data-lzled', true);\n          elts[indexOf.call(elts, elt)] = null;\n        }\n\n        function findRealSrc(elt) {\n          if (typeof opts.src === 'function') {\n            return opts.src(elt);\n          }\n\n          return elt.getAttribute(opts.src);\n        }\n\n        function register(elt) {\n          elt.onload = null;\n          elt.removeAttribute('onload');\n          elt.onerror = null;\n          elt.removeAttribute('onerror');\n\n          if (indexOf.call(elts, elt) === -1) {\n            inViewport(elt, opts, show);\n          }\n        }\n\n        return register;\n      }\n\n      function replaceGetAttribute(elementName) {\n        var fullname = 'HTML' + elementName + 'Element';\n        if (fullname in global === false) {\n          return;\n        }\n\n        var original = global[fullname].prototype.getAttribute;\n        global[fullname].prototype.getAttribute = function(name) {\n          if (name === 'src') {\n            var realSrc;\n            for (var i = 0, max = lazyAttrs.length; i < max; i++) {\n              realSrc = original.call(this, lazyAttrs[i]);\n              if (realSrc) {\n                break;\n              }\n            }\n\n            return realSrc || original.call(this, name);\n          }\n\n          // our own lazyloader will go through theses lines\n          // because we use getAttribute(opts.src)\n          return original.call(this, name);\n        };\n      }\n\n      function merge(defaults, opts) {\n        for (var name in defaults) {\n          if (opts[name] === undefined) {\n            opts[name] = defaults[name];\n          }\n        }\n\n        return opts;\n      }\n\n      // http://webreflection.blogspot.fr/2011/06/partial-polyfills.html\n      function indexOf(value) {\n        for (var i = this.length; i-- && this[i] !== value;) {}\n        return i;\n      }\n\n      module.exports = lazyload;\n\n      // export default impush;\n      /* WEBPACK VAR INJECTION */\n    }.call(exports, (function() {\n      return this;\n    }())))\n\n    /***/\n  },\n  /* 2 */\n  /***/\n  function(module, exports) {\n\n    'use strict';\n\n    var initPhotoSwipeFromDOM = function initPhotoSwipeFromDOM(gallerySelector) {\n\n      // parse slide data (url, title, size ...) from DOM elements \n      // (children of gallerySelector)\n      var parseThumbnailElements = function parseThumbnailElements(el) {\n        el = el.parentNode.parentNode;\n        var thumbElements = el.getElementsByClassName('thumb'),\n          numNodes = thumbElements.length,\n          items = [],\n          figureEl,\n          linkEl,\n          size,\n          type,\n          // video or not\n          target,\n          item;\n\n        for (var i = 0; i < numNodes; i++) {\n\n          figureEl = thumbElements[i]; // \n\n          // include only element nodes \n          if (figureEl.nodeType !== 1) {\n            continue;\n          }\n\n          linkEl = figureEl.children[0]; // \n\n          size = linkEl.getAttribute('data-size').split('x');\n          type = linkEl.getAttribute('data-type');\n          target = linkEl.getAttribute('data-target');\n          // create slide object\n          item = {\n            src: linkEl.getAttribute('href'),\n            w: parseInt(size[0], 10),\n            h: parseInt(size[1], 10)\n          };\n\n          if (figureEl.children.length > 1) {\n            item.title = figureEl.children[1].innerHTML;\n          }\n\n          if (linkEl.children.length > 0) {\n            item.msrc = linkEl.children[0].getAttribute('src');\n            item.type = type;\n            item.target = target;\n            item.html = '<video src=\"' + target + '\" controls=\"controls\" autoplay=\"autoplay\"></video>';\n            if (type === 'video') {\n              //item.src = null;\n            }\n          }\n\n          item.el = figureEl; // save link to element for getThumbBoundsFn\n          items.push(item);\n        }\n\n        return items;\n      };\n\n      // find nearest parent element\n      var closest = function closest(el, fn) {\n        return el && (fn(el) ? el : closest(el.parentNode, fn));\n      };\n\n      // triggers when user clicks on thumbnail\n      var onThumbnailsClick = function onThumbnailsClick(e) {\n        e = e || window.event;\n        e.preventDefault ? e.preventDefault() : e.returnValue = false;\n\n        var eTarget = e.target || e.srcElement;\n\n        // find root element of slide\n        var clickedListItem = closest(eTarget, function(el) {\n          return el.tagName && el.tagName.toUpperCase() === 'FIGURE';\n        });\n\n        if (!clickedListItem) {\n          return;\n        }\n\n        // find index of clicked item by looping through all child nodes\n        // alternatively, you may define index via data- attribute\n        var clickedGallery = clickedListItem.parentNode,\n\n          // childNodes = clickedListItem.parentNode.childNodes,\n          // numChildNodes = childNodes.length,\n          childNodes = document.getElementsByClassName('thumb'),\n          numChildNodes = childNodes.length,\n          nodeIndex = 0,\n          index;\n\n        for (var i = 0; i < numChildNodes; i++) {\n          if (childNodes[i].nodeType !== 1) {\n            continue;\n          }\n\n          if (childNodes[i] === clickedListItem) {\n            index = nodeIndex;\n            break;\n          }\n          nodeIndex++;\n        }\n\n        if (index >= 0) {\n          // open PhotoSwipe if valid index found\n          openPhotoSwipe(index, clickedGallery);\n        }\n        return false;\n      };\n\n      // parse picture index and gallery index from URL (#&pid=1&gid=2)\n      var photoswipeParseHash = function photoswipeParseHash() {\n        var hash = window.location.hash.substring(1),\n          params = {};\n\n        if (hash.length < 5) {\n          return params;\n        }\n\n        var vars = hash.split('&');\n        for (var i = 0; i < vars.length; i++) {\n          if (!vars[i]) {\n            continue;\n          }\n          var pair = vars[i].split('=');\n          if (pair.length < 2) {\n            continue;\n          }\n          params[pair[0]] = pair[1];\n        }\n\n        if (params.gid) {\n          params.gid = parseInt(params.gid, 10);\n        }\n\n        return params;\n      };\n\n      var openPhotoSwipe = function openPhotoSwipe(index, galleryElement, disableAnimation, fromURL) {\n        var pswpElement = document.querySelectorAll('.pswp')[0],\n          gallery,\n          options,\n          items;\n\n        items = parseThumbnailElements(galleryElement);\n        // define options (if needed)\n        options = {\n\n          // define gallery index (for URL)\n          galleryUID: galleryElement.getAttribute('data-pswp-uid'),\n\n          getThumbBoundsFn: function getThumbBoundsFn(index) {\n            // See Options -> getThumbBoundsFn section of documentation for more info\n            var thumbnail = items[index].el.getElementsByTagName('img')[0],\n              // find thumbnail\n              pageYScroll = window.pageYOffset || document.documentElement.scrollTop,\n              rect = thumbnail.getBoundingClientRect();\n\n            return {\n              x: rect.left,\n              y: rect.top + pageYScroll,\n              w: rect.width\n            };\n          }\n\n        };\n\n        // PhotoSwipe opened from URL\n        if (fromURL) {\n          if (options.galleryPIDs) {\n            // parse real index when custom PIDs are used \n            // http://photoswipe.com/documentation/faq.html#custom-pid-in-url\n            for (var j = 0; j < items.length; j++) {\n              if (items[j].pid == index) {\n                options.index = j;\n                break;\n              }\n            }\n          } else {\n            // in URL indexes start from 1\n            options.index = parseInt(index, 10) - 1;\n          }\n        } else {\n          options.index = parseInt(index, 10);\n        }\n\n        // exit if index not found\n        if (isNaN(options.index)) {\n          return;\n        }\n\n        if (disableAnimation) {\n          options.showAnimationDuration = 0;\n        }\n\n        // Pass data to PhotoSwipe and initialize it\n        gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, options);\n        gallery.init();\n\n        var $tempVideo;\n        var stopVideoHandle = function stopVideoHandle() {\n          if ($tempVideo) {\n            $tempVideo.remove();\n            $tempVideo = null;\n          }\n        };\n        var changeHandle = function changeHandle() {\n          var item = gallery.currItem;\n          stopVideoHandle();\n          if (item.type === 'video') {\n            var $ctn = item.container;\n            var style = $ctn.getElementsByClassName('pswp__img')[0].style;\n            var $video = document.createElement('video');\n            $video.setAttribute('autoplay', 'autoplay');\n            $video.setAttribute('controls', 'controls');\n            $video.setAttribute('src', item.target);\n            $video.style.width = style.width;\n            $video.style.height = style.height;\n            $video.style.position = 'absolute';\n            $video.style.zIndex = 2;\n            $tempVideo = $video;\n            $ctn.appendChild($video);\n          }\n        };\n        gallery.listen('initialZoomIn', changeHandle);\n        gallery.listen('afterChange', changeHandle);\n        gallery.listen('initialZoomOut', stopVideoHandle);\n      };\n\n      // loop through all gallery elements and bind events\n      var galleryElements = document.querySelectorAll(gallerySelector);\n      for (var i = 0, l = galleryElements.length; i < l; i++) {\n        galleryElements[i].setAttribute('data-pswp-uid', i + 1);\n        galleryElements[i].onclick = onThumbnailsClick;\n      }\n\n      // Parse URL and open gallery if it contains #&pid=3&gid=1\n      var hashData = photoswipeParseHash();\n      if (hashData.pid && hashData.gid) {\n        openPhotoSwipe(hashData.pid, galleryElements[hashData.gid - 1], true, true);\n      }\n    };\n\n    var Viewer = function() {\n      function init() {\n        initPhotoSwipeFromDOM('.photos');\n      }\n      return {\n        init: init\n      };\n    }();\n\n    module.exports = Viewer;\n\n    /***/\n  },\n  /* 3 */\n  /***/\n  function(module, exports) {\n\n    /* WEBPACK VAR INJECTION */\n    (function(global) {\n      module.exports = inViewport;\n\n      var instances = [];\n      var supportsMutationObserver = typeof global.MutationObserver === 'function';\n\n      function inViewport(elt, params, cb) {\n        var opts = {\n          container: global.document.body,\n          offset: 0\n        };\n\n        if (params === undefined || typeof params === 'function') {\n          cb = params;\n          params = {};\n        }\n\n        var container = opts.container = params.container || opts.container;\n        var offset = opts.offset = params.offset || opts.offset;\n\n        for (var i = 0; i < instances.length; i++) {\n          if (instances[i].container === container) {\n            return instances[i].isInViewport(elt, offset, cb);\n          }\n        }\n\n        return instances[\n          instances.push(createInViewport(container)) - 1\n        ].isInViewport(elt, offset, cb);\n      }\n\n      function addEvent(el, type, fn) {\n        if (el.attachEvent) {\n          el.attachEvent('on' + type, fn);\n        } else {\n          el.addEventListener(type, fn, false);\n        }\n      }\n\n      function debounce(func, wait, immediate) {\n        var timeout;\n        return function() {\n          var context = this,\n            args = arguments;\n          var callNow = immediate && !timeout;\n          clearTimeout(timeout);\n          timeout = setTimeout(later, wait);\n          if (callNow) func.apply(context, args);\n\n          function later() {\n            timeout = null;\n            if (!immediate) func.apply(context, args);\n          }\n        };\n      }\n\n      // https://github.com/jquery/sizzle/blob/3136f48b90e3edc84cbaaa6f6f7734ef03775a07/sizzle.js#L708\n      var contains = function() {\n        if (!global.document) {\n          return true;\n        }\n        return global.document.documentElement.compareDocumentPosition ?\n          function(a, b) {\n            return !!(a.compareDocumentPosition(b) & 16);\n          } :\n          global.document.documentElement.contains ?\n          function(a, b) {\n            return a !== b && (a.contains ? a.contains(b) : false);\n          } :\n          function(a, b) {\n            while (b = b.parentNode) {\n              if (b === a) {\n                return true;\n              }\n            }\n            return false;\n          };\n      }\n\n      function createInViewport(container) {\n        var watches = createWatches();\n\n        var scrollContainer = container === global.document.body ? global : container;\n        var debouncedCheck = debounce(watches.checkAll(watchInViewport), 15);\n\n        addEvent(scrollContainer, 'scroll', debouncedCheck);\n\n        if (scrollContainer === global) {\n          addEvent(global, 'resize', debouncedCheck);\n        }\n\n        if (supportsMutationObserver) {\n          observeDOM(watches, container, debouncedCheck);\n        }\n\n        // failsafe check, every 200ms we check for visible images\n        // usecase: a hidden parent containing eleements\n        // when the parent becomes visible, we have no event that the children\n        // became visible\n        setInterval(debouncedCheck, 150);\n\n        function isInViewport(elt, offset, cb) {\n          if (!cb) {\n            return isVisible(elt, offset);\n          }\n\n          var remote = createRemote(elt, offset, cb);\n          remote.watch();\n          return remote;\n        }\n\n        function createRemote(elt, offset, cb) {\n          function watch() {\n            watches.add(elt, offset, cb);\n          }\n\n          function dispose() {\n            watches.remove(elt);\n          }\n\n          return {\n            watch: watch,\n            dispose: dispose\n          };\n        }\n\n        function watchInViewport(elt, offset, cb) {\n          if (isVisible(elt, offset)) {\n            watches.remove(elt);\n            cb(elt);\n          }\n        }\n\n        function isVisible(elt, offset) {\n          if (!contains(global.document.documentElement, elt) || !contains(global.document.documentElement, container)) {\n            return false;\n          }\n\n          // Check if the element is visible\n          // https://github.com/jquery/jquery/blob/740e190223d19a114d5373758127285d14d6b71e/src/css/hiddenVisibleSelectors.js\n          if (!elt.offsetWidth || !elt.offsetHeight) {\n            return false;\n          }\n\n          var eltRect = elt.getBoundingClientRect();\n          var viewport = {};\n\n          if (container === global.document.body) {\n            viewport = {\n              top: -offset,\n              left: -offset,\n              right: global.document.documentElement.clientWidth + offset,\n              bottom: global.document.documentElement.clientHeight + offset\n            };\n          } else {\n            var containerRect = container.getBoundingClientRect();\n            viewport = {\n              top: containerRect.top - offset,\n              left: containerRect.left - offset,\n              right: containerRect.right + offset,\n              bottom: containerRect.bottom + offset\n            };\n          }\n\n          // The element must overlap with the visible part of the viewport\n          var visible =\n            (\n              (eltRect.right > viewport.left) &&\n              (eltRect.left < viewport.right) &&\n              (eltRect.bottom > viewport.top) &&\n              (eltRect.top < viewport.bottom)\n            );\n\n          return visible;\n        }\n\n        return {\n          container: container,\n          isInViewport: isInViewport\n        };\n      }\n\n      function createWatches() {\n        var watches = [];\n\n        function add(elt, offset, cb) {\n          if (!isWatched(elt)) {\n            watches.push([elt, offset, cb]);\n          }\n        }\n\n        function remove(elt) {\n          var pos = indexOf(elt);\n          if (pos !== -1) {\n            watches.splice(pos, 1);\n          }\n        }\n\n        function indexOf(elt) {\n          for (var i = watches.length - 1; i >= 0; i--) {\n            if (watches[i][0] === elt) {\n              return i;\n            }\n          }\n          return -1;\n        }\n\n        function isWatched(elt) {\n          return indexOf(elt) !== -1;\n        }\n\n        function checkAll(cb) {\n          return function() {\n            for (var i = watches.length - 1; i >= 0; i--) {\n              cb.apply(this, watches[i]);\n            }\n          };\n        }\n\n        return {\n          add: add,\n          remove: remove,\n          isWatched: isWatched,\n          checkAll: checkAll\n        };\n      }\n\n      function observeDOM(watches, container, cb) {\n        var observer = new MutationObserver(watch);\n        var filter = Array.prototype.filter;\n        var concat = Array.prototype.concat;\n\n        observer.observe(container, {\n          childList: true,\n          subtree: true,\n          // changes like style/width/height/display will be catched\n          attributes: true\n        });\n\n        function watch(mutations) {\n          // some new DOM nodes where previously watched\n          // we should check their positions\n          if (mutations.some(knownNodes) === true) {\n            setTimeout(cb, 0);\n          }\n        }\n\n        function knownNodes(mutation) {\n          var nodes = concat.call([],\n            Array.prototype.slice.call(mutation.addedNodes),\n            mutation.target\n          );\n          return filter.call(nodes, watches.isWatched).length > 0;\n        }\n      }\n\n      /* WEBPACK VAR INJECTION */\n    }.call(exports, (function() {\n      return this;\n    }())))\n\n    /***/\n  }\n  /******/\n]);"},{"_content":"{\n    \"list\": [\n        {\n            \"date\": \"2017-11\", \n            \"arr\": {\n                \"text\": [\n                    \"0009\", \n                    \"0008\",\n                    \"0006\",\n                    \"0010\"\n                ], \n                \"type\": [\n                    \"image\", \n                    \"image\",\n                    \"image\",\n                    \"image\"\n                ], \n                \"month\": 11, \n                \"link\": [\n                    \"2017-11-03_0009.jpg\", \n                    \"2017-11-03_0008.jpg\",\n                    \"2017-11-03_0006.jpg\",\n                    \"2017-11-03_0010.jpg\"\n                ], \n                \"year\": 2017\n            }\n        },\n        {\n            \"date\": \"2017-10\", \n            \"arr\": {\n                \"text\": [\n                    \"0003\", \n                    \"0000\", \n                    \"0001\",\n                    \"0005\", \n                    \"0004\"\n                ], \n                \"type\": [\n                    \"image\", \n                    \"image\", \n                    \"image\",\n                    \"image\", \n                    \"image\"\n                ], \n                \"month\": 10, \n                \"link\": [\n                    \"2017-10-22_0003.jpg\", \n                    \"2017-10-22_0000.jpg\", \n                    \"2017-10-22_0001.jpg\",\n                    \"2017-10-22_0005.jpg\", \n                    \"2017-10-22_0004.jpg\"\n                ], \n                \"year\": 2017\n            }\n        }, \n        {\n            \"date\": \"2016-08\", \n            \"arr\": {\n                \"text\": [\n                    \"0259\",\n                    \"0331\", \n                    \"1407\"\n                ], \n                \"type\": [\n                    \"image\", \n                    \"image\",\n                    \"image\"\n                ], \n                \"month\": 8, \n                \"link\": [\n                    \"2016-08-24_0259.jpg\",\n                    \"2016-08-24_0331.jpg\", \n                    \"2016-08-24_1407.jpg\"\n                ], \n                \"year\": 2016\n            }\n        }, \n        {\n            \"date\": \"2016-04\", \n            \"arr\": {\n                \"text\": [\n                    \"1319\"\n                ], \n                \"type\": [\n                    \"image\"\n                ], \n                \"month\": 4, \n                \"link\": [\n                    \"2016-04-04_1319.jpg\"\n                ], \n                \"year\": 2016\n            }\n        }\n    ]\n}","source":"photos/ins.json","raw":"{\n    \"list\": [\n        {\n            \"date\": \"2017-11\", \n            \"arr\": {\n                \"text\": [\n                    \"0009\", \n                    \"0008\",\n                    \"0006\",\n                    \"0010\"\n                ], \n                \"type\": [\n                    \"image\", \n                    \"image\",\n                    \"image\",\n                    \"image\"\n                ], \n                \"month\": 11, \n                \"link\": [\n                    \"2017-11-03_0009.jpg\", \n                    \"2017-11-03_0008.jpg\",\n                    \"2017-11-03_0006.jpg\",\n                    \"2017-11-03_0010.jpg\"\n                ], \n                \"year\": 2017\n            }\n        },\n        {\n            \"date\": \"2017-10\", \n            \"arr\": {\n                \"text\": [\n                    \"0003\", \n                    \"0000\", \n                    \"0001\",\n                    \"0005\", \n                    \"0004\"\n                ], \n                \"type\": [\n                    \"image\", \n                    \"image\", \n                    \"image\",\n                    \"image\", \n                    \"image\"\n                ], \n                \"month\": 10, \n                \"link\": [\n                    \"2017-10-22_0003.jpg\", \n                    \"2017-10-22_0000.jpg\", \n                    \"2017-10-22_0001.jpg\",\n                    \"2017-10-22_0005.jpg\", \n                    \"2017-10-22_0004.jpg\"\n                ], \n                \"year\": 2017\n            }\n        }, \n        {\n            \"date\": \"2016-08\", \n            \"arr\": {\n                \"text\": [\n                    \"0259\",\n                    \"0331\", \n                    \"1407\"\n                ], \n                \"type\": [\n                    \"image\", \n                    \"image\",\n                    \"image\"\n                ], \n                \"month\": 8, \n                \"link\": [\n                    \"2016-08-24_0259.jpg\",\n                    \"2016-08-24_0331.jpg\", \n                    \"2016-08-24_1407.jpg\"\n                ], \n                \"year\": 2016\n            }\n        }, \n        {\n            \"date\": \"2016-04\", \n            \"arr\": {\n                \"text\": [\n                    \"1319\"\n                ], \n                \"type\": [\n                    \"image\"\n                ], \n                \"month\": 4, \n                \"link\": [\n                    \"2016-04-04_1319.jpg\"\n                ], \n                \"year\": 2016\n            }\n        }\n    ]\n}","date":"2017-12-19T02:54:56.795Z","updated":"2017-12-19T02:54:56.795Z","path":"photos/ins.json","layout":"false","title":"","comments":1,"_id":"cjbd1r2560004kxuw4z76pppn","content":"{\"list\":[{\"date\":\"2017-11\",\"arr\":{\"text\":[\"0009\",\"0008\",\"0006\",\"0010\"],\"type\":[\"image\",\"image\",\"image\",\"image\"],\"month\":11,\"link\":[\"2017-11-03_0009.jpg\",\"2017-11-03_0008.jpg\",\"2017-11-03_0006.jpg\",\"2017-11-03_0010.jpg\"],\"year\":2017}},{\"date\":\"2017-10\",\"arr\":{\"text\":[\"0003\",\"0000\",\"0001\",\"0005\",\"0004\"],\"type\":[\"image\",\"image\",\"image\",\"image\",\"image\"],\"month\":10,\"link\":[\"2017-10-22_0003.jpg\",\"2017-10-22_0000.jpg\",\"2017-10-22_0001.jpg\",\"2017-10-22_0005.jpg\",\"2017-10-22_0004.jpg\"],\"year\":2017}},{\"date\":\"2016-08\",\"arr\":{\"text\":[\"0259\",\"0331\",\"1407\"],\"type\":[\"image\",\"image\",\"image\"],\"month\":8,\"link\":[\"2016-08-24_0259.jpg\",\"2016-08-24_0331.jpg\",\"2016-08-24_1407.jpg\"],\"year\":2016}},{\"date\":\"2016-04\",\"arr\":{\"text\":[\"1319\"],\"type\":[\"image\"],\"month\":4,\"link\":[\"2016-04-04_1319.jpg\"],\"year\":2016}}]}","site":{"data":{}},"excerpt":"","more":"{\"list\":[{\"date\":\"2017-11\",\"arr\":{\"text\":[\"0009\",\"0008\",\"0006\",\"0010\"],\"type\":[\"image\",\"image\",\"image\",\"image\"],\"month\":11,\"link\":[\"2017-11-03_0009.jpg\",\"2017-11-03_0008.jpg\",\"2017-11-03_0006.jpg\",\"2017-11-03_0010.jpg\"],\"year\":2017}},{\"date\":\"2017-10\",\"arr\":{\"text\":[\"0003\",\"0000\",\"0001\",\"0005\",\"0004\"],\"type\":[\"image\",\"image\",\"image\",\"image\",\"image\"],\"month\":10,\"link\":[\"2017-10-22_0003.jpg\",\"2017-10-22_0000.jpg\",\"2017-10-22_0001.jpg\",\"2017-10-22_0005.jpg\",\"2017-10-22_0004.jpg\"],\"year\":2017}},{\"date\":\"2016-08\",\"arr\":{\"text\":[\"0259\",\"0331\",\"1407\"],\"type\":[\"image\",\"image\",\"image\"],\"month\":8,\"link\":[\"2016-08-24_0259.jpg\",\"2016-08-24_0331.jpg\",\"2016-08-24_1407.jpg\"],\"year\":2016}},{\"date\":\"2016-04\",\"arr\":{\"text\":[\"1319\"],\"type\":[\"image\"],\"month\":4,\"link\":[\"2016-04-04_1319.jpg\"],\"year\":2016}}]}"}],"Post":[{"title":"Elasticsearch-DSL部分集合","date":"2017-11-14T09:26:48.000Z","_content":"\nELK是日志收集分析神器，在这篇文章中将会介绍一些ES的常用命令。\n\n点击阅读：[ELK Stack 从入门到放弃](http://blog.csdn.net/column/details/13079.html)\n<!--More-->\n# DSL中遇到的错误及解决办法\n## 分片限制错误\n```\nTrying to query 2632 shards, which is over the limit of 1000. This limit exists because querying many shards at the same time can make the job of the coordinating node very CPU and/or memory intensive. It is usually a better idea to have a smaller number of larger shards. Update [action.search.shard_count.limit] to a greater value if you really want to query that many shards at the same time.\n```\n\n解决办法：\n```\n修改该限制数目\n\ncurl -k -u admin:admin -XPUT 'http://localhost:9200/_cluster/settings' -H 'Content-Type: application/json' -d' \n{\n    \"persistent\" : {\n        \"action.search.shard_count.limit\" : \"5000\"\n    }\n}\n'\n\n-k -u admin:admin 表述如果有权限保护的话可以加上\n```\n\n## Fileddate 错误\n```\nFielddata is disabled on text fields by default. Set fielddata=true on [make] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory.\n```\n\n解决办法：\n```\ncars: 索引名\ntransactions：索引对应的类型\ncolor：字段\n\ncurl -XPUT -k -u admin:admin 'localhost:9200/cars/_mapping/transactions?pretty' -H 'Content-Type: application/json' -d'\n{\n  \"properties\": {\n    \"color\": { \n      \"type\":     \"text\",\n      \"fielddata\": true\n    }\n  }\n}\n'\n```\n\n# 指定关键词查询，排序和函数统计\n## 指定关键词\n\nfrom 为首个偏移量，size为返回数据的条数\n```\nhttp://10.10.11.139:9200/logstash-nginx-access-*/nginx-access/_search?pretty\n\n{\n    \"from\":0,size\":1000,\n    \"query\" : {\n        \"term\" : { \n        \t\"major\" : \"55\"\n        }\n    }\n}\n```\n\n## 添加排序\n\n(需要进行mapping设置，asc 为升序  desc为降序)\n```\n{\n    \"from\":0,\"size\":1000,\n    \"sort\":[\n        {\"offset\":\"desc\"}\n    ],\n    \"query\" : {\n        \"term\" : { \n            \"major\" : \"55\"\n        }\n    }\n}\n```\n\n## mode 方法\nmode方法包括 min／max／avg／sum／median\n\n假如现在要对price字段进行排序，但是price字段有多个值，这个时候就可以使用mode 方法了。\n\n```\n{\n   \"query\" : {\n      \"term\" : { \"product\" : \"chocolate\" }\n   },\n   \"sort\" : [\n      {\"price\" : {\"order\" : \"asc\", \"mode\" : \"avg\"}}\n   ]\n}\n```\n\n# IP范围和网段查询\n## IP range 搜索\n\n错误：\n```\nFielddata is disabled on text fields by default. Set fielddata=true on [clientip] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory.\n```\n\n解决办法：\n```\ncurl -k -u admin:admin -XPUT '10.10.11.139:9200/logstash-nginx-access-*/_mapping/nginx-access?pretty' -H 'Content-Type: application/json' -d'\n{\n  \"properties\": {\n    \"clientip\": { \n      \"type\":     \"text\",\n      \"fielddata\": true,\n      \"norms\": false\n    }\n  }\n}\n'\n```\n\n查看某个索引的mapping\n```\ncurl -k -u admin:admin -XGET http://10.10.11.139:9200/logstash-nginx-access-*/_mapping?pretty\n```\n\n(当IP为不可解析使就会出现错误)\n```\nhttp://10.10.11.139:9200/logstash-sshlogin-others-success-*/zhongcai/_search?pretty\n{\n    \"size\":100,\n    \"aggs\" : {\n        \"ip_ranges\" : {\n            \"ip_range\" : {\n                \"field\" : \"clientip\",\n                \"ranges\" : [\n                    { \"to\" : \"40.77.167.73\" },\n                    { \"from\" : \"40.77.167.75\" }\n                ]\n            }\n        }\n    }\n}\n```\n\n## 网段查询\n```\nhttp://10.10.11.139:9200/logstash-sshlogin-others-success-*/zhongcai/_search?pretty\n{\n    \"aggs\" : {\n        \"ip_ranges\" : {\n            \"ip_range\" : {\n                \"field\" : \"ip\",\n                \"ranges\" : [\n                    { \"mask\" : \"172.21.202.0/24\" },\n                    { \"mask\" : \"172.21.202.0/24\" }\n                ]\n            }\n        }\n    }\n}\n```\n\n# 关于索引的操作\n\n## 删除某个索引\n-k -u admin:admin 为用户名：密码\n```\ncurl -XDELETE  -k -u admin:admin 'http://localhost:9200/my_index'\n```\n\n\n## 查看某个索引的Mapping\n```\ncurl -XGET \"http://127.0.0.1:9200/my_index/_mapping?pretty\"\n```\n\n## 索引数据迁移\n\nEs索引reindex(从ip_remote上迁移到本地)\n```\ncurl -XPOST 'localhost:9200/_reindex?pretty' -H 'Content-Type: application/json' -d'\n{\n  \"source\": {\n    \"remote\": {\n      \"host\": \"http://ip_remote:9200\",\n      \"username\": \"username\",\n      \"password\": \"passwd\"\n    },\n    \"index\": \"old_index\"\n  },\n  \"dest\": {\n    \"index\": \"new_index\"\n  }\n}\n'\n```\n\n## 为某个索引添加字段\n添加number字段：\n### 唯一ID\n```\ncurl -POST 'http://127.0.0.1:9200/my_idnex/my_index_type/id/_update?pretty' -H 'Content-Type: application/json' -d'\n{\n   \"doc\" : {\n      \"number\" : 1\n   }\n}\n'\n```\n### 批量操作\n```\ncurl -XPOST 'localhost:9200/logstash-sshlogin-others-success-2017-*/_update_by_query?pretty' -H 'Content-Type: application/json' -d'\n{\n  \"script\": {\n    \"inline\": \"ctx._source.number=1\",\n    \"lang\": \"painless\"\n  },\n  \"query\": {\n    \"match_all\": {\n    }\n  }\n}\n'\n\n```\n\n# 根据指定条件进行聚合\n每小时成功登录的次数进行聚合\n```\ncurl -POST 'http://127.0.0.1:9200/logstash-sshlogin-others-success-2017-*/zhongcai-sshlogin/_search?pretty' -H 'Content-Type: application/json' -d'\n{\n  \"query\": {\n    \"term\": {\n      \"ssh_type\": \"ssh_successful_login\"\n    }\n  },\n  \"aggs\": {\n    \"sums\": {\n      \"date_histogram\": {\n        \"field\": \"@timestamp\",\n        \"interval\": \"hour\",\n        \"format\": \"yyyy-MM-dd HH\"\n      }\n    }\n  }\n}\n'\n```","source":"_posts/ELK/Elasticsearch-DSL部分集合.md","raw":"---\ntitle: Elasticsearch-DSL部分集合\ndate: 2017-11-14 17:26:48\ntags: [ELK,ES]\ncategories: 技术篇\n---\n\nELK是日志收集分析神器，在这篇文章中将会介绍一些ES的常用命令。\n\n点击阅读：[ELK Stack 从入门到放弃](http://blog.csdn.net/column/details/13079.html)\n<!--More-->\n# DSL中遇到的错误及解决办法\n## 分片限制错误\n```\nTrying to query 2632 shards, which is over the limit of 1000. This limit exists because querying many shards at the same time can make the job of the coordinating node very CPU and/or memory intensive. It is usually a better idea to have a smaller number of larger shards. Update [action.search.shard_count.limit] to a greater value if you really want to query that many shards at the same time.\n```\n\n解决办法：\n```\n修改该限制数目\n\ncurl -k -u admin:admin -XPUT 'http://localhost:9200/_cluster/settings' -H 'Content-Type: application/json' -d' \n{\n    \"persistent\" : {\n        \"action.search.shard_count.limit\" : \"5000\"\n    }\n}\n'\n\n-k -u admin:admin 表述如果有权限保护的话可以加上\n```\n\n## Fileddate 错误\n```\nFielddata is disabled on text fields by default. Set fielddata=true on [make] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory.\n```\n\n解决办法：\n```\ncars: 索引名\ntransactions：索引对应的类型\ncolor：字段\n\ncurl -XPUT -k -u admin:admin 'localhost:9200/cars/_mapping/transactions?pretty' -H 'Content-Type: application/json' -d'\n{\n  \"properties\": {\n    \"color\": { \n      \"type\":     \"text\",\n      \"fielddata\": true\n    }\n  }\n}\n'\n```\n\n# 指定关键词查询，排序和函数统计\n## 指定关键词\n\nfrom 为首个偏移量，size为返回数据的条数\n```\nhttp://10.10.11.139:9200/logstash-nginx-access-*/nginx-access/_search?pretty\n\n{\n    \"from\":0,size\":1000,\n    \"query\" : {\n        \"term\" : { \n        \t\"major\" : \"55\"\n        }\n    }\n}\n```\n\n## 添加排序\n\n(需要进行mapping设置，asc 为升序  desc为降序)\n```\n{\n    \"from\":0,\"size\":1000,\n    \"sort\":[\n        {\"offset\":\"desc\"}\n    ],\n    \"query\" : {\n        \"term\" : { \n            \"major\" : \"55\"\n        }\n    }\n}\n```\n\n## mode 方法\nmode方法包括 min／max／avg／sum／median\n\n假如现在要对price字段进行排序，但是price字段有多个值，这个时候就可以使用mode 方法了。\n\n```\n{\n   \"query\" : {\n      \"term\" : { \"product\" : \"chocolate\" }\n   },\n   \"sort\" : [\n      {\"price\" : {\"order\" : \"asc\", \"mode\" : \"avg\"}}\n   ]\n}\n```\n\n# IP范围和网段查询\n## IP range 搜索\n\n错误：\n```\nFielddata is disabled on text fields by default. Set fielddata=true on [clientip] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory.\n```\n\n解决办法：\n```\ncurl -k -u admin:admin -XPUT '10.10.11.139:9200/logstash-nginx-access-*/_mapping/nginx-access?pretty' -H 'Content-Type: application/json' -d'\n{\n  \"properties\": {\n    \"clientip\": { \n      \"type\":     \"text\",\n      \"fielddata\": true,\n      \"norms\": false\n    }\n  }\n}\n'\n```\n\n查看某个索引的mapping\n```\ncurl -k -u admin:admin -XGET http://10.10.11.139:9200/logstash-nginx-access-*/_mapping?pretty\n```\n\n(当IP为不可解析使就会出现错误)\n```\nhttp://10.10.11.139:9200/logstash-sshlogin-others-success-*/zhongcai/_search?pretty\n{\n    \"size\":100,\n    \"aggs\" : {\n        \"ip_ranges\" : {\n            \"ip_range\" : {\n                \"field\" : \"clientip\",\n                \"ranges\" : [\n                    { \"to\" : \"40.77.167.73\" },\n                    { \"from\" : \"40.77.167.75\" }\n                ]\n            }\n        }\n    }\n}\n```\n\n## 网段查询\n```\nhttp://10.10.11.139:9200/logstash-sshlogin-others-success-*/zhongcai/_search?pretty\n{\n    \"aggs\" : {\n        \"ip_ranges\" : {\n            \"ip_range\" : {\n                \"field\" : \"ip\",\n                \"ranges\" : [\n                    { \"mask\" : \"172.21.202.0/24\" },\n                    { \"mask\" : \"172.21.202.0/24\" }\n                ]\n            }\n        }\n    }\n}\n```\n\n# 关于索引的操作\n\n## 删除某个索引\n-k -u admin:admin 为用户名：密码\n```\ncurl -XDELETE  -k -u admin:admin 'http://localhost:9200/my_index'\n```\n\n\n## 查看某个索引的Mapping\n```\ncurl -XGET \"http://127.0.0.1:9200/my_index/_mapping?pretty\"\n```\n\n## 索引数据迁移\n\nEs索引reindex(从ip_remote上迁移到本地)\n```\ncurl -XPOST 'localhost:9200/_reindex?pretty' -H 'Content-Type: application/json' -d'\n{\n  \"source\": {\n    \"remote\": {\n      \"host\": \"http://ip_remote:9200\",\n      \"username\": \"username\",\n      \"password\": \"passwd\"\n    },\n    \"index\": \"old_index\"\n  },\n  \"dest\": {\n    \"index\": \"new_index\"\n  }\n}\n'\n```\n\n## 为某个索引添加字段\n添加number字段：\n### 唯一ID\n```\ncurl -POST 'http://127.0.0.1:9200/my_idnex/my_index_type/id/_update?pretty' -H 'Content-Type: application/json' -d'\n{\n   \"doc\" : {\n      \"number\" : 1\n   }\n}\n'\n```\n### 批量操作\n```\ncurl -XPOST 'localhost:9200/logstash-sshlogin-others-success-2017-*/_update_by_query?pretty' -H 'Content-Type: application/json' -d'\n{\n  \"script\": {\n    \"inline\": \"ctx._source.number=1\",\n    \"lang\": \"painless\"\n  },\n  \"query\": {\n    \"match_all\": {\n    }\n  }\n}\n'\n\n```\n\n# 根据指定条件进行聚合\n每小时成功登录的次数进行聚合\n```\ncurl -POST 'http://127.0.0.1:9200/logstash-sshlogin-others-success-2017-*/zhongcai-sshlogin/_search?pretty' -H 'Content-Type: application/json' -d'\n{\n  \"query\": {\n    \"term\": {\n      \"ssh_type\": \"ssh_successful_login\"\n    }\n  },\n  \"aggs\": {\n    \"sums\": {\n      \"date_histogram\": {\n        \"field\": \"@timestamp\",\n        \"interval\": \"hour\",\n        \"format\": \"yyyy-MM-dd HH\"\n      }\n    }\n  }\n}\n'\n```","slug":"ELK/Elasticsearch-DSL部分集合","published":1,"updated":"2017-12-19T02:54:56.787Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjbd1r25k0005kxuw9e1ibwab","content":"<p>ELK是日志收集分析神器，在这篇文章中将会介绍一些ES的常用命令。</p>\n<p>点击阅读：<a href=\"http://blog.csdn.net/column/details/13079.html\" target=\"_blank\" rel=\"external\">ELK Stack 从入门到放弃</a><br><a id=\"more\"></a></p>\n<h1 id=\"DSL中遇到的错误及解决办法\"><a href=\"#DSL中遇到的错误及解决办法\" class=\"headerlink\" title=\"DSL中遇到的错误及解决办法\"></a>DSL中遇到的错误及解决办法</h1><h2 id=\"分片限制错误\"><a href=\"#分片限制错误\" class=\"headerlink\" title=\"分片限制错误\"></a>分片限制错误</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Trying to query 2632 shards, which is over the limit of 1000. This limit exists because querying many shards at the same time can make the job of the coordinating node very CPU and/or memory intensive. It is usually a better idea to have a smaller number of larger shards. Update [action.search.shard_count.limit] to a greater value if you really want to query that many shards at the same time.</span><br></pre></td></tr></table></figure>\n<p>解决办法：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">修改该限制数目</span><br><span class=\"line\"></span><br><span class=\"line\">curl -k -u admin:admin -XPUT &apos;http://localhost:9200/_cluster/settings&apos; -H &apos;Content-Type: application/json&apos; -d&apos; </span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;persistent&quot; : &#123;</span><br><span class=\"line\">        &quot;action.search.shard_count.limit&quot; : &quot;5000&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&apos;</span><br><span class=\"line\"></span><br><span class=\"line\">-k -u admin:admin 表述如果有权限保护的话可以加上</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"Fileddate-错误\"><a href=\"#Fileddate-错误\" class=\"headerlink\" title=\"Fileddate 错误\"></a>Fileddate 错误</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Fielddata is disabled on text fields by default. Set fielddata=true on [make] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory.</span><br></pre></td></tr></table></figure>\n<p>解决办法：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cars: 索引名</span><br><span class=\"line\">transactions：索引对应的类型</span><br><span class=\"line\">color：字段</span><br><span class=\"line\"></span><br><span class=\"line\">curl -XPUT -k -u admin:admin &apos;localhost:9200/cars/_mapping/transactions?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;properties&quot;: &#123;</span><br><span class=\"line\">    &quot;color&quot;: &#123; </span><br><span class=\"line\">      &quot;type&quot;:     &quot;text&quot;,</span><br><span class=\"line\">      &quot;fielddata&quot;: true</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&apos;</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"指定关键词查询，排序和函数统计\"><a href=\"#指定关键词查询，排序和函数统计\" class=\"headerlink\" title=\"指定关键词查询，排序和函数统计\"></a>指定关键词查询，排序和函数统计</h1><h2 id=\"指定关键词\"><a href=\"#指定关键词\" class=\"headerlink\" title=\"指定关键词\"></a>指定关键词</h2><p>from 为首个偏移量，size为返回数据的条数<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">http://10.10.11.139:9200/logstash-nginx-access-*/nginx-access/_search?pretty</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;from&quot;:0,size&quot;:1000,</span><br><span class=\"line\">    &quot;query&quot; : &#123;</span><br><span class=\"line\">        &quot;term&quot; : &#123; </span><br><span class=\"line\">        \t&quot;major&quot; : &quot;55&quot;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"添加排序\"><a href=\"#添加排序\" class=\"headerlink\" title=\"添加排序\"></a>添加排序</h2><p>(需要进行mapping设置，asc 为升序  desc为降序)<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;from&quot;:0,&quot;size&quot;:1000,</span><br><span class=\"line\">    &quot;sort&quot;:[</span><br><span class=\"line\">        &#123;&quot;offset&quot;:&quot;desc&quot;&#125;</span><br><span class=\"line\">    ],</span><br><span class=\"line\">    &quot;query&quot; : &#123;</span><br><span class=\"line\">        &quot;term&quot; : &#123; </span><br><span class=\"line\">            &quot;major&quot; : &quot;55&quot;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"mode-方法\"><a href=\"#mode-方法\" class=\"headerlink\" title=\"mode 方法\"></a>mode 方法</h2><p>mode方法包括 min／max／avg／sum／median</p>\n<p>假如现在要对price字段进行排序，但是price字段有多个值，这个时候就可以使用mode 方法了。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">   &quot;query&quot; : &#123;</span><br><span class=\"line\">      &quot;term&quot; : &#123; &quot;product&quot; : &quot;chocolate&quot; &#125;</span><br><span class=\"line\">   &#125;,</span><br><span class=\"line\">   &quot;sort&quot; : [</span><br><span class=\"line\">      &#123;&quot;price&quot; : &#123;&quot;order&quot; : &quot;asc&quot;, &quot;mode&quot; : &quot;avg&quot;&#125;&#125;</span><br><span class=\"line\">   ]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h1 id=\"IP范围和网段查询\"><a href=\"#IP范围和网段查询\" class=\"headerlink\" title=\"IP范围和网段查询\"></a>IP范围和网段查询</h1><h2 id=\"IP-range-搜索\"><a href=\"#IP-range-搜索\" class=\"headerlink\" title=\"IP range 搜索\"></a>IP range 搜索</h2><p>错误：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Fielddata is disabled on text fields by default. Set fielddata=true on [clientip] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory.</span><br></pre></td></tr></table></figure></p>\n<p>解决办法：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -k -u admin:admin -XPUT &apos;10.10.11.139:9200/logstash-nginx-access-*/_mapping/nginx-access?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;properties&quot;: &#123;</span><br><span class=\"line\">    &quot;clientip&quot;: &#123; </span><br><span class=\"line\">      &quot;type&quot;:     &quot;text&quot;,</span><br><span class=\"line\">      &quot;fielddata&quot;: true,</span><br><span class=\"line\">      &quot;norms&quot;: false</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&apos;</span><br></pre></td></tr></table></figure></p>\n<p>查看某个索引的mapping<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -k -u admin:admin -XGET http://10.10.11.139:9200/logstash-nginx-access-*/_mapping?pretty</span><br></pre></td></tr></table></figure></p>\n<p>(当IP为不可解析使就会出现错误)<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">http://10.10.11.139:9200/logstash-sshlogin-others-success-*/zhongcai/_search?pretty</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;size&quot;:100,</span><br><span class=\"line\">    &quot;aggs&quot; : &#123;</span><br><span class=\"line\">        &quot;ip_ranges&quot; : &#123;</span><br><span class=\"line\">            &quot;ip_range&quot; : &#123;</span><br><span class=\"line\">                &quot;field&quot; : &quot;clientip&quot;,</span><br><span class=\"line\">                &quot;ranges&quot; : [</span><br><span class=\"line\">                    &#123; &quot;to&quot; : &quot;40.77.167.73&quot; &#125;,</span><br><span class=\"line\">                    &#123; &quot;from&quot; : &quot;40.77.167.75&quot; &#125;</span><br><span class=\"line\">                ]</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"网段查询\"><a href=\"#网段查询\" class=\"headerlink\" title=\"网段查询\"></a>网段查询</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">http://10.10.11.139:9200/logstash-sshlogin-others-success-*/zhongcai/_search?pretty</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;aggs&quot; : &#123;</span><br><span class=\"line\">        &quot;ip_ranges&quot; : &#123;</span><br><span class=\"line\">            &quot;ip_range&quot; : &#123;</span><br><span class=\"line\">                &quot;field&quot; : &quot;ip&quot;,</span><br><span class=\"line\">                &quot;ranges&quot; : [</span><br><span class=\"line\">                    &#123; &quot;mask&quot; : &quot;172.21.202.0/24&quot; &#125;,</span><br><span class=\"line\">                    &#123; &quot;mask&quot; : &quot;172.21.202.0/24&quot; &#125;</span><br><span class=\"line\">                ]</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h1 id=\"关于索引的操作\"><a href=\"#关于索引的操作\" class=\"headerlink\" title=\"关于索引的操作\"></a>关于索引的操作</h1><h2 id=\"删除某个索引\"><a href=\"#删除某个索引\" class=\"headerlink\" title=\"删除某个索引\"></a>删除某个索引</h2><p>-k -u admin:admin 为用户名：密码<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XDELETE  -k -u admin:admin &apos;http://localhost:9200/my_index&apos;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"查看某个索引的Mapping\"><a href=\"#查看某个索引的Mapping\" class=\"headerlink\" title=\"查看某个索引的Mapping\"></a>查看某个索引的Mapping</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XGET &quot;http://127.0.0.1:9200/my_index/_mapping?pretty&quot;</span><br></pre></td></tr></table></figure>\n<h2 id=\"索引数据迁移\"><a href=\"#索引数据迁移\" class=\"headerlink\" title=\"索引数据迁移\"></a>索引数据迁移</h2><p>Es索引reindex(从ip_remote上迁移到本地)<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XPOST &apos;localhost:9200/_reindex?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;source&quot;: &#123;</span><br><span class=\"line\">    &quot;remote&quot;: &#123;</span><br><span class=\"line\">      &quot;host&quot;: &quot;http://ip_remote:9200&quot;,</span><br><span class=\"line\">      &quot;username&quot;: &quot;username&quot;,</span><br><span class=\"line\">      &quot;password&quot;: &quot;passwd&quot;</span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    &quot;index&quot;: &quot;old_index&quot;</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;dest&quot;: &#123;</span><br><span class=\"line\">    &quot;index&quot;: &quot;new_index&quot;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&apos;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"为某个索引添加字段\"><a href=\"#为某个索引添加字段\" class=\"headerlink\" title=\"为某个索引添加字段\"></a>为某个索引添加字段</h2><p>添加number字段：</p>\n<h3 id=\"唯一ID\"><a href=\"#唯一ID\" class=\"headerlink\" title=\"唯一ID\"></a>唯一ID</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -POST &apos;http://127.0.0.1:9200/my_idnex/my_index_type/id/_update?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">   &quot;doc&quot; : &#123;</span><br><span class=\"line\">      &quot;number&quot; : 1</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&apos;</span><br></pre></td></tr></table></figure>\n<h3 id=\"批量操作\"><a href=\"#批量操作\" class=\"headerlink\" title=\"批量操作\"></a>批量操作</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XPOST &apos;localhost:9200/logstash-sshlogin-others-success-2017-*/_update_by_query?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;script&quot;: &#123;</span><br><span class=\"line\">    &quot;inline&quot;: &quot;ctx._source.number=1&quot;,</span><br><span class=\"line\">    &quot;lang&quot;: &quot;painless&quot;</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;query&quot;: &#123;</span><br><span class=\"line\">    &quot;match_all&quot;: &#123;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&apos;</span><br></pre></td></tr></table></figure>\n<h1 id=\"根据指定条件进行聚合\"><a href=\"#根据指定条件进行聚合\" class=\"headerlink\" title=\"根据指定条件进行聚合\"></a>根据指定条件进行聚合</h1><p>每小时成功登录的次数进行聚合<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -POST &apos;http://127.0.0.1:9200/logstash-sshlogin-others-success-2017-*/zhongcai-sshlogin/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;: &#123;</span><br><span class=\"line\">    &quot;term&quot;: &#123;</span><br><span class=\"line\">      &quot;ssh_type&quot;: &quot;ssh_successful_login&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;aggs&quot;: &#123;</span><br><span class=\"line\">    &quot;sums&quot;: &#123;</span><br><span class=\"line\">      &quot;date_histogram&quot;: &#123;</span><br><span class=\"line\">        &quot;field&quot;: &quot;@timestamp&quot;,</span><br><span class=\"line\">        &quot;interval&quot;: &quot;hour&quot;,</span><br><span class=\"line\">        &quot;format&quot;: &quot;yyyy-MM-dd HH&quot;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&apos;</span><br></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"<p>ELK是日志收集分析神器，在这篇文章中将会介绍一些ES的常用命令。</p>\n<p>点击阅读：<a href=\"http://blog.csdn.net/column/details/13079.html\" target=\"_blank\" rel=\"external\">ELK Stack 从入门到放弃</a><br>","more":"</p>\n<h1 id=\"DSL中遇到的错误及解决办法\"><a href=\"#DSL中遇到的错误及解决办法\" class=\"headerlink\" title=\"DSL中遇到的错误及解决办法\"></a>DSL中遇到的错误及解决办法</h1><h2 id=\"分片限制错误\"><a href=\"#分片限制错误\" class=\"headerlink\" title=\"分片限制错误\"></a>分片限制错误</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Trying to query 2632 shards, which is over the limit of 1000. This limit exists because querying many shards at the same time can make the job of the coordinating node very CPU and/or memory intensive. It is usually a better idea to have a smaller number of larger shards. Update [action.search.shard_count.limit] to a greater value if you really want to query that many shards at the same time.</span><br></pre></td></tr></table></figure>\n<p>解决办法：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">修改该限制数目</span><br><span class=\"line\"></span><br><span class=\"line\">curl -k -u admin:admin -XPUT &apos;http://localhost:9200/_cluster/settings&apos; -H &apos;Content-Type: application/json&apos; -d&apos; </span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;persistent&quot; : &#123;</span><br><span class=\"line\">        &quot;action.search.shard_count.limit&quot; : &quot;5000&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&apos;</span><br><span class=\"line\"></span><br><span class=\"line\">-k -u admin:admin 表述如果有权限保护的话可以加上</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"Fileddate-错误\"><a href=\"#Fileddate-错误\" class=\"headerlink\" title=\"Fileddate 错误\"></a>Fileddate 错误</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Fielddata is disabled on text fields by default. Set fielddata=true on [make] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory.</span><br></pre></td></tr></table></figure>\n<p>解决办法：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cars: 索引名</span><br><span class=\"line\">transactions：索引对应的类型</span><br><span class=\"line\">color：字段</span><br><span class=\"line\"></span><br><span class=\"line\">curl -XPUT -k -u admin:admin &apos;localhost:9200/cars/_mapping/transactions?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;properties&quot;: &#123;</span><br><span class=\"line\">    &quot;color&quot;: &#123; </span><br><span class=\"line\">      &quot;type&quot;:     &quot;text&quot;,</span><br><span class=\"line\">      &quot;fielddata&quot;: true</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&apos;</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"指定关键词查询，排序和函数统计\"><a href=\"#指定关键词查询，排序和函数统计\" class=\"headerlink\" title=\"指定关键词查询，排序和函数统计\"></a>指定关键词查询，排序和函数统计</h1><h2 id=\"指定关键词\"><a href=\"#指定关键词\" class=\"headerlink\" title=\"指定关键词\"></a>指定关键词</h2><p>from 为首个偏移量，size为返回数据的条数<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">http://10.10.11.139:9200/logstash-nginx-access-*/nginx-access/_search?pretty</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;from&quot;:0,size&quot;:1000,</span><br><span class=\"line\">    &quot;query&quot; : &#123;</span><br><span class=\"line\">        &quot;term&quot; : &#123; </span><br><span class=\"line\">        \t&quot;major&quot; : &quot;55&quot;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"添加排序\"><a href=\"#添加排序\" class=\"headerlink\" title=\"添加排序\"></a>添加排序</h2><p>(需要进行mapping设置，asc 为升序  desc为降序)<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;from&quot;:0,&quot;size&quot;:1000,</span><br><span class=\"line\">    &quot;sort&quot;:[</span><br><span class=\"line\">        &#123;&quot;offset&quot;:&quot;desc&quot;&#125;</span><br><span class=\"line\">    ],</span><br><span class=\"line\">    &quot;query&quot; : &#123;</span><br><span class=\"line\">        &quot;term&quot; : &#123; </span><br><span class=\"line\">            &quot;major&quot; : &quot;55&quot;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"mode-方法\"><a href=\"#mode-方法\" class=\"headerlink\" title=\"mode 方法\"></a>mode 方法</h2><p>mode方法包括 min／max／avg／sum／median</p>\n<p>假如现在要对price字段进行排序，但是price字段有多个值，这个时候就可以使用mode 方法了。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">   &quot;query&quot; : &#123;</span><br><span class=\"line\">      &quot;term&quot; : &#123; &quot;product&quot; : &quot;chocolate&quot; &#125;</span><br><span class=\"line\">   &#125;,</span><br><span class=\"line\">   &quot;sort&quot; : [</span><br><span class=\"line\">      &#123;&quot;price&quot; : &#123;&quot;order&quot; : &quot;asc&quot;, &quot;mode&quot; : &quot;avg&quot;&#125;&#125;</span><br><span class=\"line\">   ]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h1 id=\"IP范围和网段查询\"><a href=\"#IP范围和网段查询\" class=\"headerlink\" title=\"IP范围和网段查询\"></a>IP范围和网段查询</h1><h2 id=\"IP-range-搜索\"><a href=\"#IP-range-搜索\" class=\"headerlink\" title=\"IP range 搜索\"></a>IP range 搜索</h2><p>错误：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Fielddata is disabled on text fields by default. Set fielddata=true on [clientip] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory.</span><br></pre></td></tr></table></figure></p>\n<p>解决办法：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -k -u admin:admin -XPUT &apos;10.10.11.139:9200/logstash-nginx-access-*/_mapping/nginx-access?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;properties&quot;: &#123;</span><br><span class=\"line\">    &quot;clientip&quot;: &#123; </span><br><span class=\"line\">      &quot;type&quot;:     &quot;text&quot;,</span><br><span class=\"line\">      &quot;fielddata&quot;: true,</span><br><span class=\"line\">      &quot;norms&quot;: false</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&apos;</span><br></pre></td></tr></table></figure></p>\n<p>查看某个索引的mapping<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -k -u admin:admin -XGET http://10.10.11.139:9200/logstash-nginx-access-*/_mapping?pretty</span><br></pre></td></tr></table></figure></p>\n<p>(当IP为不可解析使就会出现错误)<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">http://10.10.11.139:9200/logstash-sshlogin-others-success-*/zhongcai/_search?pretty</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;size&quot;:100,</span><br><span class=\"line\">    &quot;aggs&quot; : &#123;</span><br><span class=\"line\">        &quot;ip_ranges&quot; : &#123;</span><br><span class=\"line\">            &quot;ip_range&quot; : &#123;</span><br><span class=\"line\">                &quot;field&quot; : &quot;clientip&quot;,</span><br><span class=\"line\">                &quot;ranges&quot; : [</span><br><span class=\"line\">                    &#123; &quot;to&quot; : &quot;40.77.167.73&quot; &#125;,</span><br><span class=\"line\">                    &#123; &quot;from&quot; : &quot;40.77.167.75&quot; &#125;</span><br><span class=\"line\">                ]</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"网段查询\"><a href=\"#网段查询\" class=\"headerlink\" title=\"网段查询\"></a>网段查询</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">http://10.10.11.139:9200/logstash-sshlogin-others-success-*/zhongcai/_search?pretty</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;aggs&quot; : &#123;</span><br><span class=\"line\">        &quot;ip_ranges&quot; : &#123;</span><br><span class=\"line\">            &quot;ip_range&quot; : &#123;</span><br><span class=\"line\">                &quot;field&quot; : &quot;ip&quot;,</span><br><span class=\"line\">                &quot;ranges&quot; : [</span><br><span class=\"line\">                    &#123; &quot;mask&quot; : &quot;172.21.202.0/24&quot; &#125;,</span><br><span class=\"line\">                    &#123; &quot;mask&quot; : &quot;172.21.202.0/24&quot; &#125;</span><br><span class=\"line\">                ]</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h1 id=\"关于索引的操作\"><a href=\"#关于索引的操作\" class=\"headerlink\" title=\"关于索引的操作\"></a>关于索引的操作</h1><h2 id=\"删除某个索引\"><a href=\"#删除某个索引\" class=\"headerlink\" title=\"删除某个索引\"></a>删除某个索引</h2><p>-k -u admin:admin 为用户名：密码<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XDELETE  -k -u admin:admin &apos;http://localhost:9200/my_index&apos;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"查看某个索引的Mapping\"><a href=\"#查看某个索引的Mapping\" class=\"headerlink\" title=\"查看某个索引的Mapping\"></a>查看某个索引的Mapping</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XGET &quot;http://127.0.0.1:9200/my_index/_mapping?pretty&quot;</span><br></pre></td></tr></table></figure>\n<h2 id=\"索引数据迁移\"><a href=\"#索引数据迁移\" class=\"headerlink\" title=\"索引数据迁移\"></a>索引数据迁移</h2><p>Es索引reindex(从ip_remote上迁移到本地)<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XPOST &apos;localhost:9200/_reindex?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;source&quot;: &#123;</span><br><span class=\"line\">    &quot;remote&quot;: &#123;</span><br><span class=\"line\">      &quot;host&quot;: &quot;http://ip_remote:9200&quot;,</span><br><span class=\"line\">      &quot;username&quot;: &quot;username&quot;,</span><br><span class=\"line\">      &quot;password&quot;: &quot;passwd&quot;</span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    &quot;index&quot;: &quot;old_index&quot;</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;dest&quot;: &#123;</span><br><span class=\"line\">    &quot;index&quot;: &quot;new_index&quot;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&apos;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"为某个索引添加字段\"><a href=\"#为某个索引添加字段\" class=\"headerlink\" title=\"为某个索引添加字段\"></a>为某个索引添加字段</h2><p>添加number字段：</p>\n<h3 id=\"唯一ID\"><a href=\"#唯一ID\" class=\"headerlink\" title=\"唯一ID\"></a>唯一ID</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -POST &apos;http://127.0.0.1:9200/my_idnex/my_index_type/id/_update?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">   &quot;doc&quot; : &#123;</span><br><span class=\"line\">      &quot;number&quot; : 1</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&apos;</span><br></pre></td></tr></table></figure>\n<h3 id=\"批量操作\"><a href=\"#批量操作\" class=\"headerlink\" title=\"批量操作\"></a>批量操作</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XPOST &apos;localhost:9200/logstash-sshlogin-others-success-2017-*/_update_by_query?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;script&quot;: &#123;</span><br><span class=\"line\">    &quot;inline&quot;: &quot;ctx._source.number=1&quot;,</span><br><span class=\"line\">    &quot;lang&quot;: &quot;painless&quot;</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;query&quot;: &#123;</span><br><span class=\"line\">    &quot;match_all&quot;: &#123;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&apos;</span><br></pre></td></tr></table></figure>\n<h1 id=\"根据指定条件进行聚合\"><a href=\"#根据指定条件进行聚合\" class=\"headerlink\" title=\"根据指定条件进行聚合\"></a>根据指定条件进行聚合</h1><p>每小时成功登录的次数进行聚合<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -POST &apos;http://127.0.0.1:9200/logstash-sshlogin-others-success-2017-*/zhongcai-sshlogin/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;: &#123;</span><br><span class=\"line\">    &quot;term&quot;: &#123;</span><br><span class=\"line\">      &quot;ssh_type&quot;: &quot;ssh_successful_login&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;aggs&quot;: &#123;</span><br><span class=\"line\">    &quot;sums&quot;: &#123;</span><br><span class=\"line\">      &quot;date_histogram&quot;: &#123;</span><br><span class=\"line\">        &quot;field&quot;: &quot;@timestamp&quot;,</span><br><span class=\"line\">        &quot;interval&quot;: &quot;hour&quot;,</span><br><span class=\"line\">        &quot;format&quot;: &quot;yyyy-MM-dd HH&quot;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&apos;</span><br></pre></td></tr></table></figure></p>"},{"title":"异常检测之指数平滑（利用elasticsearch来实现）","date":"2017-11-20T09:18:54.000Z","_content":"\n指数平滑法是一种特殊的加权平均法，加权的特点是对离预测值较近的历史数据给予较大的权数，对离预测期较远的历史数据给予较小的权数，权数由近到远按指数规律递减，所以，这种预测方法被称为指数平滑法。它可分为一次指数平滑法、二次指数平滑法及更高次指数平滑法。\n<!--More-->\n# 关于指数平滑的得相关资料：\n\n- ES API接口：\n> https://github.com/IBBD/IBBD.github.io/blob/master/elk/aggregations-pipeline.md\n<br><br>\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-pipeline-movavg-aggregation.html\n\n- 理论概念\n> http://blog.sina.com.cn/s/blog_4b9acb5201016nkd.html\n\n# ES移动平均聚合：Moving Average的四种模型\n## simple\n就是使用窗口内的值的和除于窗口值，通常窗口值越大，最后的结果越平滑: (a1 + a2 + ... + an) / n\n```\ncurl -XPOST 'localhost:9200/_search?pretty' -H 'Content-Type: application/json' -d'\n{\n    \"size\": 0,\n    \"aggs\": {\n        \"my_date_histo\":{\n            \"date_histogram\":{\n                \"field\":\"date\",\n                \"interval\":\"1M\"\n            },\n            \"aggs\":{\n                \"the_sum\":{\n                    \"sum\":{ \"field\": \"price\" }\n                },\n                \"the_movavg\":{\n                    \"moving_avg\":{\n                        \"buckets_path\": \"the_sum\",\n                        \"window\" : 30,\n                        \"model\" : \"simple\"\n                    }\n                }\n            }\n        }\n    }\n}\n'\n```\n\n## 线性模型：Linear\n对窗口内的值先做线性变换处理，再求平均：(a1 * 1 + a2 * 2 + ... + an * n) / (1 + 2 + ... + n)\n\n```\ncurl -XPOST 'localhost:9200/_search?pretty' -H 'Content-Type: application/json' -d'\n{\n    \"size\": 0,\n    \"aggs\": {\n        \"my_date_histo\":{\n            \"date_histogram\":{\n                \"field\":\"date\",\n                \"interval\":\"1M\"\n            },\n            \"aggs\":{\n                \"the_sum\":{\n                    \"sum\":{ \"field\": \"price\" }\n                },\n                \"the_movavg\": {\n                    \"moving_avg\":{\n                        \"buckets_path\": \"the_sum\",\n                        \"window\" : 30,\n                        \"model\" : \"linear\"\n                    }\n                }\n            }\n        }\n    }\n}\n'\n```\n\n## 指数平滑模型\n### 指数模型：EWMA (Exponentially Weighted)\n即： 一次指数平滑模型\n\nEWMA模型通常也成为单指数模型（single-exponential）, 和线性模型的思路类似，离当前点越远的点，重要性越低，具体化为数值的指数下降，对应的参数是alpha。 alpha值越小，下降越慢。（估计是用1 - alpha去计算的）默认的alpha=0.3\n\n计算模型：s2 = α * x2 + (1 - α) * s1\n\n其中α是平滑系数，si是之前i个数据的平滑值，α取值为[0,1]，越接近1，平滑后的值越接近当前时间的数据值，数据越不平滑，α越接近0，平滑后的值越接近前i个数据的平滑值，数据越平滑，α的值通常可以多尝试几次以达到最佳效果。 一次指数平滑算法进行预测的公式为：xi+h=si，其中i为当前最后的一个数据记录的坐标，亦即预测的时间序列为一条直线，不能反映时间序列的趋势和季节性。\n\n```\ncurl -XPOST 'localhost:9200/_search?pretty' -H 'Content-Type: application/json' -d'\n{\n    \"size\": 0,\n    \"aggs\": {\n        \"my_date_histo\":{\n            \"date_histogram\":{\n                \"field\":\"date\",\n                \"interval\":\"1M\"\n            },\n            \"aggs\":{\n                \"the_sum\":{\n                    \"sum\":{ \"field\": \"price\" }\n                },\n                \"the_movavg\": {\n                    \"moving_avg\":{\n                        \"buckets_path\": \"the_sum\",\n                        \"window\" : 30,\n                        \"model\" : \"ewma\",\n                        \"settings\" : {\n                            \"alpha\" : 0.5\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n'\n```\n\n### 二次指数平滑模型: Holt-Linear\n计算模型：\n\ns2 = α * x2 + (1 - α) * (s1 + t1)\n\nt2 = ß * (s2 - s1) + (1 - ß) * t1\n\n默认alpha = 0.3 and beta = 0.1\n\n二次指数平滑保留了趋势的信息，使得预测的时间序列可以包含之前数据的趋势。二次指数平滑的预测公式为 xi+h=si+hti 二次指数平滑的预测结果是一条斜的直线。\n\n```\ncurl -XPOST 'localhost:9200/_search?pretty' -H 'Content-Type: application/json' -d'\n{\n    \"size\": 0,\n    \"aggs\": {\n        \"my_date_histo\":{\n            \"date_histogram\":{\n                \"field\":\"date\",\n                \"interval\":\"1M\"\n            },\n            \"aggs\":{\n                \"the_sum\":{\n                    \"sum\":{ \"field\": \"price\" }\n                },\n                \"the_movavg\": {\n                    \"moving_avg\":{\n                        \"buckets_path\": \"the_sum\",\n                        \"window\" : 30,\n                        \"model\" : \"holt\",\n                        \"settings\" : {\n                            \"alpha\" : 0.5,\n                            \"beta\" : 0.5\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n'\n```\n### 三次指数平滑模型：Holt-Winters无季节模型\n三次指数平滑在二次指数平滑的基础上保留了季节性的信息，使得其可以预测带有季节性的时间序列。三次指数平滑添加了一个新的参数p来表示平滑后的趋势。\n\n1: Additive Holt-Winters：Holt-Winters加法模型\n\n下面是累加的三次指数平滑\n```\nsi=α(xi-pi-k)+(1-α)(si-1+ti-1)\nti=ß(si-si-1)+(1-ß)ti-1\npi=γ(xi-si)+(1-γ)pi-k\n```\n其中k为周期\n\n累加三次指数平滑的预测公式为： xi+h=si+hti+pi-k+(h mod k)\n\n```\ncurl -XPOST 'localhost:9200/_search?pretty' -H 'Content-Type: application/json' -d'\n{\n    \"size\": 0,\n    \"aggs\": {\n        \"my_date_histo\":{\n            \"date_histogram\":{\n                \"field\":\"date\",\n                \"interval\":\"1M\"\n            },\n            \"aggs\":{\n                \"the_sum\":{\n                    \"sum\":{ \"field\": \"price\" }\n                },\n                \"the_movavg\": {\n                    \"moving_avg\":{\n                        \"buckets_path\": \"the_sum\",\n                        \"window\" : 30,\n                        \"model\" : \"holt_winters\",\n                        \"settings\" : {\n                            \"type\" : \"add\",\n                            \"alpha\" : 0.5,\n                            \"beta\" : 0.5,\n                            \"gamma\" : 0.5,\n                            \"period\" : 7\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n'\n```\n\n2: Multiplicative Holt-Winters：Holt-Winters乘法模型\n\n下式为累乘的三次指数平滑：\n```\nsi=αxi/pi-k+(1-α)(si-1+ti-1)\nti=ß(si-si-1)+(1-ß)ti-1\npi=γxi/si+(1-γ)pi-k  其中k为周期\n```\n累乘三次指数平滑的预测公式为： xi+h=(si+hti)pi-k+(h mod k)\n\nα，ß，γ的值都位于[0,1]之间，可以多试验几次以达到最佳效果。\n\ns,t,p初始值的选取对于算法整体的影响不是特别大，通常的取值为s0=x0,t0=x1-x0,累加时p=0,累乘时p=1.\n\n```\ncurl -XPOST 'localhost:9200/_search?pretty' -H 'Content-Type: application/json' -d'\n{\n    \"size\": 0,\n    \"aggs\": {\n        \"my_date_histo\":{\n            \"date_histogram\":{\n                \"field\":\"date\",\n                \"interval\":\"1M\"\n            },\n            \"aggs\":{\n                \"the_sum\":{\n                    \"sum\":{ \"field\": \"price\" }\n                },\n                \"the_movavg\": {\n                    \"moving_avg\":{\n                        \"buckets_path\": \"the_sum\",\n                        \"window\" : 30,\n                        \"model\" : \"holt_winters\",\n                        \"settings\" : {\n                            \"type\" : \"mult\",\n                            \"alpha\" : 0.5,\n                            \"beta\" : 0.5,\n                            \"gamma\" : 0.5,\n                            \"period\" : 7,\n                            \"pad\" : true\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n'\n```\n## 预测模型：Prediction\n使用当前值减去前一个值，其实就是环比增长\n\n```\ncurl -XPOST 'localhost:9200/_search?pretty' -H 'Content-Type: application/json' -d'\n{\n    \"size\": 0,\n    \"aggs\": {\n        \"my_date_histo\":{\n            \"date_histogram\":{\n                \"field\":\"date\",\n                \"interval\":\"1M\"\n            },\n            \"aggs\":{\n                \"the_sum\":{\n                    \"sum\":{ \"field\": \"price\" }\n                },\n                \"the_movavg\": {\n                    \"moving_avg\":{\n                        \"buckets_path\": \"the_sum\",\n                        \"window\" : 30,\n                        \"model\" : \"simple\",\n                        \"predict\" : 10\n                    }\n                }\n            }\n        }\n    }\n}\n'\n```\n\n## 最小化：Minimization\n某些模型（EWMA，Holt-Linear，Holt-Winters）需要配置一个或多个参数。参数选择可能会非常棘手，有时不直观。此外，这些参数的小偏差有时会对输出移动平均线产生剧烈的影响。\n\n出于这个原因，三个“可调”模型可以在算法上最小化。最小化是一个参数调整的过程，直到模型生成的预测与输出数据紧密匹配为止。最小化并不是完全防护的，并且可能容易过度配合，但是它往往比手动调整有更好的结果。\n\newma和holt_linear默认情况下禁用最小化，而holt_winters默认启用最小化。 Holt-Winters最小化是最有用的，因为它有助于提高预测的准确性。 EWMA和Holt-Linear不是很好的预测指标，主要用于平滑数据，所以最小化对于这些模型来说不太有用。\n\n通过最小化参数启用/禁用最小化：\"minimize\" : true\n\n# 原始数据\n数据为SSH login数据其中 IP／user已处理\n```\n{\n    \"_index\": \"logstash-sshlogin-others-success-2017-10\",\n    \"_type\": \"sshlogin\",\n    \"_id\": \"AV-weLF8c2nHCDojUbat\",\n    \"_version\": 2,\n    \"_score\": 1,\n    \"_source\": {\n        \"srcip\": \"222.221.238.162\",\n        \"dstport\": \"\",\n        \"pid\": \"20604\",\n        \"program\": \"sshd\",\n        \"message\": \"dwasw-ibb01:Oct 19 23:38:02 176.231.228.130 sshd[20604]: Accepted publickey for nmuser from 222.221.238.162 port 49484 ssh2\",\n        \"type\": \"zhongcai-sshlogin\",\n        \"ssh_type\": \"ssh_successful_login\",\n        \"forwarded\": \"false\",\n        \"manufacturer\": \"others\",\n        \"IndexTime\": \"2017-10\",\n        \"path\": \"/home/logstash/log/logstash_data/audit10/sshlogin/11.txt\",\n        \"number\": 1,\n        \"hostname\": \"176.231.228.130\",\n        \"protocol\": \"ssh2\",\n        \"@timestamp\": \"2017-10-19T15:38:02.000Z\",\n        \"ssh_method\": \"publickey\",\n        \"_hostname\": \"dwasw-ibb01\",\n        \"@version\": \"1\",\n        \"host\": \"localhost\",\n        \"srcport\": \"49484\",\n        \"dstip\": \"\",\n        \"category\": \"sshlogin\",\n        \"user\": \"nmuser\"\n    }\n}\n```\n\n# 利用ES API接口去调用查询数据\n\n\"interval\": \"hour\": hour为单位，这里可以是分钟，小时，天，周，月\n\n\"format\": \"yyyy-MM-dd HH\": 聚合结果得日期格式\n\n```\n\"the_sum\": {\n    \"sum\": {\n        \"field\": \"number\"\n    }\n}\n```\nnumber为要聚合得字段\n\n```\ncurl -POST  'localhost:9200/logstash-sshlogin-others-success-2017-10/sshlogin/_search?pretty' -H 'Content-Type: application/json' -d'\n{\n  \"size\": 0,\n  \"query\": {\n    \"term\": {\n      \"ssh_type\": \"ssh_successful_login\"\n    }\n  },\n  \"aggs\": {\n    \"hour_sum\": {\n      \"date_histogram\": {\n        \"field\": \"@timestamp\",\n        \"interval\": \"hour\",\n        \"format\": \"yyyy-MM-dd HH\"\n      },\n      \"aggs\": {\n        \"the_sum\": {\n          \"sum\": {\n            \"field\": \"number\"\n          }\n        },\n        \"the_movavg\": {\n          \"moving_avg\": {\n            \"buckets_path\": \"the_sum\",\n            \"window\": 30,\n            \"model\": \"holt\",\n            \"settings\": {\n              \"alpha\": 0.5,\n              \"beta\": 0.7\n            }\n          }\n        }\n      }\n    }\n  }\n}'\n```\n得到的结果形式为：\n```\n{\n  \"took\" : 35,\n  \"timed_out\" : false,\n  \"_shards\" : {\n    \"total\" : 1,\n    \"successful\" : 1,\n    \"failed\" : 0\n  },\n  \"hits\" : {\n    \"total\" : 206821,\n    \"max_score\" : 0.0,\n    \"hits\" : [ ]\n  },\n  \"aggregations\" : {\n    \"hour_sum\" : {\n      \"buckets\" : [\n        {\n          \"key_as_string\" : \"2017-09-30 16\",\n          \"key\" : 1506787200000,\n          \"doc_count\" : 227,\n          \"the_sum\" : {\n            \"value\" : 227.0\n          }\n        },\n        {\n          \"key_as_string\" : \"2017-09-30 17\",\n          \"key\" : 1506790800000,\n          \"doc_count\" : 210,\n          \"the_sum\" : {\n            \"value\" : 210.0\n          },\n          \"the_movavg\" : {\n            \"value\" : 113.5\n          }\n        },\n        {\n          \"key_as_string\" : \"2017-09-30 18\",\n          \"key\" : 1506794400000,\n          \"doc_count\" : 365,\n          \"the_sum\" : {\n            \"value\" : 365.0\n          },\n          \"the_movavg\" : {\n            \"value\" : 210.0\n          }\n        },\n    ...\n    }\n}\n```\n\n# 对应得python代码（查询数据到画图）\n```\n# coding: utf-8\nfrom elasticsearch import Elasticsearch\nimport matplotlib.pyplot as plt\nfrom matplotlib.font_manager import FontManager, FontProperties\n\nclass Smooth:\n    def __init__(self,index):\n        self.es = Elasticsearch(['localhost:9200'])\n        self.index = index\n        \n    # 处理mac中文编码错误\n    def getChineseFont(self):\n        return FontProperties(fname='/System/Library/Fonts/PingFang.ttc')\n    \n    # 对index进行聚合\n    def agg(self):\n        # \"format\": \"yyyy-MM-dd HH:mm:SS\"\n        dsl = '''\n                {\n                  \"size\": 0,\n                  \"query\": {\n                    \"term\": {\n                      \"ssh_type\": \"ssh_successful_login\"\n                    }\n                  },\n                  \"aggs\": {\n                    \"hour_sum\": {\n                      \"date_histogram\": {\n                        \"field\": \"@timestamp\",\n                        \"interval\": \"day\",\n                        \"format\": \"dd\"\n                      },\n                      \"aggs\": {\n                        \"the_sum\": {\n                          \"sum\": {\n                            \"field\": \"number\"\n                          }\n                        },\n                        \"the_movavg\": {\n                          \"moving_avg\": {\n                            \"buckets_path\": \"the_sum\",\n                            \"window\": 30,\n                            \"model\": \"holt_winters\",\n                            \"settings\": {\n                              \"alpha\": 0.5,\n                              \"beta\": 0.7\n                            }\n                          }\n                        }\n                      }\n                    }\n                  }\n                }\n                '''\n        res = self.es.search(index=self.index, body=dsl)\n        return res['aggregations']['hour_sum']['buckets']\n    \n    # 画图\n    def draw(self):\n        x,y_true,y_pred = [],[],[]\n        for one in self.agg():\n            x.append(one['key_as_string'])\n            y_true.append(one['the_sum']['value'])\n            if 'the_movavg' in one.keys():       # 前几条数据没有 the_movavg 字段，故将真实值赋值给pred值\n                y_pred.append(one['the_movavg']['value'])\n            else:\n                y_pred.append(one['the_sum']['value'])\n        \n        x_line = range(len(x))\n        \n        plt.figure(figsize=(10,5))\n        plt.plot(x_line,y_true,color=\"r\")\n        plt.plot(x_line,y_pred,color=\"g\")\n        \n        plt.xlabel(u\"每单位时间\",fontproperties=self.getChineseFont()) #X轴标签 \n        plt.xticks(range(len(x)), x)\n        plt.ylabel(u\"聚合结果\",fontproperties=self.getChineseFont()) #Y轴标签  \n        plt.title(u\"10月份 SSH 主机登录成功聚合图\",fontproperties=self.getChineseFont()) # 标题\n        plt.legend([u\"True value\",u\"Predict value\"])\n        plt.show()\n\nsmooth = Smooth(\"logstash-sshlogin-others-success-2017-10\")\nprint smooth.draw()\n```\n结果图示为：\n![这里写图片描述](http://img.blog.csdn.net/20171120171404972?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n","source":"_posts/ELK/异常检测之指数平滑（利用elasticsearch来实现）.md","raw":"---\ntitle: 异常检测之指数平滑（利用elasticsearch来实现）\ndate: 2017-11-20 17:18:54\ntags: [ELK,异常检测,ES]\ncategories: 技术篇\n---\n\n指数平滑法是一种特殊的加权平均法，加权的特点是对离预测值较近的历史数据给予较大的权数，对离预测期较远的历史数据给予较小的权数，权数由近到远按指数规律递减，所以，这种预测方法被称为指数平滑法。它可分为一次指数平滑法、二次指数平滑法及更高次指数平滑法。\n<!--More-->\n# 关于指数平滑的得相关资料：\n\n- ES API接口：\n> https://github.com/IBBD/IBBD.github.io/blob/master/elk/aggregations-pipeline.md\n<br><br>\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-pipeline-movavg-aggregation.html\n\n- 理论概念\n> http://blog.sina.com.cn/s/blog_4b9acb5201016nkd.html\n\n# ES移动平均聚合：Moving Average的四种模型\n## simple\n就是使用窗口内的值的和除于窗口值，通常窗口值越大，最后的结果越平滑: (a1 + a2 + ... + an) / n\n```\ncurl -XPOST 'localhost:9200/_search?pretty' -H 'Content-Type: application/json' -d'\n{\n    \"size\": 0,\n    \"aggs\": {\n        \"my_date_histo\":{\n            \"date_histogram\":{\n                \"field\":\"date\",\n                \"interval\":\"1M\"\n            },\n            \"aggs\":{\n                \"the_sum\":{\n                    \"sum\":{ \"field\": \"price\" }\n                },\n                \"the_movavg\":{\n                    \"moving_avg\":{\n                        \"buckets_path\": \"the_sum\",\n                        \"window\" : 30,\n                        \"model\" : \"simple\"\n                    }\n                }\n            }\n        }\n    }\n}\n'\n```\n\n## 线性模型：Linear\n对窗口内的值先做线性变换处理，再求平均：(a1 * 1 + a2 * 2 + ... + an * n) / (1 + 2 + ... + n)\n\n```\ncurl -XPOST 'localhost:9200/_search?pretty' -H 'Content-Type: application/json' -d'\n{\n    \"size\": 0,\n    \"aggs\": {\n        \"my_date_histo\":{\n            \"date_histogram\":{\n                \"field\":\"date\",\n                \"interval\":\"1M\"\n            },\n            \"aggs\":{\n                \"the_sum\":{\n                    \"sum\":{ \"field\": \"price\" }\n                },\n                \"the_movavg\": {\n                    \"moving_avg\":{\n                        \"buckets_path\": \"the_sum\",\n                        \"window\" : 30,\n                        \"model\" : \"linear\"\n                    }\n                }\n            }\n        }\n    }\n}\n'\n```\n\n## 指数平滑模型\n### 指数模型：EWMA (Exponentially Weighted)\n即： 一次指数平滑模型\n\nEWMA模型通常也成为单指数模型（single-exponential）, 和线性模型的思路类似，离当前点越远的点，重要性越低，具体化为数值的指数下降，对应的参数是alpha。 alpha值越小，下降越慢。（估计是用1 - alpha去计算的）默认的alpha=0.3\n\n计算模型：s2 = α * x2 + (1 - α) * s1\n\n其中α是平滑系数，si是之前i个数据的平滑值，α取值为[0,1]，越接近1，平滑后的值越接近当前时间的数据值，数据越不平滑，α越接近0，平滑后的值越接近前i个数据的平滑值，数据越平滑，α的值通常可以多尝试几次以达到最佳效果。 一次指数平滑算法进行预测的公式为：xi+h=si，其中i为当前最后的一个数据记录的坐标，亦即预测的时间序列为一条直线，不能反映时间序列的趋势和季节性。\n\n```\ncurl -XPOST 'localhost:9200/_search?pretty' -H 'Content-Type: application/json' -d'\n{\n    \"size\": 0,\n    \"aggs\": {\n        \"my_date_histo\":{\n            \"date_histogram\":{\n                \"field\":\"date\",\n                \"interval\":\"1M\"\n            },\n            \"aggs\":{\n                \"the_sum\":{\n                    \"sum\":{ \"field\": \"price\" }\n                },\n                \"the_movavg\": {\n                    \"moving_avg\":{\n                        \"buckets_path\": \"the_sum\",\n                        \"window\" : 30,\n                        \"model\" : \"ewma\",\n                        \"settings\" : {\n                            \"alpha\" : 0.5\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n'\n```\n\n### 二次指数平滑模型: Holt-Linear\n计算模型：\n\ns2 = α * x2 + (1 - α) * (s1 + t1)\n\nt2 = ß * (s2 - s1) + (1 - ß) * t1\n\n默认alpha = 0.3 and beta = 0.1\n\n二次指数平滑保留了趋势的信息，使得预测的时间序列可以包含之前数据的趋势。二次指数平滑的预测公式为 xi+h=si+hti 二次指数平滑的预测结果是一条斜的直线。\n\n```\ncurl -XPOST 'localhost:9200/_search?pretty' -H 'Content-Type: application/json' -d'\n{\n    \"size\": 0,\n    \"aggs\": {\n        \"my_date_histo\":{\n            \"date_histogram\":{\n                \"field\":\"date\",\n                \"interval\":\"1M\"\n            },\n            \"aggs\":{\n                \"the_sum\":{\n                    \"sum\":{ \"field\": \"price\" }\n                },\n                \"the_movavg\": {\n                    \"moving_avg\":{\n                        \"buckets_path\": \"the_sum\",\n                        \"window\" : 30,\n                        \"model\" : \"holt\",\n                        \"settings\" : {\n                            \"alpha\" : 0.5,\n                            \"beta\" : 0.5\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n'\n```\n### 三次指数平滑模型：Holt-Winters无季节模型\n三次指数平滑在二次指数平滑的基础上保留了季节性的信息，使得其可以预测带有季节性的时间序列。三次指数平滑添加了一个新的参数p来表示平滑后的趋势。\n\n1: Additive Holt-Winters：Holt-Winters加法模型\n\n下面是累加的三次指数平滑\n```\nsi=α(xi-pi-k)+(1-α)(si-1+ti-1)\nti=ß(si-si-1)+(1-ß)ti-1\npi=γ(xi-si)+(1-γ)pi-k\n```\n其中k为周期\n\n累加三次指数平滑的预测公式为： xi+h=si+hti+pi-k+(h mod k)\n\n```\ncurl -XPOST 'localhost:9200/_search?pretty' -H 'Content-Type: application/json' -d'\n{\n    \"size\": 0,\n    \"aggs\": {\n        \"my_date_histo\":{\n            \"date_histogram\":{\n                \"field\":\"date\",\n                \"interval\":\"1M\"\n            },\n            \"aggs\":{\n                \"the_sum\":{\n                    \"sum\":{ \"field\": \"price\" }\n                },\n                \"the_movavg\": {\n                    \"moving_avg\":{\n                        \"buckets_path\": \"the_sum\",\n                        \"window\" : 30,\n                        \"model\" : \"holt_winters\",\n                        \"settings\" : {\n                            \"type\" : \"add\",\n                            \"alpha\" : 0.5,\n                            \"beta\" : 0.5,\n                            \"gamma\" : 0.5,\n                            \"period\" : 7\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n'\n```\n\n2: Multiplicative Holt-Winters：Holt-Winters乘法模型\n\n下式为累乘的三次指数平滑：\n```\nsi=αxi/pi-k+(1-α)(si-1+ti-1)\nti=ß(si-si-1)+(1-ß)ti-1\npi=γxi/si+(1-γ)pi-k  其中k为周期\n```\n累乘三次指数平滑的预测公式为： xi+h=(si+hti)pi-k+(h mod k)\n\nα，ß，γ的值都位于[0,1]之间，可以多试验几次以达到最佳效果。\n\ns,t,p初始值的选取对于算法整体的影响不是特别大，通常的取值为s0=x0,t0=x1-x0,累加时p=0,累乘时p=1.\n\n```\ncurl -XPOST 'localhost:9200/_search?pretty' -H 'Content-Type: application/json' -d'\n{\n    \"size\": 0,\n    \"aggs\": {\n        \"my_date_histo\":{\n            \"date_histogram\":{\n                \"field\":\"date\",\n                \"interval\":\"1M\"\n            },\n            \"aggs\":{\n                \"the_sum\":{\n                    \"sum\":{ \"field\": \"price\" }\n                },\n                \"the_movavg\": {\n                    \"moving_avg\":{\n                        \"buckets_path\": \"the_sum\",\n                        \"window\" : 30,\n                        \"model\" : \"holt_winters\",\n                        \"settings\" : {\n                            \"type\" : \"mult\",\n                            \"alpha\" : 0.5,\n                            \"beta\" : 0.5,\n                            \"gamma\" : 0.5,\n                            \"period\" : 7,\n                            \"pad\" : true\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n'\n```\n## 预测模型：Prediction\n使用当前值减去前一个值，其实就是环比增长\n\n```\ncurl -XPOST 'localhost:9200/_search?pretty' -H 'Content-Type: application/json' -d'\n{\n    \"size\": 0,\n    \"aggs\": {\n        \"my_date_histo\":{\n            \"date_histogram\":{\n                \"field\":\"date\",\n                \"interval\":\"1M\"\n            },\n            \"aggs\":{\n                \"the_sum\":{\n                    \"sum\":{ \"field\": \"price\" }\n                },\n                \"the_movavg\": {\n                    \"moving_avg\":{\n                        \"buckets_path\": \"the_sum\",\n                        \"window\" : 30,\n                        \"model\" : \"simple\",\n                        \"predict\" : 10\n                    }\n                }\n            }\n        }\n    }\n}\n'\n```\n\n## 最小化：Minimization\n某些模型（EWMA，Holt-Linear，Holt-Winters）需要配置一个或多个参数。参数选择可能会非常棘手，有时不直观。此外，这些参数的小偏差有时会对输出移动平均线产生剧烈的影响。\n\n出于这个原因，三个“可调”模型可以在算法上最小化。最小化是一个参数调整的过程，直到模型生成的预测与输出数据紧密匹配为止。最小化并不是完全防护的，并且可能容易过度配合，但是它往往比手动调整有更好的结果。\n\newma和holt_linear默认情况下禁用最小化，而holt_winters默认启用最小化。 Holt-Winters最小化是最有用的，因为它有助于提高预测的准确性。 EWMA和Holt-Linear不是很好的预测指标，主要用于平滑数据，所以最小化对于这些模型来说不太有用。\n\n通过最小化参数启用/禁用最小化：\"minimize\" : true\n\n# 原始数据\n数据为SSH login数据其中 IP／user已处理\n```\n{\n    \"_index\": \"logstash-sshlogin-others-success-2017-10\",\n    \"_type\": \"sshlogin\",\n    \"_id\": \"AV-weLF8c2nHCDojUbat\",\n    \"_version\": 2,\n    \"_score\": 1,\n    \"_source\": {\n        \"srcip\": \"222.221.238.162\",\n        \"dstport\": \"\",\n        \"pid\": \"20604\",\n        \"program\": \"sshd\",\n        \"message\": \"dwasw-ibb01:Oct 19 23:38:02 176.231.228.130 sshd[20604]: Accepted publickey for nmuser from 222.221.238.162 port 49484 ssh2\",\n        \"type\": \"zhongcai-sshlogin\",\n        \"ssh_type\": \"ssh_successful_login\",\n        \"forwarded\": \"false\",\n        \"manufacturer\": \"others\",\n        \"IndexTime\": \"2017-10\",\n        \"path\": \"/home/logstash/log/logstash_data/audit10/sshlogin/11.txt\",\n        \"number\": 1,\n        \"hostname\": \"176.231.228.130\",\n        \"protocol\": \"ssh2\",\n        \"@timestamp\": \"2017-10-19T15:38:02.000Z\",\n        \"ssh_method\": \"publickey\",\n        \"_hostname\": \"dwasw-ibb01\",\n        \"@version\": \"1\",\n        \"host\": \"localhost\",\n        \"srcport\": \"49484\",\n        \"dstip\": \"\",\n        \"category\": \"sshlogin\",\n        \"user\": \"nmuser\"\n    }\n}\n```\n\n# 利用ES API接口去调用查询数据\n\n\"interval\": \"hour\": hour为单位，这里可以是分钟，小时，天，周，月\n\n\"format\": \"yyyy-MM-dd HH\": 聚合结果得日期格式\n\n```\n\"the_sum\": {\n    \"sum\": {\n        \"field\": \"number\"\n    }\n}\n```\nnumber为要聚合得字段\n\n```\ncurl -POST  'localhost:9200/logstash-sshlogin-others-success-2017-10/sshlogin/_search?pretty' -H 'Content-Type: application/json' -d'\n{\n  \"size\": 0,\n  \"query\": {\n    \"term\": {\n      \"ssh_type\": \"ssh_successful_login\"\n    }\n  },\n  \"aggs\": {\n    \"hour_sum\": {\n      \"date_histogram\": {\n        \"field\": \"@timestamp\",\n        \"interval\": \"hour\",\n        \"format\": \"yyyy-MM-dd HH\"\n      },\n      \"aggs\": {\n        \"the_sum\": {\n          \"sum\": {\n            \"field\": \"number\"\n          }\n        },\n        \"the_movavg\": {\n          \"moving_avg\": {\n            \"buckets_path\": \"the_sum\",\n            \"window\": 30,\n            \"model\": \"holt\",\n            \"settings\": {\n              \"alpha\": 0.5,\n              \"beta\": 0.7\n            }\n          }\n        }\n      }\n    }\n  }\n}'\n```\n得到的结果形式为：\n```\n{\n  \"took\" : 35,\n  \"timed_out\" : false,\n  \"_shards\" : {\n    \"total\" : 1,\n    \"successful\" : 1,\n    \"failed\" : 0\n  },\n  \"hits\" : {\n    \"total\" : 206821,\n    \"max_score\" : 0.0,\n    \"hits\" : [ ]\n  },\n  \"aggregations\" : {\n    \"hour_sum\" : {\n      \"buckets\" : [\n        {\n          \"key_as_string\" : \"2017-09-30 16\",\n          \"key\" : 1506787200000,\n          \"doc_count\" : 227,\n          \"the_sum\" : {\n            \"value\" : 227.0\n          }\n        },\n        {\n          \"key_as_string\" : \"2017-09-30 17\",\n          \"key\" : 1506790800000,\n          \"doc_count\" : 210,\n          \"the_sum\" : {\n            \"value\" : 210.0\n          },\n          \"the_movavg\" : {\n            \"value\" : 113.5\n          }\n        },\n        {\n          \"key_as_string\" : \"2017-09-30 18\",\n          \"key\" : 1506794400000,\n          \"doc_count\" : 365,\n          \"the_sum\" : {\n            \"value\" : 365.0\n          },\n          \"the_movavg\" : {\n            \"value\" : 210.0\n          }\n        },\n    ...\n    }\n}\n```\n\n# 对应得python代码（查询数据到画图）\n```\n# coding: utf-8\nfrom elasticsearch import Elasticsearch\nimport matplotlib.pyplot as plt\nfrom matplotlib.font_manager import FontManager, FontProperties\n\nclass Smooth:\n    def __init__(self,index):\n        self.es = Elasticsearch(['localhost:9200'])\n        self.index = index\n        \n    # 处理mac中文编码错误\n    def getChineseFont(self):\n        return FontProperties(fname='/System/Library/Fonts/PingFang.ttc')\n    \n    # 对index进行聚合\n    def agg(self):\n        # \"format\": \"yyyy-MM-dd HH:mm:SS\"\n        dsl = '''\n                {\n                  \"size\": 0,\n                  \"query\": {\n                    \"term\": {\n                      \"ssh_type\": \"ssh_successful_login\"\n                    }\n                  },\n                  \"aggs\": {\n                    \"hour_sum\": {\n                      \"date_histogram\": {\n                        \"field\": \"@timestamp\",\n                        \"interval\": \"day\",\n                        \"format\": \"dd\"\n                      },\n                      \"aggs\": {\n                        \"the_sum\": {\n                          \"sum\": {\n                            \"field\": \"number\"\n                          }\n                        },\n                        \"the_movavg\": {\n                          \"moving_avg\": {\n                            \"buckets_path\": \"the_sum\",\n                            \"window\": 30,\n                            \"model\": \"holt_winters\",\n                            \"settings\": {\n                              \"alpha\": 0.5,\n                              \"beta\": 0.7\n                            }\n                          }\n                        }\n                      }\n                    }\n                  }\n                }\n                '''\n        res = self.es.search(index=self.index, body=dsl)\n        return res['aggregations']['hour_sum']['buckets']\n    \n    # 画图\n    def draw(self):\n        x,y_true,y_pred = [],[],[]\n        for one in self.agg():\n            x.append(one['key_as_string'])\n            y_true.append(one['the_sum']['value'])\n            if 'the_movavg' in one.keys():       # 前几条数据没有 the_movavg 字段，故将真实值赋值给pred值\n                y_pred.append(one['the_movavg']['value'])\n            else:\n                y_pred.append(one['the_sum']['value'])\n        \n        x_line = range(len(x))\n        \n        plt.figure(figsize=(10,5))\n        plt.plot(x_line,y_true,color=\"r\")\n        plt.plot(x_line,y_pred,color=\"g\")\n        \n        plt.xlabel(u\"每单位时间\",fontproperties=self.getChineseFont()) #X轴标签 \n        plt.xticks(range(len(x)), x)\n        plt.ylabel(u\"聚合结果\",fontproperties=self.getChineseFont()) #Y轴标签  \n        plt.title(u\"10月份 SSH 主机登录成功聚合图\",fontproperties=self.getChineseFont()) # 标题\n        plt.legend([u\"True value\",u\"Predict value\"])\n        plt.show()\n\nsmooth = Smooth(\"logstash-sshlogin-others-success-2017-10\")\nprint smooth.draw()\n```\n结果图示为：\n![这里写图片描述](http://img.blog.csdn.net/20171120171404972?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n","slug":"ELK/异常检测之指数平滑（利用elasticsearch来实现）","published":1,"updated":"2017-12-19T02:54:56.787Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjbd1r25p0006kxuw058ndgn1","content":"<p>指数平滑法是一种特殊的加权平均法，加权的特点是对离预测值较近的历史数据给予较大的权数，对离预测期较远的历史数据给予较小的权数，权数由近到远按指数规律递减，所以，这种预测方法被称为指数平滑法。它可分为一次指数平滑法、二次指数平滑法及更高次指数平滑法。<br><a id=\"more\"></a></p>\n<h1 id=\"关于指数平滑的得相关资料：\"><a href=\"#关于指数平滑的得相关资料：\" class=\"headerlink\" title=\"关于指数平滑的得相关资料：\"></a>关于指数平滑的得相关资料：</h1><ul>\n<li><p>ES API接口：</p>\n<blockquote>\n<p><a href=\"https://github.com/IBBD/IBBD.github.io/blob/master/elk/aggregations-pipeline.md\" target=\"_blank\" rel=\"external\">https://github.com/IBBD/IBBD.github.io/blob/master/elk/aggregations-pipeline.md</a><br><br><br><br><a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-pipeline-movavg-aggregation.html\" target=\"_blank\" rel=\"external\">https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-pipeline-movavg-aggregation.html</a></p>\n</blockquote>\n</li>\n<li><p>理论概念</p>\n<blockquote>\n<p><a href=\"http://blog.sina.com.cn/s/blog_4b9acb5201016nkd.html\" target=\"_blank\" rel=\"external\">http://blog.sina.com.cn/s/blog_4b9acb5201016nkd.html</a></p>\n</blockquote>\n</li>\n</ul>\n<h1 id=\"ES移动平均聚合：Moving-Average的四种模型\"><a href=\"#ES移动平均聚合：Moving-Average的四种模型\" class=\"headerlink\" title=\"ES移动平均聚合：Moving Average的四种模型\"></a>ES移动平均聚合：Moving Average的四种模型</h1><h2 id=\"simple\"><a href=\"#simple\" class=\"headerlink\" title=\"simple\"></a>simple</h2><p>就是使用窗口内的值的和除于窗口值，通常窗口值越大，最后的结果越平滑: (a1 + a2 + … + an) / n<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XPOST &apos;localhost:9200/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;size&quot;: 0,</span><br><span class=\"line\">    &quot;aggs&quot;: &#123;</span><br><span class=\"line\">        &quot;my_date_histo&quot;:&#123;</span><br><span class=\"line\">            &quot;date_histogram&quot;:&#123;</span><br><span class=\"line\">                &quot;field&quot;:&quot;date&quot;,</span><br><span class=\"line\">                &quot;interval&quot;:&quot;1M&quot;</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            &quot;aggs&quot;:&#123;</span><br><span class=\"line\">                &quot;the_sum&quot;:&#123;</span><br><span class=\"line\">                    &quot;sum&quot;:&#123; &quot;field&quot;: &quot;price&quot; &#125;</span><br><span class=\"line\">                &#125;,</span><br><span class=\"line\">                &quot;the_movavg&quot;:&#123;</span><br><span class=\"line\">                    &quot;moving_avg&quot;:&#123;</span><br><span class=\"line\">                        &quot;buckets_path&quot;: &quot;the_sum&quot;,</span><br><span class=\"line\">                        &quot;window&quot; : 30,</span><br><span class=\"line\">                        &quot;model&quot; : &quot;simple&quot;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&apos;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"线性模型：Linear\"><a href=\"#线性模型：Linear\" class=\"headerlink\" title=\"线性模型：Linear\"></a>线性模型：Linear</h2><p>对窗口内的值先做线性变换处理，再求平均：(a1 <em> 1 + a2 </em> 2 + … + an * n) / (1 + 2 + … + n)</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XPOST &apos;localhost:9200/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;size&quot;: 0,</span><br><span class=\"line\">    &quot;aggs&quot;: &#123;</span><br><span class=\"line\">        &quot;my_date_histo&quot;:&#123;</span><br><span class=\"line\">            &quot;date_histogram&quot;:&#123;</span><br><span class=\"line\">                &quot;field&quot;:&quot;date&quot;,</span><br><span class=\"line\">                &quot;interval&quot;:&quot;1M&quot;</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            &quot;aggs&quot;:&#123;</span><br><span class=\"line\">                &quot;the_sum&quot;:&#123;</span><br><span class=\"line\">                    &quot;sum&quot;:&#123; &quot;field&quot;: &quot;price&quot; &#125;</span><br><span class=\"line\">                &#125;,</span><br><span class=\"line\">                &quot;the_movavg&quot;: &#123;</span><br><span class=\"line\">                    &quot;moving_avg&quot;:&#123;</span><br><span class=\"line\">                        &quot;buckets_path&quot;: &quot;the_sum&quot;,</span><br><span class=\"line\">                        &quot;window&quot; : 30,</span><br><span class=\"line\">                        &quot;model&quot; : &quot;linear&quot;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&apos;</span><br></pre></td></tr></table></figure>\n<h2 id=\"指数平滑模型\"><a href=\"#指数平滑模型\" class=\"headerlink\" title=\"指数平滑模型\"></a>指数平滑模型</h2><h3 id=\"指数模型：EWMA-Exponentially-Weighted\"><a href=\"#指数模型：EWMA-Exponentially-Weighted\" class=\"headerlink\" title=\"指数模型：EWMA (Exponentially Weighted)\"></a>指数模型：EWMA (Exponentially Weighted)</h3><p>即： 一次指数平滑模型</p>\n<p>EWMA模型通常也成为单指数模型（single-exponential）, 和线性模型的思路类似，离当前点越远的点，重要性越低，具体化为数值的指数下降，对应的参数是alpha。 alpha值越小，下降越慢。（估计是用1 - alpha去计算的）默认的alpha=0.3</p>\n<p>计算模型：s2 = α <em> x2 + (1 - α) </em> s1</p>\n<p>其中α是平滑系数，si是之前i个数据的平滑值，α取值为[0,1]，越接近1，平滑后的值越接近当前时间的数据值，数据越不平滑，α越接近0，平滑后的值越接近前i个数据的平滑值，数据越平滑，α的值通常可以多尝试几次以达到最佳效果。 一次指数平滑算法进行预测的公式为：xi+h=si，其中i为当前最后的一个数据记录的坐标，亦即预测的时间序列为一条直线，不能反映时间序列的趋势和季节性。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XPOST &apos;localhost:9200/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;size&quot;: 0,</span><br><span class=\"line\">    &quot;aggs&quot;: &#123;</span><br><span class=\"line\">        &quot;my_date_histo&quot;:&#123;</span><br><span class=\"line\">            &quot;date_histogram&quot;:&#123;</span><br><span class=\"line\">                &quot;field&quot;:&quot;date&quot;,</span><br><span class=\"line\">                &quot;interval&quot;:&quot;1M&quot;</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            &quot;aggs&quot;:&#123;</span><br><span class=\"line\">                &quot;the_sum&quot;:&#123;</span><br><span class=\"line\">                    &quot;sum&quot;:&#123; &quot;field&quot;: &quot;price&quot; &#125;</span><br><span class=\"line\">                &#125;,</span><br><span class=\"line\">                &quot;the_movavg&quot;: &#123;</span><br><span class=\"line\">                    &quot;moving_avg&quot;:&#123;</span><br><span class=\"line\">                        &quot;buckets_path&quot;: &quot;the_sum&quot;,</span><br><span class=\"line\">                        &quot;window&quot; : 30,</span><br><span class=\"line\">                        &quot;model&quot; : &quot;ewma&quot;,</span><br><span class=\"line\">                        &quot;settings&quot; : &#123;</span><br><span class=\"line\">                            &quot;alpha&quot; : 0.5</span><br><span class=\"line\">                        &#125;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&apos;</span><br></pre></td></tr></table></figure>\n<h3 id=\"二次指数平滑模型-Holt-Linear\"><a href=\"#二次指数平滑模型-Holt-Linear\" class=\"headerlink\" title=\"二次指数平滑模型: Holt-Linear\"></a>二次指数平滑模型: Holt-Linear</h3><p>计算模型：</p>\n<p>s2 = α <em> x2 + (1 - α) </em> (s1 + t1)</p>\n<p>t2 = ß <em> (s2 - s1) + (1 - ß) </em> t1</p>\n<p>默认alpha = 0.3 and beta = 0.1</p>\n<p>二次指数平滑保留了趋势的信息，使得预测的时间序列可以包含之前数据的趋势。二次指数平滑的预测公式为 xi+h=si+hti 二次指数平滑的预测结果是一条斜的直线。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XPOST &apos;localhost:9200/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;size&quot;: 0,</span><br><span class=\"line\">    &quot;aggs&quot;: &#123;</span><br><span class=\"line\">        &quot;my_date_histo&quot;:&#123;</span><br><span class=\"line\">            &quot;date_histogram&quot;:&#123;</span><br><span class=\"line\">                &quot;field&quot;:&quot;date&quot;,</span><br><span class=\"line\">                &quot;interval&quot;:&quot;1M&quot;</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            &quot;aggs&quot;:&#123;</span><br><span class=\"line\">                &quot;the_sum&quot;:&#123;</span><br><span class=\"line\">                    &quot;sum&quot;:&#123; &quot;field&quot;: &quot;price&quot; &#125;</span><br><span class=\"line\">                &#125;,</span><br><span class=\"line\">                &quot;the_movavg&quot;: &#123;</span><br><span class=\"line\">                    &quot;moving_avg&quot;:&#123;</span><br><span class=\"line\">                        &quot;buckets_path&quot;: &quot;the_sum&quot;,</span><br><span class=\"line\">                        &quot;window&quot; : 30,</span><br><span class=\"line\">                        &quot;model&quot; : &quot;holt&quot;,</span><br><span class=\"line\">                        &quot;settings&quot; : &#123;</span><br><span class=\"line\">                            &quot;alpha&quot; : 0.5,</span><br><span class=\"line\">                            &quot;beta&quot; : 0.5</span><br><span class=\"line\">                        &#125;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&apos;</span><br></pre></td></tr></table></figure>\n<h3 id=\"三次指数平滑模型：Holt-Winters无季节模型\"><a href=\"#三次指数平滑模型：Holt-Winters无季节模型\" class=\"headerlink\" title=\"三次指数平滑模型：Holt-Winters无季节模型\"></a>三次指数平滑模型：Holt-Winters无季节模型</h3><p>三次指数平滑在二次指数平滑的基础上保留了季节性的信息，使得其可以预测带有季节性的时间序列。三次指数平滑添加了一个新的参数p来表示平滑后的趋势。</p>\n<p>1: Additive Holt-Winters：Holt-Winters加法模型</p>\n<p>下面是累加的三次指数平滑<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">si=α(xi-pi-k)+(1-α)(si-1+ti-1)</span><br><span class=\"line\">ti=ß(si-si-1)+(1-ß)ti-1</span><br><span class=\"line\">pi=γ(xi-si)+(1-γ)pi-k</span><br></pre></td></tr></table></figure></p>\n<p>其中k为周期</p>\n<p>累加三次指数平滑的预测公式为： xi+h=si+hti+pi-k+(h mod k)</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XPOST &apos;localhost:9200/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;size&quot;: 0,</span><br><span class=\"line\">    &quot;aggs&quot;: &#123;</span><br><span class=\"line\">        &quot;my_date_histo&quot;:&#123;</span><br><span class=\"line\">            &quot;date_histogram&quot;:&#123;</span><br><span class=\"line\">                &quot;field&quot;:&quot;date&quot;,</span><br><span class=\"line\">                &quot;interval&quot;:&quot;1M&quot;</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            &quot;aggs&quot;:&#123;</span><br><span class=\"line\">                &quot;the_sum&quot;:&#123;</span><br><span class=\"line\">                    &quot;sum&quot;:&#123; &quot;field&quot;: &quot;price&quot; &#125;</span><br><span class=\"line\">                &#125;,</span><br><span class=\"line\">                &quot;the_movavg&quot;: &#123;</span><br><span class=\"line\">                    &quot;moving_avg&quot;:&#123;</span><br><span class=\"line\">                        &quot;buckets_path&quot;: &quot;the_sum&quot;,</span><br><span class=\"line\">                        &quot;window&quot; : 30,</span><br><span class=\"line\">                        &quot;model&quot; : &quot;holt_winters&quot;,</span><br><span class=\"line\">                        &quot;settings&quot; : &#123;</span><br><span class=\"line\">                            &quot;type&quot; : &quot;add&quot;,</span><br><span class=\"line\">                            &quot;alpha&quot; : 0.5,</span><br><span class=\"line\">                            &quot;beta&quot; : 0.5,</span><br><span class=\"line\">                            &quot;gamma&quot; : 0.5,</span><br><span class=\"line\">                            &quot;period&quot; : 7</span><br><span class=\"line\">                        &#125;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&apos;</span><br></pre></td></tr></table></figure>\n<p>2: Multiplicative Holt-Winters：Holt-Winters乘法模型</p>\n<p>下式为累乘的三次指数平滑：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">si=αxi/pi-k+(1-α)(si-1+ti-1)</span><br><span class=\"line\">ti=ß(si-si-1)+(1-ß)ti-1</span><br><span class=\"line\">pi=γxi/si+(1-γ)pi-k  其中k为周期</span><br></pre></td></tr></table></figure></p>\n<p>累乘三次指数平滑的预测公式为： xi+h=(si+hti)pi-k+(h mod k)</p>\n<p>α，ß，γ的值都位于[0,1]之间，可以多试验几次以达到最佳效果。</p>\n<p>s,t,p初始值的选取对于算法整体的影响不是特别大，通常的取值为s0=x0,t0=x1-x0,累加时p=0,累乘时p=1.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XPOST &apos;localhost:9200/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;size&quot;: 0,</span><br><span class=\"line\">    &quot;aggs&quot;: &#123;</span><br><span class=\"line\">        &quot;my_date_histo&quot;:&#123;</span><br><span class=\"line\">            &quot;date_histogram&quot;:&#123;</span><br><span class=\"line\">                &quot;field&quot;:&quot;date&quot;,</span><br><span class=\"line\">                &quot;interval&quot;:&quot;1M&quot;</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            &quot;aggs&quot;:&#123;</span><br><span class=\"line\">                &quot;the_sum&quot;:&#123;</span><br><span class=\"line\">                    &quot;sum&quot;:&#123; &quot;field&quot;: &quot;price&quot; &#125;</span><br><span class=\"line\">                &#125;,</span><br><span class=\"line\">                &quot;the_movavg&quot;: &#123;</span><br><span class=\"line\">                    &quot;moving_avg&quot;:&#123;</span><br><span class=\"line\">                        &quot;buckets_path&quot;: &quot;the_sum&quot;,</span><br><span class=\"line\">                        &quot;window&quot; : 30,</span><br><span class=\"line\">                        &quot;model&quot; : &quot;holt_winters&quot;,</span><br><span class=\"line\">                        &quot;settings&quot; : &#123;</span><br><span class=\"line\">                            &quot;type&quot; : &quot;mult&quot;,</span><br><span class=\"line\">                            &quot;alpha&quot; : 0.5,</span><br><span class=\"line\">                            &quot;beta&quot; : 0.5,</span><br><span class=\"line\">                            &quot;gamma&quot; : 0.5,</span><br><span class=\"line\">                            &quot;period&quot; : 7,</span><br><span class=\"line\">                            &quot;pad&quot; : true</span><br><span class=\"line\">                        &#125;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&apos;</span><br></pre></td></tr></table></figure>\n<h2 id=\"预测模型：Prediction\"><a href=\"#预测模型：Prediction\" class=\"headerlink\" title=\"预测模型：Prediction\"></a>预测模型：Prediction</h2><p>使用当前值减去前一个值，其实就是环比增长</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XPOST &apos;localhost:9200/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;size&quot;: 0,</span><br><span class=\"line\">    &quot;aggs&quot;: &#123;</span><br><span class=\"line\">        &quot;my_date_histo&quot;:&#123;</span><br><span class=\"line\">            &quot;date_histogram&quot;:&#123;</span><br><span class=\"line\">                &quot;field&quot;:&quot;date&quot;,</span><br><span class=\"line\">                &quot;interval&quot;:&quot;1M&quot;</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            &quot;aggs&quot;:&#123;</span><br><span class=\"line\">                &quot;the_sum&quot;:&#123;</span><br><span class=\"line\">                    &quot;sum&quot;:&#123; &quot;field&quot;: &quot;price&quot; &#125;</span><br><span class=\"line\">                &#125;,</span><br><span class=\"line\">                &quot;the_movavg&quot;: &#123;</span><br><span class=\"line\">                    &quot;moving_avg&quot;:&#123;</span><br><span class=\"line\">                        &quot;buckets_path&quot;: &quot;the_sum&quot;,</span><br><span class=\"line\">                        &quot;window&quot; : 30,</span><br><span class=\"line\">                        &quot;model&quot; : &quot;simple&quot;,</span><br><span class=\"line\">                        &quot;predict&quot; : 10</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&apos;</span><br></pre></td></tr></table></figure>\n<h2 id=\"最小化：Minimization\"><a href=\"#最小化：Minimization\" class=\"headerlink\" title=\"最小化：Minimization\"></a>最小化：Minimization</h2><p>某些模型（EWMA，Holt-Linear，Holt-Winters）需要配置一个或多个参数。参数选择可能会非常棘手，有时不直观。此外，这些参数的小偏差有时会对输出移动平均线产生剧烈的影响。</p>\n<p>出于这个原因，三个“可调”模型可以在算法上最小化。最小化是一个参数调整的过程，直到模型生成的预测与输出数据紧密匹配为止。最小化并不是完全防护的，并且可能容易过度配合，但是它往往比手动调整有更好的结果。</p>\n<p>ewma和holt_linear默认情况下禁用最小化，而holt_winters默认启用最小化。 Holt-Winters最小化是最有用的，因为它有助于提高预测的准确性。 EWMA和Holt-Linear不是很好的预测指标，主要用于平滑数据，所以最小化对于这些模型来说不太有用。</p>\n<p>通过最小化参数启用/禁用最小化：”minimize” : true</p>\n<h1 id=\"原始数据\"><a href=\"#原始数据\" class=\"headerlink\" title=\"原始数据\"></a>原始数据</h1><p>数据为SSH login数据其中 IP／user已处理<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;_index&quot;: &quot;logstash-sshlogin-others-success-2017-10&quot;,</span><br><span class=\"line\">    &quot;_type&quot;: &quot;sshlogin&quot;,</span><br><span class=\"line\">    &quot;_id&quot;: &quot;AV-weLF8c2nHCDojUbat&quot;,</span><br><span class=\"line\">    &quot;_version&quot;: 2,</span><br><span class=\"line\">    &quot;_score&quot;: 1,</span><br><span class=\"line\">    &quot;_source&quot;: &#123;</span><br><span class=\"line\">        &quot;srcip&quot;: &quot;222.221.238.162&quot;,</span><br><span class=\"line\">        &quot;dstport&quot;: &quot;&quot;,</span><br><span class=\"line\">        &quot;pid&quot;: &quot;20604&quot;,</span><br><span class=\"line\">        &quot;program&quot;: &quot;sshd&quot;,</span><br><span class=\"line\">        &quot;message&quot;: &quot;dwasw-ibb01:Oct 19 23:38:02 176.231.228.130 sshd[20604]: Accepted publickey for nmuser from 222.221.238.162 port 49484 ssh2&quot;,</span><br><span class=\"line\">        &quot;type&quot;: &quot;zhongcai-sshlogin&quot;,</span><br><span class=\"line\">        &quot;ssh_type&quot;: &quot;ssh_successful_login&quot;,</span><br><span class=\"line\">        &quot;forwarded&quot;: &quot;false&quot;,</span><br><span class=\"line\">        &quot;manufacturer&quot;: &quot;others&quot;,</span><br><span class=\"line\">        &quot;IndexTime&quot;: &quot;2017-10&quot;,</span><br><span class=\"line\">        &quot;path&quot;: &quot;/home/logstash/log/logstash_data/audit10/sshlogin/11.txt&quot;,</span><br><span class=\"line\">        &quot;number&quot;: 1,</span><br><span class=\"line\">        &quot;hostname&quot;: &quot;176.231.228.130&quot;,</span><br><span class=\"line\">        &quot;protocol&quot;: &quot;ssh2&quot;,</span><br><span class=\"line\">        &quot;@timestamp&quot;: &quot;2017-10-19T15:38:02.000Z&quot;,</span><br><span class=\"line\">        &quot;ssh_method&quot;: &quot;publickey&quot;,</span><br><span class=\"line\">        &quot;_hostname&quot;: &quot;dwasw-ibb01&quot;,</span><br><span class=\"line\">        &quot;@version&quot;: &quot;1&quot;,</span><br><span class=\"line\">        &quot;host&quot;: &quot;localhost&quot;,</span><br><span class=\"line\">        &quot;srcport&quot;: &quot;49484&quot;,</span><br><span class=\"line\">        &quot;dstip&quot;: &quot;&quot;,</span><br><span class=\"line\">        &quot;category&quot;: &quot;sshlogin&quot;,</span><br><span class=\"line\">        &quot;user&quot;: &quot;nmuser&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"利用ES-API接口去调用查询数据\"><a href=\"#利用ES-API接口去调用查询数据\" class=\"headerlink\" title=\"利用ES API接口去调用查询数据\"></a>利用ES API接口去调用查询数据</h1><p>“interval”: “hour”: hour为单位，这里可以是分钟，小时，天，周，月</p>\n<p>“format”: “yyyy-MM-dd HH”: 聚合结果得日期格式</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&quot;the_sum&quot;: &#123;</span><br><span class=\"line\">    &quot;sum&quot;: &#123;</span><br><span class=\"line\">        &quot;field&quot;: &quot;number&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>number为要聚合得字段</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -POST  &apos;localhost:9200/logstash-sshlogin-others-success-2017-10/sshlogin/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;size&quot;: 0,</span><br><span class=\"line\">  &quot;query&quot;: &#123;</span><br><span class=\"line\">    &quot;term&quot;: &#123;</span><br><span class=\"line\">      &quot;ssh_type&quot;: &quot;ssh_successful_login&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;aggs&quot;: &#123;</span><br><span class=\"line\">    &quot;hour_sum&quot;: &#123;</span><br><span class=\"line\">      &quot;date_histogram&quot;: &#123;</span><br><span class=\"line\">        &quot;field&quot;: &quot;@timestamp&quot;,</span><br><span class=\"line\">        &quot;interval&quot;: &quot;hour&quot;,</span><br><span class=\"line\">        &quot;format&quot;: &quot;yyyy-MM-dd HH&quot;</span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      &quot;aggs&quot;: &#123;</span><br><span class=\"line\">        &quot;the_sum&quot;: &#123;</span><br><span class=\"line\">          &quot;sum&quot;: &#123;</span><br><span class=\"line\">            &quot;field&quot;: &quot;number&quot;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &quot;the_movavg&quot;: &#123;</span><br><span class=\"line\">          &quot;moving_avg&quot;: &#123;</span><br><span class=\"line\">            &quot;buckets_path&quot;: &quot;the_sum&quot;,</span><br><span class=\"line\">            &quot;window&quot;: 30,</span><br><span class=\"line\">            &quot;model&quot;: &quot;holt&quot;,</span><br><span class=\"line\">            &quot;settings&quot;: &#123;</span><br><span class=\"line\">              &quot;alpha&quot;: 0.5,</span><br><span class=\"line\">              &quot;beta&quot;: 0.7</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;&apos;</span><br></pre></td></tr></table></figure>\n<p>得到的结果形式为：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;took&quot; : 35,</span><br><span class=\"line\">  &quot;timed_out&quot; : false,</span><br><span class=\"line\">  &quot;_shards&quot; : &#123;</span><br><span class=\"line\">    &quot;total&quot; : 1,</span><br><span class=\"line\">    &quot;successful&quot; : 1,</span><br><span class=\"line\">    &quot;failed&quot; : 0</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;hits&quot; : &#123;</span><br><span class=\"line\">    &quot;total&quot; : 206821,</span><br><span class=\"line\">    &quot;max_score&quot; : 0.0,</span><br><span class=\"line\">    &quot;hits&quot; : [ ]</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;aggregations&quot; : &#123;</span><br><span class=\"line\">    &quot;hour_sum&quot; : &#123;</span><br><span class=\"line\">      &quot;buckets&quot; : [</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">          &quot;key_as_string&quot; : &quot;2017-09-30 16&quot;,</span><br><span class=\"line\">          &quot;key&quot; : 1506787200000,</span><br><span class=\"line\">          &quot;doc_count&quot; : 227,</span><br><span class=\"line\">          &quot;the_sum&quot; : &#123;</span><br><span class=\"line\">            &quot;value&quot; : 227.0</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">          &quot;key_as_string&quot; : &quot;2017-09-30 17&quot;,</span><br><span class=\"line\">          &quot;key&quot; : 1506790800000,</span><br><span class=\"line\">          &quot;doc_count&quot; : 210,</span><br><span class=\"line\">          &quot;the_sum&quot; : &#123;</span><br><span class=\"line\">            &quot;value&quot; : 210.0</span><br><span class=\"line\">          &#125;,</span><br><span class=\"line\">          &quot;the_movavg&quot; : &#123;</span><br><span class=\"line\">            &quot;value&quot; : 113.5</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">          &quot;key_as_string&quot; : &quot;2017-09-30 18&quot;,</span><br><span class=\"line\">          &quot;key&quot; : 1506794400000,</span><br><span class=\"line\">          &quot;doc_count&quot; : 365,</span><br><span class=\"line\">          &quot;the_sum&quot; : &#123;</span><br><span class=\"line\">            &quot;value&quot; : 365.0</span><br><span class=\"line\">          &#125;,</span><br><span class=\"line\">          &quot;the_movavg&quot; : &#123;</span><br><span class=\"line\">            &quot;value&quot; : 210.0</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"对应得python代码（查询数据到画图）\"><a href=\"#对应得python代码（查询数据到画图）\" class=\"headerlink\" title=\"对应得python代码（查询数据到画图）\"></a>对应得python代码（查询数据到画图）</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># coding: utf-8</span><br><span class=\"line\">from elasticsearch import Elasticsearch</span><br><span class=\"line\">import matplotlib.pyplot as plt</span><br><span class=\"line\">from matplotlib.font_manager import FontManager, FontProperties</span><br><span class=\"line\"></span><br><span class=\"line\">class Smooth:</span><br><span class=\"line\">    def __init__(self,index):</span><br><span class=\"line\">        self.es = Elasticsearch([&apos;localhost:9200&apos;])</span><br><span class=\"line\">        self.index = index</span><br><span class=\"line\">        </span><br><span class=\"line\">    # 处理mac中文编码错误</span><br><span class=\"line\">    def getChineseFont(self):</span><br><span class=\"line\">        return FontProperties(fname=&apos;/System/Library/Fonts/PingFang.ttc&apos;)</span><br><span class=\"line\">    </span><br><span class=\"line\">    # 对index进行聚合</span><br><span class=\"line\">    def agg(self):</span><br><span class=\"line\">        # &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:SS&quot;</span><br><span class=\"line\">        dsl = &apos;&apos;&apos;</span><br><span class=\"line\">                &#123;</span><br><span class=\"line\">                  &quot;size&quot;: 0,</span><br><span class=\"line\">                  &quot;query&quot;: &#123;</span><br><span class=\"line\">                    &quot;term&quot;: &#123;</span><br><span class=\"line\">                      &quot;ssh_type&quot;: &quot;ssh_successful_login&quot;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                  &#125;,</span><br><span class=\"line\">                  &quot;aggs&quot;: &#123;</span><br><span class=\"line\">                    &quot;hour_sum&quot;: &#123;</span><br><span class=\"line\">                      &quot;date_histogram&quot;: &#123;</span><br><span class=\"line\">                        &quot;field&quot;: &quot;@timestamp&quot;,</span><br><span class=\"line\">                        &quot;interval&quot;: &quot;day&quot;,</span><br><span class=\"line\">                        &quot;format&quot;: &quot;dd&quot;</span><br><span class=\"line\">                      &#125;,</span><br><span class=\"line\">                      &quot;aggs&quot;: &#123;</span><br><span class=\"line\">                        &quot;the_sum&quot;: &#123;</span><br><span class=\"line\">                          &quot;sum&quot;: &#123;</span><br><span class=\"line\">                            &quot;field&quot;: &quot;number&quot;</span><br><span class=\"line\">                          &#125;</span><br><span class=\"line\">                        &#125;,</span><br><span class=\"line\">                        &quot;the_movavg&quot;: &#123;</span><br><span class=\"line\">                          &quot;moving_avg&quot;: &#123;</span><br><span class=\"line\">                            &quot;buckets_path&quot;: &quot;the_sum&quot;,</span><br><span class=\"line\">                            &quot;window&quot;: 30,</span><br><span class=\"line\">                            &quot;model&quot;: &quot;holt_winters&quot;,</span><br><span class=\"line\">                            &quot;settings&quot;: &#123;</span><br><span class=\"line\">                              &quot;alpha&quot;: 0.5,</span><br><span class=\"line\">                              &quot;beta&quot;: 0.7</span><br><span class=\"line\">                            &#125;</span><br><span class=\"line\">                          &#125;</span><br><span class=\"line\">                        &#125;</span><br><span class=\"line\">                      &#125;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                  &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                &apos;&apos;&apos;</span><br><span class=\"line\">        res = self.es.search(index=self.index, body=dsl)</span><br><span class=\"line\">        return res[&apos;aggregations&apos;][&apos;hour_sum&apos;][&apos;buckets&apos;]</span><br><span class=\"line\">    </span><br><span class=\"line\">    # 画图</span><br><span class=\"line\">    def draw(self):</span><br><span class=\"line\">        x,y_true,y_pred = [],[],[]</span><br><span class=\"line\">        for one in self.agg():</span><br><span class=\"line\">            x.append(one[&apos;key_as_string&apos;])</span><br><span class=\"line\">            y_true.append(one[&apos;the_sum&apos;][&apos;value&apos;])</span><br><span class=\"line\">            if &apos;the_movavg&apos; in one.keys():       # 前几条数据没有 the_movavg 字段，故将真实值赋值给pred值</span><br><span class=\"line\">                y_pred.append(one[&apos;the_movavg&apos;][&apos;value&apos;])</span><br><span class=\"line\">            else:</span><br><span class=\"line\">                y_pred.append(one[&apos;the_sum&apos;][&apos;value&apos;])</span><br><span class=\"line\">        </span><br><span class=\"line\">        x_line = range(len(x))</span><br><span class=\"line\">        </span><br><span class=\"line\">        plt.figure(figsize=(10,5))</span><br><span class=\"line\">        plt.plot(x_line,y_true,color=&quot;r&quot;)</span><br><span class=\"line\">        plt.plot(x_line,y_pred,color=&quot;g&quot;)</span><br><span class=\"line\">        </span><br><span class=\"line\">        plt.xlabel(u&quot;每单位时间&quot;,fontproperties=self.getChineseFont()) #X轴标签 </span><br><span class=\"line\">        plt.xticks(range(len(x)), x)</span><br><span class=\"line\">        plt.ylabel(u&quot;聚合结果&quot;,fontproperties=self.getChineseFont()) #Y轴标签  </span><br><span class=\"line\">        plt.title(u&quot;10月份 SSH 主机登录成功聚合图&quot;,fontproperties=self.getChineseFont()) # 标题</span><br><span class=\"line\">        plt.legend([u&quot;True value&quot;,u&quot;Predict value&quot;])</span><br><span class=\"line\">        plt.show()</span><br><span class=\"line\"></span><br><span class=\"line\">smooth = Smooth(&quot;logstash-sshlogin-others-success-2017-10&quot;)</span><br><span class=\"line\">print smooth.draw()</span><br></pre></td></tr></table></figure>\n<p>结果图示为：<br><img src=\"http://img.blog.csdn.net/20171120171404972?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"这里写图片描述\"></p>\n","site":{"data":{}},"excerpt":"<p>指数平滑法是一种特殊的加权平均法，加权的特点是对离预测值较近的历史数据给予较大的权数，对离预测期较远的历史数据给予较小的权数，权数由近到远按指数规律递减，所以，这种预测方法被称为指数平滑法。它可分为一次指数平滑法、二次指数平滑法及更高次指数平滑法。<br>","more":"</p>\n<h1 id=\"关于指数平滑的得相关资料：\"><a href=\"#关于指数平滑的得相关资料：\" class=\"headerlink\" title=\"关于指数平滑的得相关资料：\"></a>关于指数平滑的得相关资料：</h1><ul>\n<li><p>ES API接口：</p>\n<blockquote>\n<p><a href=\"https://github.com/IBBD/IBBD.github.io/blob/master/elk/aggregations-pipeline.md\" target=\"_blank\" rel=\"external\">https://github.com/IBBD/IBBD.github.io/blob/master/elk/aggregations-pipeline.md</a><br><br><br><br><a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-pipeline-movavg-aggregation.html\" target=\"_blank\" rel=\"external\">https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-pipeline-movavg-aggregation.html</a></p>\n</blockquote>\n</li>\n<li><p>理论概念</p>\n<blockquote>\n<p><a href=\"http://blog.sina.com.cn/s/blog_4b9acb5201016nkd.html\" target=\"_blank\" rel=\"external\">http://blog.sina.com.cn/s/blog_4b9acb5201016nkd.html</a></p>\n</blockquote>\n</li>\n</ul>\n<h1 id=\"ES移动平均聚合：Moving-Average的四种模型\"><a href=\"#ES移动平均聚合：Moving-Average的四种模型\" class=\"headerlink\" title=\"ES移动平均聚合：Moving Average的四种模型\"></a>ES移动平均聚合：Moving Average的四种模型</h1><h2 id=\"simple\"><a href=\"#simple\" class=\"headerlink\" title=\"simple\"></a>simple</h2><p>就是使用窗口内的值的和除于窗口值，通常窗口值越大，最后的结果越平滑: (a1 + a2 + … + an) / n<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XPOST &apos;localhost:9200/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;size&quot;: 0,</span><br><span class=\"line\">    &quot;aggs&quot;: &#123;</span><br><span class=\"line\">        &quot;my_date_histo&quot;:&#123;</span><br><span class=\"line\">            &quot;date_histogram&quot;:&#123;</span><br><span class=\"line\">                &quot;field&quot;:&quot;date&quot;,</span><br><span class=\"line\">                &quot;interval&quot;:&quot;1M&quot;</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            &quot;aggs&quot;:&#123;</span><br><span class=\"line\">                &quot;the_sum&quot;:&#123;</span><br><span class=\"line\">                    &quot;sum&quot;:&#123; &quot;field&quot;: &quot;price&quot; &#125;</span><br><span class=\"line\">                &#125;,</span><br><span class=\"line\">                &quot;the_movavg&quot;:&#123;</span><br><span class=\"line\">                    &quot;moving_avg&quot;:&#123;</span><br><span class=\"line\">                        &quot;buckets_path&quot;: &quot;the_sum&quot;,</span><br><span class=\"line\">                        &quot;window&quot; : 30,</span><br><span class=\"line\">                        &quot;model&quot; : &quot;simple&quot;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&apos;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"线性模型：Linear\"><a href=\"#线性模型：Linear\" class=\"headerlink\" title=\"线性模型：Linear\"></a>线性模型：Linear</h2><p>对窗口内的值先做线性变换处理，再求平均：(a1 <em> 1 + a2 </em> 2 + … + an * n) / (1 + 2 + … + n)</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XPOST &apos;localhost:9200/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;size&quot;: 0,</span><br><span class=\"line\">    &quot;aggs&quot;: &#123;</span><br><span class=\"line\">        &quot;my_date_histo&quot;:&#123;</span><br><span class=\"line\">            &quot;date_histogram&quot;:&#123;</span><br><span class=\"line\">                &quot;field&quot;:&quot;date&quot;,</span><br><span class=\"line\">                &quot;interval&quot;:&quot;1M&quot;</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            &quot;aggs&quot;:&#123;</span><br><span class=\"line\">                &quot;the_sum&quot;:&#123;</span><br><span class=\"line\">                    &quot;sum&quot;:&#123; &quot;field&quot;: &quot;price&quot; &#125;</span><br><span class=\"line\">                &#125;,</span><br><span class=\"line\">                &quot;the_movavg&quot;: &#123;</span><br><span class=\"line\">                    &quot;moving_avg&quot;:&#123;</span><br><span class=\"line\">                        &quot;buckets_path&quot;: &quot;the_sum&quot;,</span><br><span class=\"line\">                        &quot;window&quot; : 30,</span><br><span class=\"line\">                        &quot;model&quot; : &quot;linear&quot;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&apos;</span><br></pre></td></tr></table></figure>\n<h2 id=\"指数平滑模型\"><a href=\"#指数平滑模型\" class=\"headerlink\" title=\"指数平滑模型\"></a>指数平滑模型</h2><h3 id=\"指数模型：EWMA-Exponentially-Weighted\"><a href=\"#指数模型：EWMA-Exponentially-Weighted\" class=\"headerlink\" title=\"指数模型：EWMA (Exponentially Weighted)\"></a>指数模型：EWMA (Exponentially Weighted)</h3><p>即： 一次指数平滑模型</p>\n<p>EWMA模型通常也成为单指数模型（single-exponential）, 和线性模型的思路类似，离当前点越远的点，重要性越低，具体化为数值的指数下降，对应的参数是alpha。 alpha值越小，下降越慢。（估计是用1 - alpha去计算的）默认的alpha=0.3</p>\n<p>计算模型：s2 = α <em> x2 + (1 - α) </em> s1</p>\n<p>其中α是平滑系数，si是之前i个数据的平滑值，α取值为[0,1]，越接近1，平滑后的值越接近当前时间的数据值，数据越不平滑，α越接近0，平滑后的值越接近前i个数据的平滑值，数据越平滑，α的值通常可以多尝试几次以达到最佳效果。 一次指数平滑算法进行预测的公式为：xi+h=si，其中i为当前最后的一个数据记录的坐标，亦即预测的时间序列为一条直线，不能反映时间序列的趋势和季节性。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XPOST &apos;localhost:9200/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;size&quot;: 0,</span><br><span class=\"line\">    &quot;aggs&quot;: &#123;</span><br><span class=\"line\">        &quot;my_date_histo&quot;:&#123;</span><br><span class=\"line\">            &quot;date_histogram&quot;:&#123;</span><br><span class=\"line\">                &quot;field&quot;:&quot;date&quot;,</span><br><span class=\"line\">                &quot;interval&quot;:&quot;1M&quot;</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            &quot;aggs&quot;:&#123;</span><br><span class=\"line\">                &quot;the_sum&quot;:&#123;</span><br><span class=\"line\">                    &quot;sum&quot;:&#123; &quot;field&quot;: &quot;price&quot; &#125;</span><br><span class=\"line\">                &#125;,</span><br><span class=\"line\">                &quot;the_movavg&quot;: &#123;</span><br><span class=\"line\">                    &quot;moving_avg&quot;:&#123;</span><br><span class=\"line\">                        &quot;buckets_path&quot;: &quot;the_sum&quot;,</span><br><span class=\"line\">                        &quot;window&quot; : 30,</span><br><span class=\"line\">                        &quot;model&quot; : &quot;ewma&quot;,</span><br><span class=\"line\">                        &quot;settings&quot; : &#123;</span><br><span class=\"line\">                            &quot;alpha&quot; : 0.5</span><br><span class=\"line\">                        &#125;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&apos;</span><br></pre></td></tr></table></figure>\n<h3 id=\"二次指数平滑模型-Holt-Linear\"><a href=\"#二次指数平滑模型-Holt-Linear\" class=\"headerlink\" title=\"二次指数平滑模型: Holt-Linear\"></a>二次指数平滑模型: Holt-Linear</h3><p>计算模型：</p>\n<p>s2 = α <em> x2 + (1 - α) </em> (s1 + t1)</p>\n<p>t2 = ß <em> (s2 - s1) + (1 - ß) </em> t1</p>\n<p>默认alpha = 0.3 and beta = 0.1</p>\n<p>二次指数平滑保留了趋势的信息，使得预测的时间序列可以包含之前数据的趋势。二次指数平滑的预测公式为 xi+h=si+hti 二次指数平滑的预测结果是一条斜的直线。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XPOST &apos;localhost:9200/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;size&quot;: 0,</span><br><span class=\"line\">    &quot;aggs&quot;: &#123;</span><br><span class=\"line\">        &quot;my_date_histo&quot;:&#123;</span><br><span class=\"line\">            &quot;date_histogram&quot;:&#123;</span><br><span class=\"line\">                &quot;field&quot;:&quot;date&quot;,</span><br><span class=\"line\">                &quot;interval&quot;:&quot;1M&quot;</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            &quot;aggs&quot;:&#123;</span><br><span class=\"line\">                &quot;the_sum&quot;:&#123;</span><br><span class=\"line\">                    &quot;sum&quot;:&#123; &quot;field&quot;: &quot;price&quot; &#125;</span><br><span class=\"line\">                &#125;,</span><br><span class=\"line\">                &quot;the_movavg&quot;: &#123;</span><br><span class=\"line\">                    &quot;moving_avg&quot;:&#123;</span><br><span class=\"line\">                        &quot;buckets_path&quot;: &quot;the_sum&quot;,</span><br><span class=\"line\">                        &quot;window&quot; : 30,</span><br><span class=\"line\">                        &quot;model&quot; : &quot;holt&quot;,</span><br><span class=\"line\">                        &quot;settings&quot; : &#123;</span><br><span class=\"line\">                            &quot;alpha&quot; : 0.5,</span><br><span class=\"line\">                            &quot;beta&quot; : 0.5</span><br><span class=\"line\">                        &#125;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&apos;</span><br></pre></td></tr></table></figure>\n<h3 id=\"三次指数平滑模型：Holt-Winters无季节模型\"><a href=\"#三次指数平滑模型：Holt-Winters无季节模型\" class=\"headerlink\" title=\"三次指数平滑模型：Holt-Winters无季节模型\"></a>三次指数平滑模型：Holt-Winters无季节模型</h3><p>三次指数平滑在二次指数平滑的基础上保留了季节性的信息，使得其可以预测带有季节性的时间序列。三次指数平滑添加了一个新的参数p来表示平滑后的趋势。</p>\n<p>1: Additive Holt-Winters：Holt-Winters加法模型</p>\n<p>下面是累加的三次指数平滑<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">si=α(xi-pi-k)+(1-α)(si-1+ti-1)</span><br><span class=\"line\">ti=ß(si-si-1)+(1-ß)ti-1</span><br><span class=\"line\">pi=γ(xi-si)+(1-γ)pi-k</span><br></pre></td></tr></table></figure></p>\n<p>其中k为周期</p>\n<p>累加三次指数平滑的预测公式为： xi+h=si+hti+pi-k+(h mod k)</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XPOST &apos;localhost:9200/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;size&quot;: 0,</span><br><span class=\"line\">    &quot;aggs&quot;: &#123;</span><br><span class=\"line\">        &quot;my_date_histo&quot;:&#123;</span><br><span class=\"line\">            &quot;date_histogram&quot;:&#123;</span><br><span class=\"line\">                &quot;field&quot;:&quot;date&quot;,</span><br><span class=\"line\">                &quot;interval&quot;:&quot;1M&quot;</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            &quot;aggs&quot;:&#123;</span><br><span class=\"line\">                &quot;the_sum&quot;:&#123;</span><br><span class=\"line\">                    &quot;sum&quot;:&#123; &quot;field&quot;: &quot;price&quot; &#125;</span><br><span class=\"line\">                &#125;,</span><br><span class=\"line\">                &quot;the_movavg&quot;: &#123;</span><br><span class=\"line\">                    &quot;moving_avg&quot;:&#123;</span><br><span class=\"line\">                        &quot;buckets_path&quot;: &quot;the_sum&quot;,</span><br><span class=\"line\">                        &quot;window&quot; : 30,</span><br><span class=\"line\">                        &quot;model&quot; : &quot;holt_winters&quot;,</span><br><span class=\"line\">                        &quot;settings&quot; : &#123;</span><br><span class=\"line\">                            &quot;type&quot; : &quot;add&quot;,</span><br><span class=\"line\">                            &quot;alpha&quot; : 0.5,</span><br><span class=\"line\">                            &quot;beta&quot; : 0.5,</span><br><span class=\"line\">                            &quot;gamma&quot; : 0.5,</span><br><span class=\"line\">                            &quot;period&quot; : 7</span><br><span class=\"line\">                        &#125;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&apos;</span><br></pre></td></tr></table></figure>\n<p>2: Multiplicative Holt-Winters：Holt-Winters乘法模型</p>\n<p>下式为累乘的三次指数平滑：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">si=αxi/pi-k+(1-α)(si-1+ti-1)</span><br><span class=\"line\">ti=ß(si-si-1)+(1-ß)ti-1</span><br><span class=\"line\">pi=γxi/si+(1-γ)pi-k  其中k为周期</span><br></pre></td></tr></table></figure></p>\n<p>累乘三次指数平滑的预测公式为： xi+h=(si+hti)pi-k+(h mod k)</p>\n<p>α，ß，γ的值都位于[0,1]之间，可以多试验几次以达到最佳效果。</p>\n<p>s,t,p初始值的选取对于算法整体的影响不是特别大，通常的取值为s0=x0,t0=x1-x0,累加时p=0,累乘时p=1.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XPOST &apos;localhost:9200/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;size&quot;: 0,</span><br><span class=\"line\">    &quot;aggs&quot;: &#123;</span><br><span class=\"line\">        &quot;my_date_histo&quot;:&#123;</span><br><span class=\"line\">            &quot;date_histogram&quot;:&#123;</span><br><span class=\"line\">                &quot;field&quot;:&quot;date&quot;,</span><br><span class=\"line\">                &quot;interval&quot;:&quot;1M&quot;</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            &quot;aggs&quot;:&#123;</span><br><span class=\"line\">                &quot;the_sum&quot;:&#123;</span><br><span class=\"line\">                    &quot;sum&quot;:&#123; &quot;field&quot;: &quot;price&quot; &#125;</span><br><span class=\"line\">                &#125;,</span><br><span class=\"line\">                &quot;the_movavg&quot;: &#123;</span><br><span class=\"line\">                    &quot;moving_avg&quot;:&#123;</span><br><span class=\"line\">                        &quot;buckets_path&quot;: &quot;the_sum&quot;,</span><br><span class=\"line\">                        &quot;window&quot; : 30,</span><br><span class=\"line\">                        &quot;model&quot; : &quot;holt_winters&quot;,</span><br><span class=\"line\">                        &quot;settings&quot; : &#123;</span><br><span class=\"line\">                            &quot;type&quot; : &quot;mult&quot;,</span><br><span class=\"line\">                            &quot;alpha&quot; : 0.5,</span><br><span class=\"line\">                            &quot;beta&quot; : 0.5,</span><br><span class=\"line\">                            &quot;gamma&quot; : 0.5,</span><br><span class=\"line\">                            &quot;period&quot; : 7,</span><br><span class=\"line\">                            &quot;pad&quot; : true</span><br><span class=\"line\">                        &#125;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&apos;</span><br></pre></td></tr></table></figure>\n<h2 id=\"预测模型：Prediction\"><a href=\"#预测模型：Prediction\" class=\"headerlink\" title=\"预测模型：Prediction\"></a>预测模型：Prediction</h2><p>使用当前值减去前一个值，其实就是环比增长</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XPOST &apos;localhost:9200/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;size&quot;: 0,</span><br><span class=\"line\">    &quot;aggs&quot;: &#123;</span><br><span class=\"line\">        &quot;my_date_histo&quot;:&#123;</span><br><span class=\"line\">            &quot;date_histogram&quot;:&#123;</span><br><span class=\"line\">                &quot;field&quot;:&quot;date&quot;,</span><br><span class=\"line\">                &quot;interval&quot;:&quot;1M&quot;</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            &quot;aggs&quot;:&#123;</span><br><span class=\"line\">                &quot;the_sum&quot;:&#123;</span><br><span class=\"line\">                    &quot;sum&quot;:&#123; &quot;field&quot;: &quot;price&quot; &#125;</span><br><span class=\"line\">                &#125;,</span><br><span class=\"line\">                &quot;the_movavg&quot;: &#123;</span><br><span class=\"line\">                    &quot;moving_avg&quot;:&#123;</span><br><span class=\"line\">                        &quot;buckets_path&quot;: &quot;the_sum&quot;,</span><br><span class=\"line\">                        &quot;window&quot; : 30,</span><br><span class=\"line\">                        &quot;model&quot; : &quot;simple&quot;,</span><br><span class=\"line\">                        &quot;predict&quot; : 10</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&apos;</span><br></pre></td></tr></table></figure>\n<h2 id=\"最小化：Minimization\"><a href=\"#最小化：Minimization\" class=\"headerlink\" title=\"最小化：Minimization\"></a>最小化：Minimization</h2><p>某些模型（EWMA，Holt-Linear，Holt-Winters）需要配置一个或多个参数。参数选择可能会非常棘手，有时不直观。此外，这些参数的小偏差有时会对输出移动平均线产生剧烈的影响。</p>\n<p>出于这个原因，三个“可调”模型可以在算法上最小化。最小化是一个参数调整的过程，直到模型生成的预测与输出数据紧密匹配为止。最小化并不是完全防护的，并且可能容易过度配合，但是它往往比手动调整有更好的结果。</p>\n<p>ewma和holt_linear默认情况下禁用最小化，而holt_winters默认启用最小化。 Holt-Winters最小化是最有用的，因为它有助于提高预测的准确性。 EWMA和Holt-Linear不是很好的预测指标，主要用于平滑数据，所以最小化对于这些模型来说不太有用。</p>\n<p>通过最小化参数启用/禁用最小化：”minimize” : true</p>\n<h1 id=\"原始数据\"><a href=\"#原始数据\" class=\"headerlink\" title=\"原始数据\"></a>原始数据</h1><p>数据为SSH login数据其中 IP／user已处理<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;_index&quot;: &quot;logstash-sshlogin-others-success-2017-10&quot;,</span><br><span class=\"line\">    &quot;_type&quot;: &quot;sshlogin&quot;,</span><br><span class=\"line\">    &quot;_id&quot;: &quot;AV-weLF8c2nHCDojUbat&quot;,</span><br><span class=\"line\">    &quot;_version&quot;: 2,</span><br><span class=\"line\">    &quot;_score&quot;: 1,</span><br><span class=\"line\">    &quot;_source&quot;: &#123;</span><br><span class=\"line\">        &quot;srcip&quot;: &quot;222.221.238.162&quot;,</span><br><span class=\"line\">        &quot;dstport&quot;: &quot;&quot;,</span><br><span class=\"line\">        &quot;pid&quot;: &quot;20604&quot;,</span><br><span class=\"line\">        &quot;program&quot;: &quot;sshd&quot;,</span><br><span class=\"line\">        &quot;message&quot;: &quot;dwasw-ibb01:Oct 19 23:38:02 176.231.228.130 sshd[20604]: Accepted publickey for nmuser from 222.221.238.162 port 49484 ssh2&quot;,</span><br><span class=\"line\">        &quot;type&quot;: &quot;zhongcai-sshlogin&quot;,</span><br><span class=\"line\">        &quot;ssh_type&quot;: &quot;ssh_successful_login&quot;,</span><br><span class=\"line\">        &quot;forwarded&quot;: &quot;false&quot;,</span><br><span class=\"line\">        &quot;manufacturer&quot;: &quot;others&quot;,</span><br><span class=\"line\">        &quot;IndexTime&quot;: &quot;2017-10&quot;,</span><br><span class=\"line\">        &quot;path&quot;: &quot;/home/logstash/log/logstash_data/audit10/sshlogin/11.txt&quot;,</span><br><span class=\"line\">        &quot;number&quot;: 1,</span><br><span class=\"line\">        &quot;hostname&quot;: &quot;176.231.228.130&quot;,</span><br><span class=\"line\">        &quot;protocol&quot;: &quot;ssh2&quot;,</span><br><span class=\"line\">        &quot;@timestamp&quot;: &quot;2017-10-19T15:38:02.000Z&quot;,</span><br><span class=\"line\">        &quot;ssh_method&quot;: &quot;publickey&quot;,</span><br><span class=\"line\">        &quot;_hostname&quot;: &quot;dwasw-ibb01&quot;,</span><br><span class=\"line\">        &quot;@version&quot;: &quot;1&quot;,</span><br><span class=\"line\">        &quot;host&quot;: &quot;localhost&quot;,</span><br><span class=\"line\">        &quot;srcport&quot;: &quot;49484&quot;,</span><br><span class=\"line\">        &quot;dstip&quot;: &quot;&quot;,</span><br><span class=\"line\">        &quot;category&quot;: &quot;sshlogin&quot;,</span><br><span class=\"line\">        &quot;user&quot;: &quot;nmuser&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"利用ES-API接口去调用查询数据\"><a href=\"#利用ES-API接口去调用查询数据\" class=\"headerlink\" title=\"利用ES API接口去调用查询数据\"></a>利用ES API接口去调用查询数据</h1><p>“interval”: “hour”: hour为单位，这里可以是分钟，小时，天，周，月</p>\n<p>“format”: “yyyy-MM-dd HH”: 聚合结果得日期格式</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&quot;the_sum&quot;: &#123;</span><br><span class=\"line\">    &quot;sum&quot;: &#123;</span><br><span class=\"line\">        &quot;field&quot;: &quot;number&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>number为要聚合得字段</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -POST  &apos;localhost:9200/logstash-sshlogin-others-success-2017-10/sshlogin/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;size&quot;: 0,</span><br><span class=\"line\">  &quot;query&quot;: &#123;</span><br><span class=\"line\">    &quot;term&quot;: &#123;</span><br><span class=\"line\">      &quot;ssh_type&quot;: &quot;ssh_successful_login&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;aggs&quot;: &#123;</span><br><span class=\"line\">    &quot;hour_sum&quot;: &#123;</span><br><span class=\"line\">      &quot;date_histogram&quot;: &#123;</span><br><span class=\"line\">        &quot;field&quot;: &quot;@timestamp&quot;,</span><br><span class=\"line\">        &quot;interval&quot;: &quot;hour&quot;,</span><br><span class=\"line\">        &quot;format&quot;: &quot;yyyy-MM-dd HH&quot;</span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      &quot;aggs&quot;: &#123;</span><br><span class=\"line\">        &quot;the_sum&quot;: &#123;</span><br><span class=\"line\">          &quot;sum&quot;: &#123;</span><br><span class=\"line\">            &quot;field&quot;: &quot;number&quot;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &quot;the_movavg&quot;: &#123;</span><br><span class=\"line\">          &quot;moving_avg&quot;: &#123;</span><br><span class=\"line\">            &quot;buckets_path&quot;: &quot;the_sum&quot;,</span><br><span class=\"line\">            &quot;window&quot;: 30,</span><br><span class=\"line\">            &quot;model&quot;: &quot;holt&quot;,</span><br><span class=\"line\">            &quot;settings&quot;: &#123;</span><br><span class=\"line\">              &quot;alpha&quot;: 0.5,</span><br><span class=\"line\">              &quot;beta&quot;: 0.7</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;&apos;</span><br></pre></td></tr></table></figure>\n<p>得到的结果形式为：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;took&quot; : 35,</span><br><span class=\"line\">  &quot;timed_out&quot; : false,</span><br><span class=\"line\">  &quot;_shards&quot; : &#123;</span><br><span class=\"line\">    &quot;total&quot; : 1,</span><br><span class=\"line\">    &quot;successful&quot; : 1,</span><br><span class=\"line\">    &quot;failed&quot; : 0</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;hits&quot; : &#123;</span><br><span class=\"line\">    &quot;total&quot; : 206821,</span><br><span class=\"line\">    &quot;max_score&quot; : 0.0,</span><br><span class=\"line\">    &quot;hits&quot; : [ ]</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;aggregations&quot; : &#123;</span><br><span class=\"line\">    &quot;hour_sum&quot; : &#123;</span><br><span class=\"line\">      &quot;buckets&quot; : [</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">          &quot;key_as_string&quot; : &quot;2017-09-30 16&quot;,</span><br><span class=\"line\">          &quot;key&quot; : 1506787200000,</span><br><span class=\"line\">          &quot;doc_count&quot; : 227,</span><br><span class=\"line\">          &quot;the_sum&quot; : &#123;</span><br><span class=\"line\">            &quot;value&quot; : 227.0</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">          &quot;key_as_string&quot; : &quot;2017-09-30 17&quot;,</span><br><span class=\"line\">          &quot;key&quot; : 1506790800000,</span><br><span class=\"line\">          &quot;doc_count&quot; : 210,</span><br><span class=\"line\">          &quot;the_sum&quot; : &#123;</span><br><span class=\"line\">            &quot;value&quot; : 210.0</span><br><span class=\"line\">          &#125;,</span><br><span class=\"line\">          &quot;the_movavg&quot; : &#123;</span><br><span class=\"line\">            &quot;value&quot; : 113.5</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">          &quot;key_as_string&quot; : &quot;2017-09-30 18&quot;,</span><br><span class=\"line\">          &quot;key&quot; : 1506794400000,</span><br><span class=\"line\">          &quot;doc_count&quot; : 365,</span><br><span class=\"line\">          &quot;the_sum&quot; : &#123;</span><br><span class=\"line\">            &quot;value&quot; : 365.0</span><br><span class=\"line\">          &#125;,</span><br><span class=\"line\">          &quot;the_movavg&quot; : &#123;</span><br><span class=\"line\">            &quot;value&quot; : 210.0</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"对应得python代码（查询数据到画图）\"><a href=\"#对应得python代码（查询数据到画图）\" class=\"headerlink\" title=\"对应得python代码（查询数据到画图）\"></a>对应得python代码（查询数据到画图）</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># coding: utf-8</span><br><span class=\"line\">from elasticsearch import Elasticsearch</span><br><span class=\"line\">import matplotlib.pyplot as plt</span><br><span class=\"line\">from matplotlib.font_manager import FontManager, FontProperties</span><br><span class=\"line\"></span><br><span class=\"line\">class Smooth:</span><br><span class=\"line\">    def __init__(self,index):</span><br><span class=\"line\">        self.es = Elasticsearch([&apos;localhost:9200&apos;])</span><br><span class=\"line\">        self.index = index</span><br><span class=\"line\">        </span><br><span class=\"line\">    # 处理mac中文编码错误</span><br><span class=\"line\">    def getChineseFont(self):</span><br><span class=\"line\">        return FontProperties(fname=&apos;/System/Library/Fonts/PingFang.ttc&apos;)</span><br><span class=\"line\">    </span><br><span class=\"line\">    # 对index进行聚合</span><br><span class=\"line\">    def agg(self):</span><br><span class=\"line\">        # &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:SS&quot;</span><br><span class=\"line\">        dsl = &apos;&apos;&apos;</span><br><span class=\"line\">                &#123;</span><br><span class=\"line\">                  &quot;size&quot;: 0,</span><br><span class=\"line\">                  &quot;query&quot;: &#123;</span><br><span class=\"line\">                    &quot;term&quot;: &#123;</span><br><span class=\"line\">                      &quot;ssh_type&quot;: &quot;ssh_successful_login&quot;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                  &#125;,</span><br><span class=\"line\">                  &quot;aggs&quot;: &#123;</span><br><span class=\"line\">                    &quot;hour_sum&quot;: &#123;</span><br><span class=\"line\">                      &quot;date_histogram&quot;: &#123;</span><br><span class=\"line\">                        &quot;field&quot;: &quot;@timestamp&quot;,</span><br><span class=\"line\">                        &quot;interval&quot;: &quot;day&quot;,</span><br><span class=\"line\">                        &quot;format&quot;: &quot;dd&quot;</span><br><span class=\"line\">                      &#125;,</span><br><span class=\"line\">                      &quot;aggs&quot;: &#123;</span><br><span class=\"line\">                        &quot;the_sum&quot;: &#123;</span><br><span class=\"line\">                          &quot;sum&quot;: &#123;</span><br><span class=\"line\">                            &quot;field&quot;: &quot;number&quot;</span><br><span class=\"line\">                          &#125;</span><br><span class=\"line\">                        &#125;,</span><br><span class=\"line\">                        &quot;the_movavg&quot;: &#123;</span><br><span class=\"line\">                          &quot;moving_avg&quot;: &#123;</span><br><span class=\"line\">                            &quot;buckets_path&quot;: &quot;the_sum&quot;,</span><br><span class=\"line\">                            &quot;window&quot;: 30,</span><br><span class=\"line\">                            &quot;model&quot;: &quot;holt_winters&quot;,</span><br><span class=\"line\">                            &quot;settings&quot;: &#123;</span><br><span class=\"line\">                              &quot;alpha&quot;: 0.5,</span><br><span class=\"line\">                              &quot;beta&quot;: 0.7</span><br><span class=\"line\">                            &#125;</span><br><span class=\"line\">                          &#125;</span><br><span class=\"line\">                        &#125;</span><br><span class=\"line\">                      &#125;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                  &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                &apos;&apos;&apos;</span><br><span class=\"line\">        res = self.es.search(index=self.index, body=dsl)</span><br><span class=\"line\">        return res[&apos;aggregations&apos;][&apos;hour_sum&apos;][&apos;buckets&apos;]</span><br><span class=\"line\">    </span><br><span class=\"line\">    # 画图</span><br><span class=\"line\">    def draw(self):</span><br><span class=\"line\">        x,y_true,y_pred = [],[],[]</span><br><span class=\"line\">        for one in self.agg():</span><br><span class=\"line\">            x.append(one[&apos;key_as_string&apos;])</span><br><span class=\"line\">            y_true.append(one[&apos;the_sum&apos;][&apos;value&apos;])</span><br><span class=\"line\">            if &apos;the_movavg&apos; in one.keys():       # 前几条数据没有 the_movavg 字段，故将真实值赋值给pred值</span><br><span class=\"line\">                y_pred.append(one[&apos;the_movavg&apos;][&apos;value&apos;])</span><br><span class=\"line\">            else:</span><br><span class=\"line\">                y_pred.append(one[&apos;the_sum&apos;][&apos;value&apos;])</span><br><span class=\"line\">        </span><br><span class=\"line\">        x_line = range(len(x))</span><br><span class=\"line\">        </span><br><span class=\"line\">        plt.figure(figsize=(10,5))</span><br><span class=\"line\">        plt.plot(x_line,y_true,color=&quot;r&quot;)</span><br><span class=\"line\">        plt.plot(x_line,y_pred,color=&quot;g&quot;)</span><br><span class=\"line\">        </span><br><span class=\"line\">        plt.xlabel(u&quot;每单位时间&quot;,fontproperties=self.getChineseFont()) #X轴标签 </span><br><span class=\"line\">        plt.xticks(range(len(x)), x)</span><br><span class=\"line\">        plt.ylabel(u&quot;聚合结果&quot;,fontproperties=self.getChineseFont()) #Y轴标签  </span><br><span class=\"line\">        plt.title(u&quot;10月份 SSH 主机登录成功聚合图&quot;,fontproperties=self.getChineseFont()) # 标题</span><br><span class=\"line\">        plt.legend([u&quot;True value&quot;,u&quot;Predict value&quot;])</span><br><span class=\"line\">        plt.show()</span><br><span class=\"line\"></span><br><span class=\"line\">smooth = Smooth(&quot;logstash-sshlogin-others-success-2017-10&quot;)</span><br><span class=\"line\">print smooth.draw()</span><br></pre></td></tr></table></figure>\n<p>结果图示为：<br><img src=\"http://img.blog.csdn.net/20171120171404972?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"这里写图片描述\"></p>"},{"title":"这夏未眠.序","date":"2016-09-15T10:08:51.000Z","_content":"\n这本书的整体构思是小主大学一年级时刚去的时候的一个想法，当时刚刚步入大学的我们，心里是那么的迷茫与懵懂，开学前两周，除了军训还是军训，晚上偶尔有个空闲时间，我想大概也许是无聊的，记得那个时候坐在图书馆靠窗的位置，看着窗外，没有明月，没有佳人，有的只是一望无际的黑暗。\n<!--More-->\n那个时候，还没有遇到你所想遇到的人，或许回忆还沉淀在高中的时光里，或是幸福，或是苦涩，或是幸福之后的苦涩，回过头来，看着满屋子的学长学姐，心里是及其复杂的，有种说不出的难过，那时候我是不是在想，现在的你（们）会在哪里念大学呢？\n\n想着想着眼角便淌出了泪水，我想我的大学要完成一件至少我自己觉得满意的事，于是便有了你现在看到的这个序，不知道是不是受郭敬明的影响，因为我看过他的唯一一本小说，也是我看过的唯一一本小说——《夏至未至》，我想写一本书，或者更准确的说，我想写一个人的青春。\n\n在13年军训结束之后，我构思了整个体系，定了这本书的名字——《这夏未眠》，熟悉我的朋友，也知道这是我的QQ网名，QQ作为那个时代的记忆，总会残留一些悲伤的故事，于是我到现在四年了，我从没换过QQ网名，或许是害怕，害怕那些好久不联系的朋友，找不到我吧。\n","source":"_posts/夏未眠/这夏未眠-序.md","raw":"---\ntitle: 这夏未眠.序\ndate: 2016-09-15 18:08:51\ntags: [夏未眠]\ncategories: 夏未眠\n---\n\n这本书的整体构思是小主大学一年级时刚去的时候的一个想法，当时刚刚步入大学的我们，心里是那么的迷茫与懵懂，开学前两周，除了军训还是军训，晚上偶尔有个空闲时间，我想大概也许是无聊的，记得那个时候坐在图书馆靠窗的位置，看着窗外，没有明月，没有佳人，有的只是一望无际的黑暗。\n<!--More-->\n那个时候，还没有遇到你所想遇到的人，或许回忆还沉淀在高中的时光里，或是幸福，或是苦涩，或是幸福之后的苦涩，回过头来，看着满屋子的学长学姐，心里是及其复杂的，有种说不出的难过，那时候我是不是在想，现在的你（们）会在哪里念大学呢？\n\n想着想着眼角便淌出了泪水，我想我的大学要完成一件至少我自己觉得满意的事，于是便有了你现在看到的这个序，不知道是不是受郭敬明的影响，因为我看过他的唯一一本小说，也是我看过的唯一一本小说——《夏至未至》，我想写一本书，或者更准确的说，我想写一个人的青春。\n\n在13年军训结束之后，我构思了整个体系，定了这本书的名字——《这夏未眠》，熟悉我的朋友，也知道这是我的QQ网名，QQ作为那个时代的记忆，总会残留一些悲伤的故事，于是我到现在四年了，我从没换过QQ网名，或许是害怕，害怕那些好久不联系的朋友，找不到我吧。\n","slug":"夏未眠/这夏未眠-序","published":1,"updated":"2017-12-19T02:54:56.787Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjbd1r25t0009kxuwl8pnfjew","content":"<p>这本书的整体构思是小主大学一年级时刚去的时候的一个想法，当时刚刚步入大学的我们，心里是那么的迷茫与懵懂，开学前两周，除了军训还是军训，晚上偶尔有个空闲时间，我想大概也许是无聊的，记得那个时候坐在图书馆靠窗的位置，看着窗外，没有明月，没有佳人，有的只是一望无际的黑暗。<br><a id=\"more\"></a><br>那个时候，还没有遇到你所想遇到的人，或许回忆还沉淀在高中的时光里，或是幸福，或是苦涩，或是幸福之后的苦涩，回过头来，看着满屋子的学长学姐，心里是及其复杂的，有种说不出的难过，那时候我是不是在想，现在的你（们）会在哪里念大学呢？</p>\n<p>想着想着眼角便淌出了泪水，我想我的大学要完成一件至少我自己觉得满意的事，于是便有了你现在看到的这个序，不知道是不是受郭敬明的影响，因为我看过他的唯一一本小说，也是我看过的唯一一本小说——《夏至未至》，我想写一本书，或者更准确的说，我想写一个人的青春。</p>\n<p>在13年军训结束之后，我构思了整个体系，定了这本书的名字——《这夏未眠》，熟悉我的朋友，也知道这是我的QQ网名，QQ作为那个时代的记忆，总会残留一些悲伤的故事，于是我到现在四年了，我从没换过QQ网名，或许是害怕，害怕那些好久不联系的朋友，找不到我吧。</p>\n","site":{"data":{}},"excerpt":"<p>这本书的整体构思是小主大学一年级时刚去的时候的一个想法，当时刚刚步入大学的我们，心里是那么的迷茫与懵懂，开学前两周，除了军训还是军训，晚上偶尔有个空闲时间，我想大概也许是无聊的，记得那个时候坐在图书馆靠窗的位置，看着窗外，没有明月，没有佳人，有的只是一望无际的黑暗。<br>","more":"<br>那个时候，还没有遇到你所想遇到的人，或许回忆还沉淀在高中的时光里，或是幸福，或是苦涩，或是幸福之后的苦涩，回过头来，看着满屋子的学长学姐，心里是及其复杂的，有种说不出的难过，那时候我是不是在想，现在的你（们）会在哪里念大学呢？</p>\n<p>想着想着眼角便淌出了泪水，我想我的大学要完成一件至少我自己觉得满意的事，于是便有了你现在看到的这个序，不知道是不是受郭敬明的影响，因为我看过他的唯一一本小说，也是我看过的唯一一本小说——《夏至未至》，我想写一本书，或者更准确的说，我想写一个人的青春。</p>\n<p>在13年军训结束之后，我构思了整个体系，定了这本书的名字——《这夏未眠》，熟悉我的朋友，也知道这是我的QQ网名，QQ作为那个时代的记忆，总会残留一些悲伤的故事，于是我到现在四年了，我从没换过QQ网名，或许是害怕，害怕那些好久不联系的朋友，找不到我吧。</p>"},{"title":"这夏未眠.简介","date":"2016-09-15T10:13:50.000Z","_content":"书整体分为三部分《夏之过往》，《夏之流年》，《夏之未至》。整本书讲的是男主人公顾艾哲（小艾）与莫晨（晨晨）之间的故事，从初中到高中再到大学，从相遇到相知再到相离。\n<!--More-->\n两人同在陌乘一中念初中，同班同学，在中考来临的那段日子，两人相互鼓励，于是顾艾哲（小艾）考上了他从来都没有想过能考上的孟川一高，而莫晨（晨晨）呢，考上了预料之中的平阳一高，而她在这之前却从来没告诉过顾艾哲（小艾）她要去平阳，就这样，两个人分开了，一些都看起来那么顺理成章，一切又看起来那么暗淡失望。\n\n在经历过高中的二年之后，顾艾哲（小艾）终于联系上了莫晨（晨晨），那天晚上，他用妈妈的电话给莫晨（晨晨）通了两个小时的电话，似乎要把两人两年里没有说的话都说完，可是有太多的话是无法用言语表达的，就这样电话欠费了，终止了聊天，可是那天晚上，小艾高兴的一宿没睡，那一晚上，他的笑容都是幸福的。\n\n可是事情永远不会那么顺利，在香山公园里，当他拿起他买的情侣戒指送给晨晨时，晨晨没有接受，说了一堆他也没有听进去的话，就这样，又开始了分离，而谁也不知道这次分离竟然时一辈子的再也不见。\n\n后来的后来，他又遇见了别的女孩，不知道是不是因为后来的女孩都像小艾记忆里的莫晨。只知道，他都很珍惜。","source":"_posts/夏未眠/这夏未眠-简介.md","raw":"---\ntitle: 这夏未眠.简介\ndate: 2016-09-15 18:13:50\ntags: [夏未眠]\ncategories: 夏未眠\n---\n书整体分为三部分《夏之过往》，《夏之流年》，《夏之未至》。整本书讲的是男主人公顾艾哲（小艾）与莫晨（晨晨）之间的故事，从初中到高中再到大学，从相遇到相知再到相离。\n<!--More-->\n两人同在陌乘一中念初中，同班同学，在中考来临的那段日子，两人相互鼓励，于是顾艾哲（小艾）考上了他从来都没有想过能考上的孟川一高，而莫晨（晨晨）呢，考上了预料之中的平阳一高，而她在这之前却从来没告诉过顾艾哲（小艾）她要去平阳，就这样，两个人分开了，一些都看起来那么顺理成章，一切又看起来那么暗淡失望。\n\n在经历过高中的二年之后，顾艾哲（小艾）终于联系上了莫晨（晨晨），那天晚上，他用妈妈的电话给莫晨（晨晨）通了两个小时的电话，似乎要把两人两年里没有说的话都说完，可是有太多的话是无法用言语表达的，就这样电话欠费了，终止了聊天，可是那天晚上，小艾高兴的一宿没睡，那一晚上，他的笑容都是幸福的。\n\n可是事情永远不会那么顺利，在香山公园里，当他拿起他买的情侣戒指送给晨晨时，晨晨没有接受，说了一堆他也没有听进去的话，就这样，又开始了分离，而谁也不知道这次分离竟然时一辈子的再也不见。\n\n后来的后来，他又遇见了别的女孩，不知道是不是因为后来的女孩都像小艾记忆里的莫晨。只知道，他都很珍惜。","slug":"夏未眠/这夏未眠-简介","published":1,"updated":"2017-12-19T02:54:56.787Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjbd1r25v000akxuwv2ds2h7l","content":"<p>书整体分为三部分《夏之过往》，《夏之流年》，《夏之未至》。整本书讲的是男主人公顾艾哲（小艾）与莫晨（晨晨）之间的故事，从初中到高中再到大学，从相遇到相知再到相离。<br><a id=\"more\"></a><br>两人同在陌乘一中念初中，同班同学，在中考来临的那段日子，两人相互鼓励，于是顾艾哲（小艾）考上了他从来都没有想过能考上的孟川一高，而莫晨（晨晨）呢，考上了预料之中的平阳一高，而她在这之前却从来没告诉过顾艾哲（小艾）她要去平阳，就这样，两个人分开了，一些都看起来那么顺理成章，一切又看起来那么暗淡失望。</p>\n<p>在经历过高中的二年之后，顾艾哲（小艾）终于联系上了莫晨（晨晨），那天晚上，他用妈妈的电话给莫晨（晨晨）通了两个小时的电话，似乎要把两人两年里没有说的话都说完，可是有太多的话是无法用言语表达的，就这样电话欠费了，终止了聊天，可是那天晚上，小艾高兴的一宿没睡，那一晚上，他的笑容都是幸福的。</p>\n<p>可是事情永远不会那么顺利，在香山公园里，当他拿起他买的情侣戒指送给晨晨时，晨晨没有接受，说了一堆他也没有听进去的话，就这样，又开始了分离，而谁也不知道这次分离竟然时一辈子的再也不见。</p>\n<p>后来的后来，他又遇见了别的女孩，不知道是不是因为后来的女孩都像小艾记忆里的莫晨。只知道，他都很珍惜。</p>\n","site":{"data":{}},"excerpt":"<p>书整体分为三部分《夏之过往》，《夏之流年》，《夏之未至》。整本书讲的是男主人公顾艾哲（小艾）与莫晨（晨晨）之间的故事，从初中到高中再到大学，从相遇到相知再到相离。<br>","more":"<br>两人同在陌乘一中念初中，同班同学，在中考来临的那段日子，两人相互鼓励，于是顾艾哲（小艾）考上了他从来都没有想过能考上的孟川一高，而莫晨（晨晨）呢，考上了预料之中的平阳一高，而她在这之前却从来没告诉过顾艾哲（小艾）她要去平阳，就这样，两个人分开了，一些都看起来那么顺理成章，一切又看起来那么暗淡失望。</p>\n<p>在经历过高中的二年之后，顾艾哲（小艾）终于联系上了莫晨（晨晨），那天晚上，他用妈妈的电话给莫晨（晨晨）通了两个小时的电话，似乎要把两人两年里没有说的话都说完，可是有太多的话是无法用言语表达的，就这样电话欠费了，终止了聊天，可是那天晚上，小艾高兴的一宿没睡，那一晚上，他的笑容都是幸福的。</p>\n<p>可是事情永远不会那么顺利，在香山公园里，当他拿起他买的情侣戒指送给晨晨时，晨晨没有接受，说了一堆他也没有听进去的话，就这样，又开始了分离，而谁也不知道这次分离竟然时一辈子的再也不见。</p>\n<p>后来的后来，他又遇见了别的女孩，不知道是不是因为后来的女孩都像小艾记忆里的莫晨。只知道，他都很珍惜。</p>"},{"title":"数据结构算法之二叉树","date":"2017-11-12T16:44:41.000Z","_content":"数据结构面试中经常会被问到篇二叉树相关的问题，那么这篇文章会研究下怎么用python来进行二叉树的构建和遍历。\n\n<!--More-->\n\n注意：py2中\n```\nprint root.elem,\n```\n在py3中要换成\n```\nprint (root.elem,end=\"  \")\n```\n```\n# coding:utf-8\n\n# 定义节点类\nclass Node:\n    def __init__(self,elem = -1,):\n        self.elem = elem\n        self.left = None\n        self.right = None\n        \n# 定义二叉树\nclass Tree:\n    def __init__(self):\n        self.root = Node()\n        self.myqu = []\n    \n    # 添加节点\n    def add(self,elem):\n        node = Node(elem)\n        if self.root.elem == -1:         # 判断如果是根节点\n            self.root  = node\n            self.myqu.append(self.root)\n        else:\n            treenode = self.myqu[0]\n            if treenode.left == None:\n                treenode.left = node\n                self.myqu.append(treenode.left)\n            else:\n                treenode.right = node\n                self.myqu.append(treenode.right)\n                self.myqu.pop(0)\n        \n    # 利用递归实现树的先序遍历\n    def xianxu(self,root):\n        if root == None:\n            return\n        print root.elem,\n        self.xianxu(root.left)\n        self.xianxu(root.right)\n        \n    # 利用递归实现树的中序遍历\n    def zhongxu(self,root):\n        if root == None:\n            return \n        self.zhongxu(root.left)\n        print root.elem,\n        self.zhongxu(root.right)\n        \n    # 利用递归实现树的后序遍历\n    def houxu(self,root):\n        if root == None:\n            return \n        self.houxu(root.left)\n        self.houxu(root.right)\n        print root.elem,\n    \n    # 利用队列实现层次遍历\n    def cengci(self,root):\n        if root == None:\n            return\n        myq = []\n        node = root\n        myq.append(node)\n        while myq:\n            node = myq.pop(0)\n            print node.elem,\n            if node.left != None:\n                myq.append(node.left)\n            if node.right != None:\n                myq.append(node.right)\n    \n    # 求树的叶子节点\n    def getYeJiedian(self,root):\n        if root == None:\n            return 0\n        if root.left == None and root.right == None:\n            return 1\n\n        return self.getYeJiedian(root.left) + self.getYeJiedian(root.right)\n\n    # 由先序和中序,还原二叉树\n    def preMidToHou(self,pre,mid):\n        if len(pre)==0:\n            return None\n        if len(pre)==1:\n            Node(mid[0])\n        root = Node(pre[0])\n        root_index = mid.index(pre[0])\n        root.left = self.preMidToHou(pre[1:root_index + 1],mid[:root_index])\n        root.right = self.preMidToHou(pre[root_index + 1:],mid[root_index + 1:])\n        return root\n\n    # 由后序和中序,还原二叉树\n    def preMidToHou(self,mid,hou):\n        if len(hou)==0:\n            return None\n        if len(hou)==1:\n            Node(mid[0])\n        root = Node(hou[-1])\n        root_index = mid.index(hou[-1])\n        root.left = self.preMidToHou(mid[:root_index],hou[:root_index])\n        root.right = self.preMidToHou(mid[root_index + 1:],mid[root_index + 1:])\n        return root\n\n# 创建一个树，添加节点\ntree = Tree()\nfor i in range(10):\n    tree.add(i)\n    \nprint(\"二叉树的先序遍历:\")\nprint(tree.xianxu(tree.root))\n\nprint(\"二叉树的中序遍历:\")\nprint(tree.zhongxu(tree.root))\n\nprint(\"二叉树的后序遍历:\")\nprint(tree.houxu(tree.root))\n\nprint(\"二叉树的层次遍历\")\nprint(tree.cengci(tree.root))\n\nprint(\"\\n二叉树的叶子节点为:\")\nprint(tree.getYeJiedian(tree.root))\n\nprint(\"\\n已知二叉树先序遍历和中序遍历，求后序:\")\nprint(\"先序:\")\nprint(tree.xianxu(tree.root))\nprint(\"中序:\")\nprint(tree.zhongxu(tree.root))\nprint(\"后序:\")\nroot = tree.preMidToHou([0,1,3,7,8,4,9,2,5,6],[7,3,8,1,9,4,0,5,2,6])\nprint(tree.houxu(root))\n\nprint(\"\\n已知二叉树后序遍历和中序遍历，求前序:\")\nprint(\"后序:\")\nprint(tree.houxu(tree.root))\nprint(\"中序:\")\nprint(tree.zhongxu(tree.root))\nprint(\"前序:\")\nroot = tree.preMidToHou([7,3,8,1,9,4,0,5,2,6],[7,8,3,9,4,1,5,6,2,0])\nprint(tree.xianxu(root))\n```\n\n运行结果为：\n```\n二叉树的先序遍历:\n0 1 3 7 8 4 9 2 5 6 None\n\n二叉树的中序遍历:\n7 3 8 1 9 4 0 5 2 6 None\n\n二叉树的后序遍历:\n7 8 3 9 4 1 5 6 2 0 None\n\n二叉树的层次遍历\n0 1 2 3 4 5 6 7 8 9 None\n\n二叉树的叶子节点为:\n5\n\n已知二叉树先序遍历和中序遍历，求后序:\n先序:\n0  1  3  7  8  4  9  2  5  6  None\n中序:\n7  3  8  1  9  4  0  5  2  6  None\n后序:\n1  3  7  8  4  9  0  5  2  6  None\n\n已知二叉树后序遍历和中序遍历，求前序:\n后序:\n7  8  3  9  4  1  5  6  2  0  None\n中序:\n7  3  8  1  9  4  0  5  2  6  None\n前序:\n0  1  3  7  8  4  9  6  2  5  None\n```\n","source":"_posts/数据结构/数据结构算法之二叉树.md","raw":"---\ntitle: 数据结构算法之二叉树\ndate: 2017-11-13 00:44:41\ntags: [数据结构]\ncategories: 技术篇\n---\n数据结构面试中经常会被问到篇二叉树相关的问题，那么这篇文章会研究下怎么用python来进行二叉树的构建和遍历。\n\n<!--More-->\n\n注意：py2中\n```\nprint root.elem,\n```\n在py3中要换成\n```\nprint (root.elem,end=\"  \")\n```\n```\n# coding:utf-8\n\n# 定义节点类\nclass Node:\n    def __init__(self,elem = -1,):\n        self.elem = elem\n        self.left = None\n        self.right = None\n        \n# 定义二叉树\nclass Tree:\n    def __init__(self):\n        self.root = Node()\n        self.myqu = []\n    \n    # 添加节点\n    def add(self,elem):\n        node = Node(elem)\n        if self.root.elem == -1:         # 判断如果是根节点\n            self.root  = node\n            self.myqu.append(self.root)\n        else:\n            treenode = self.myqu[0]\n            if treenode.left == None:\n                treenode.left = node\n                self.myqu.append(treenode.left)\n            else:\n                treenode.right = node\n                self.myqu.append(treenode.right)\n                self.myqu.pop(0)\n        \n    # 利用递归实现树的先序遍历\n    def xianxu(self,root):\n        if root == None:\n            return\n        print root.elem,\n        self.xianxu(root.left)\n        self.xianxu(root.right)\n        \n    # 利用递归实现树的中序遍历\n    def zhongxu(self,root):\n        if root == None:\n            return \n        self.zhongxu(root.left)\n        print root.elem,\n        self.zhongxu(root.right)\n        \n    # 利用递归实现树的后序遍历\n    def houxu(self,root):\n        if root == None:\n            return \n        self.houxu(root.left)\n        self.houxu(root.right)\n        print root.elem,\n    \n    # 利用队列实现层次遍历\n    def cengci(self,root):\n        if root == None:\n            return\n        myq = []\n        node = root\n        myq.append(node)\n        while myq:\n            node = myq.pop(0)\n            print node.elem,\n            if node.left != None:\n                myq.append(node.left)\n            if node.right != None:\n                myq.append(node.right)\n    \n    # 求树的叶子节点\n    def getYeJiedian(self,root):\n        if root == None:\n            return 0\n        if root.left == None and root.right == None:\n            return 1\n\n        return self.getYeJiedian(root.left) + self.getYeJiedian(root.right)\n\n    # 由先序和中序,还原二叉树\n    def preMidToHou(self,pre,mid):\n        if len(pre)==0:\n            return None\n        if len(pre)==1:\n            Node(mid[0])\n        root = Node(pre[0])\n        root_index = mid.index(pre[0])\n        root.left = self.preMidToHou(pre[1:root_index + 1],mid[:root_index])\n        root.right = self.preMidToHou(pre[root_index + 1:],mid[root_index + 1:])\n        return root\n\n    # 由后序和中序,还原二叉树\n    def preMidToHou(self,mid,hou):\n        if len(hou)==0:\n            return None\n        if len(hou)==1:\n            Node(mid[0])\n        root = Node(hou[-1])\n        root_index = mid.index(hou[-1])\n        root.left = self.preMidToHou(mid[:root_index],hou[:root_index])\n        root.right = self.preMidToHou(mid[root_index + 1:],mid[root_index + 1:])\n        return root\n\n# 创建一个树，添加节点\ntree = Tree()\nfor i in range(10):\n    tree.add(i)\n    \nprint(\"二叉树的先序遍历:\")\nprint(tree.xianxu(tree.root))\n\nprint(\"二叉树的中序遍历:\")\nprint(tree.zhongxu(tree.root))\n\nprint(\"二叉树的后序遍历:\")\nprint(tree.houxu(tree.root))\n\nprint(\"二叉树的层次遍历\")\nprint(tree.cengci(tree.root))\n\nprint(\"\\n二叉树的叶子节点为:\")\nprint(tree.getYeJiedian(tree.root))\n\nprint(\"\\n已知二叉树先序遍历和中序遍历，求后序:\")\nprint(\"先序:\")\nprint(tree.xianxu(tree.root))\nprint(\"中序:\")\nprint(tree.zhongxu(tree.root))\nprint(\"后序:\")\nroot = tree.preMidToHou([0,1,3,7,8,4,9,2,5,6],[7,3,8,1,9,4,0,5,2,6])\nprint(tree.houxu(root))\n\nprint(\"\\n已知二叉树后序遍历和中序遍历，求前序:\")\nprint(\"后序:\")\nprint(tree.houxu(tree.root))\nprint(\"中序:\")\nprint(tree.zhongxu(tree.root))\nprint(\"前序:\")\nroot = tree.preMidToHou([7,3,8,1,9,4,0,5,2,6],[7,8,3,9,4,1,5,6,2,0])\nprint(tree.xianxu(root))\n```\n\n运行结果为：\n```\n二叉树的先序遍历:\n0 1 3 7 8 4 9 2 5 6 None\n\n二叉树的中序遍历:\n7 3 8 1 9 4 0 5 2 6 None\n\n二叉树的后序遍历:\n7 8 3 9 4 1 5 6 2 0 None\n\n二叉树的层次遍历\n0 1 2 3 4 5 6 7 8 9 None\n\n二叉树的叶子节点为:\n5\n\n已知二叉树先序遍历和中序遍历，求后序:\n先序:\n0  1  3  7  8  4  9  2  5  6  None\n中序:\n7  3  8  1  9  4  0  5  2  6  None\n后序:\n1  3  7  8  4  9  0  5  2  6  None\n\n已知二叉树后序遍历和中序遍历，求前序:\n后序:\n7  8  3  9  4  1  5  6  2  0  None\n中序:\n7  3  8  1  9  4  0  5  2  6  None\n前序:\n0  1  3  7  8  4  9  6  2  5  None\n```\n","slug":"数据结构/数据结构算法之二叉树","published":1,"updated":"2017-12-19T02:54:56.787Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjbd1r25w000bkxuwizlacbbw","content":"<p>数据结构面试中经常会被问到篇二叉树相关的问题，那么这篇文章会研究下怎么用python来进行二叉树的构建和遍历。</p>\n<a id=\"more\"></a>\n<p>注意：py2中<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print root.elem,</span><br></pre></td></tr></table></figure></p>\n<p>在py3中要换成<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print (root.elem,end=&quot;  &quot;)</span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># coding:utf-8</span><br><span class=\"line\"></span><br><span class=\"line\"># 定义节点类</span><br><span class=\"line\">class Node:</span><br><span class=\"line\">    def __init__(self,elem = -1,):</span><br><span class=\"line\">        self.elem = elem</span><br><span class=\"line\">        self.left = None</span><br><span class=\"line\">        self.right = None</span><br><span class=\"line\">        </span><br><span class=\"line\"># 定义二叉树</span><br><span class=\"line\">class Tree:</span><br><span class=\"line\">    def __init__(self):</span><br><span class=\"line\">        self.root = Node()</span><br><span class=\"line\">        self.myqu = []</span><br><span class=\"line\">    </span><br><span class=\"line\">    # 添加节点</span><br><span class=\"line\">    def add(self,elem):</span><br><span class=\"line\">        node = Node(elem)</span><br><span class=\"line\">        if self.root.elem == -1:         # 判断如果是根节点</span><br><span class=\"line\">            self.root  = node</span><br><span class=\"line\">            self.myqu.append(self.root)</span><br><span class=\"line\">        else:</span><br><span class=\"line\">            treenode = self.myqu[0]</span><br><span class=\"line\">            if treenode.left == None:</span><br><span class=\"line\">                treenode.left = node</span><br><span class=\"line\">                self.myqu.append(treenode.left)</span><br><span class=\"line\">            else:</span><br><span class=\"line\">                treenode.right = node</span><br><span class=\"line\">                self.myqu.append(treenode.right)</span><br><span class=\"line\">                self.myqu.pop(0)</span><br><span class=\"line\">        </span><br><span class=\"line\">    # 利用递归实现树的先序遍历</span><br><span class=\"line\">    def xianxu(self,root):</span><br><span class=\"line\">        if root == None:</span><br><span class=\"line\">            return</span><br><span class=\"line\">        print root.elem,</span><br><span class=\"line\">        self.xianxu(root.left)</span><br><span class=\"line\">        self.xianxu(root.right)</span><br><span class=\"line\">        </span><br><span class=\"line\">    # 利用递归实现树的中序遍历</span><br><span class=\"line\">    def zhongxu(self,root):</span><br><span class=\"line\">        if root == None:</span><br><span class=\"line\">            return </span><br><span class=\"line\">        self.zhongxu(root.left)</span><br><span class=\"line\">        print root.elem,</span><br><span class=\"line\">        self.zhongxu(root.right)</span><br><span class=\"line\">        </span><br><span class=\"line\">    # 利用递归实现树的后序遍历</span><br><span class=\"line\">    def houxu(self,root):</span><br><span class=\"line\">        if root == None:</span><br><span class=\"line\">            return </span><br><span class=\"line\">        self.houxu(root.left)</span><br><span class=\"line\">        self.houxu(root.right)</span><br><span class=\"line\">        print root.elem,</span><br><span class=\"line\">    </span><br><span class=\"line\">    # 利用队列实现层次遍历</span><br><span class=\"line\">    def cengci(self,root):</span><br><span class=\"line\">        if root == None:</span><br><span class=\"line\">            return</span><br><span class=\"line\">        myq = []</span><br><span class=\"line\">        node = root</span><br><span class=\"line\">        myq.append(node)</span><br><span class=\"line\">        while myq:</span><br><span class=\"line\">            node = myq.pop(0)</span><br><span class=\"line\">            print node.elem,</span><br><span class=\"line\">            if node.left != None:</span><br><span class=\"line\">                myq.append(node.left)</span><br><span class=\"line\">            if node.right != None:</span><br><span class=\"line\">                myq.append(node.right)</span><br><span class=\"line\">    </span><br><span class=\"line\">    # 求树的叶子节点</span><br><span class=\"line\">    def getYeJiedian(self,root):</span><br><span class=\"line\">        if root == None:</span><br><span class=\"line\">            return 0</span><br><span class=\"line\">        if root.left == None and root.right == None:</span><br><span class=\"line\">            return 1</span><br><span class=\"line\"></span><br><span class=\"line\">        return self.getYeJiedian(root.left) + self.getYeJiedian(root.right)</span><br><span class=\"line\"></span><br><span class=\"line\">    # 由先序和中序,还原二叉树</span><br><span class=\"line\">    def preMidToHou(self,pre,mid):</span><br><span class=\"line\">        if len(pre)==0:</span><br><span class=\"line\">            return None</span><br><span class=\"line\">        if len(pre)==1:</span><br><span class=\"line\">            Node(mid[0])</span><br><span class=\"line\">        root = Node(pre[0])</span><br><span class=\"line\">        root_index = mid.index(pre[0])</span><br><span class=\"line\">        root.left = self.preMidToHou(pre[1:root_index + 1],mid[:root_index])</span><br><span class=\"line\">        root.right = self.preMidToHou(pre[root_index + 1:],mid[root_index + 1:])</span><br><span class=\"line\">        return root</span><br><span class=\"line\"></span><br><span class=\"line\">    # 由后序和中序,还原二叉树</span><br><span class=\"line\">    def preMidToHou(self,mid,hou):</span><br><span class=\"line\">        if len(hou)==0:</span><br><span class=\"line\">            return None</span><br><span class=\"line\">        if len(hou)==1:</span><br><span class=\"line\">            Node(mid[0])</span><br><span class=\"line\">        root = Node(hou[-1])</span><br><span class=\"line\">        root_index = mid.index(hou[-1])</span><br><span class=\"line\">        root.left = self.preMidToHou(mid[:root_index],hou[:root_index])</span><br><span class=\"line\">        root.right = self.preMidToHou(mid[root_index + 1:],mid[root_index + 1:])</span><br><span class=\"line\">        return root</span><br><span class=\"line\"></span><br><span class=\"line\"># 创建一个树，添加节点</span><br><span class=\"line\">tree = Tree()</span><br><span class=\"line\">for i in range(10):</span><br><span class=\"line\">    tree.add(i)</span><br><span class=\"line\">    </span><br><span class=\"line\">print(&quot;二叉树的先序遍历:&quot;)</span><br><span class=\"line\">print(tree.xianxu(tree.root))</span><br><span class=\"line\"></span><br><span class=\"line\">print(&quot;二叉树的中序遍历:&quot;)</span><br><span class=\"line\">print(tree.zhongxu(tree.root))</span><br><span class=\"line\"></span><br><span class=\"line\">print(&quot;二叉树的后序遍历:&quot;)</span><br><span class=\"line\">print(tree.houxu(tree.root))</span><br><span class=\"line\"></span><br><span class=\"line\">print(&quot;二叉树的层次遍历&quot;)</span><br><span class=\"line\">print(tree.cengci(tree.root))</span><br><span class=\"line\"></span><br><span class=\"line\">print(&quot;\\n二叉树的叶子节点为:&quot;)</span><br><span class=\"line\">print(tree.getYeJiedian(tree.root))</span><br><span class=\"line\"></span><br><span class=\"line\">print(&quot;\\n已知二叉树先序遍历和中序遍历，求后序:&quot;)</span><br><span class=\"line\">print(&quot;先序:&quot;)</span><br><span class=\"line\">print(tree.xianxu(tree.root))</span><br><span class=\"line\">print(&quot;中序:&quot;)</span><br><span class=\"line\">print(tree.zhongxu(tree.root))</span><br><span class=\"line\">print(&quot;后序:&quot;)</span><br><span class=\"line\">root = tree.preMidToHou([0,1,3,7,8,4,9,2,5,6],[7,3,8,1,9,4,0,5,2,6])</span><br><span class=\"line\">print(tree.houxu(root))</span><br><span class=\"line\"></span><br><span class=\"line\">print(&quot;\\n已知二叉树后序遍历和中序遍历，求前序:&quot;)</span><br><span class=\"line\">print(&quot;后序:&quot;)</span><br><span class=\"line\">print(tree.houxu(tree.root))</span><br><span class=\"line\">print(&quot;中序:&quot;)</span><br><span class=\"line\">print(tree.zhongxu(tree.root))</span><br><span class=\"line\">print(&quot;前序:&quot;)</span><br><span class=\"line\">root = tree.preMidToHou([7,3,8,1,9,4,0,5,2,6],[7,8,3,9,4,1,5,6,2,0])</span><br><span class=\"line\">print(tree.xianxu(root))</span><br></pre></td></tr></table></figure>\n<p>运行结果为：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">二叉树的先序遍历:</span><br><span class=\"line\">0 1 3 7 8 4 9 2 5 6 None</span><br><span class=\"line\"></span><br><span class=\"line\">二叉树的中序遍历:</span><br><span class=\"line\">7 3 8 1 9 4 0 5 2 6 None</span><br><span class=\"line\"></span><br><span class=\"line\">二叉树的后序遍历:</span><br><span class=\"line\">7 8 3 9 4 1 5 6 2 0 None</span><br><span class=\"line\"></span><br><span class=\"line\">二叉树的层次遍历</span><br><span class=\"line\">0 1 2 3 4 5 6 7 8 9 None</span><br><span class=\"line\"></span><br><span class=\"line\">二叉树的叶子节点为:</span><br><span class=\"line\">5</span><br><span class=\"line\"></span><br><span class=\"line\">已知二叉树先序遍历和中序遍历，求后序:</span><br><span class=\"line\">先序:</span><br><span class=\"line\">0  1  3  7  8  4  9  2  5  6  None</span><br><span class=\"line\">中序:</span><br><span class=\"line\">7  3  8  1  9  4  0  5  2  6  None</span><br><span class=\"line\">后序:</span><br><span class=\"line\">1  3  7  8  4  9  0  5  2  6  None</span><br><span class=\"line\"></span><br><span class=\"line\">已知二叉树后序遍历和中序遍历，求前序:</span><br><span class=\"line\">后序:</span><br><span class=\"line\">7  8  3  9  4  1  5  6  2  0  None</span><br><span class=\"line\">中序:</span><br><span class=\"line\">7  3  8  1  9  4  0  5  2  6  None</span><br><span class=\"line\">前序:</span><br><span class=\"line\">0  1  3  7  8  4  9  6  2  5  None</span><br></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"<p>数据结构面试中经常会被问到篇二叉树相关的问题，那么这篇文章会研究下怎么用python来进行二叉树的构建和遍历。</p>","more":"<p>注意：py2中<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print root.elem,</span><br></pre></td></tr></table></figure></p>\n<p>在py3中要换成<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print (root.elem,end=&quot;  &quot;)</span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># coding:utf-8</span><br><span class=\"line\"></span><br><span class=\"line\"># 定义节点类</span><br><span class=\"line\">class Node:</span><br><span class=\"line\">    def __init__(self,elem = -1,):</span><br><span class=\"line\">        self.elem = elem</span><br><span class=\"line\">        self.left = None</span><br><span class=\"line\">        self.right = None</span><br><span class=\"line\">        </span><br><span class=\"line\"># 定义二叉树</span><br><span class=\"line\">class Tree:</span><br><span class=\"line\">    def __init__(self):</span><br><span class=\"line\">        self.root = Node()</span><br><span class=\"line\">        self.myqu = []</span><br><span class=\"line\">    </span><br><span class=\"line\">    # 添加节点</span><br><span class=\"line\">    def add(self,elem):</span><br><span class=\"line\">        node = Node(elem)</span><br><span class=\"line\">        if self.root.elem == -1:         # 判断如果是根节点</span><br><span class=\"line\">            self.root  = node</span><br><span class=\"line\">            self.myqu.append(self.root)</span><br><span class=\"line\">        else:</span><br><span class=\"line\">            treenode = self.myqu[0]</span><br><span class=\"line\">            if treenode.left == None:</span><br><span class=\"line\">                treenode.left = node</span><br><span class=\"line\">                self.myqu.append(treenode.left)</span><br><span class=\"line\">            else:</span><br><span class=\"line\">                treenode.right = node</span><br><span class=\"line\">                self.myqu.append(treenode.right)</span><br><span class=\"line\">                self.myqu.pop(0)</span><br><span class=\"line\">        </span><br><span class=\"line\">    # 利用递归实现树的先序遍历</span><br><span class=\"line\">    def xianxu(self,root):</span><br><span class=\"line\">        if root == None:</span><br><span class=\"line\">            return</span><br><span class=\"line\">        print root.elem,</span><br><span class=\"line\">        self.xianxu(root.left)</span><br><span class=\"line\">        self.xianxu(root.right)</span><br><span class=\"line\">        </span><br><span class=\"line\">    # 利用递归实现树的中序遍历</span><br><span class=\"line\">    def zhongxu(self,root):</span><br><span class=\"line\">        if root == None:</span><br><span class=\"line\">            return </span><br><span class=\"line\">        self.zhongxu(root.left)</span><br><span class=\"line\">        print root.elem,</span><br><span class=\"line\">        self.zhongxu(root.right)</span><br><span class=\"line\">        </span><br><span class=\"line\">    # 利用递归实现树的后序遍历</span><br><span class=\"line\">    def houxu(self,root):</span><br><span class=\"line\">        if root == None:</span><br><span class=\"line\">            return </span><br><span class=\"line\">        self.houxu(root.left)</span><br><span class=\"line\">        self.houxu(root.right)</span><br><span class=\"line\">        print root.elem,</span><br><span class=\"line\">    </span><br><span class=\"line\">    # 利用队列实现层次遍历</span><br><span class=\"line\">    def cengci(self,root):</span><br><span class=\"line\">        if root == None:</span><br><span class=\"line\">            return</span><br><span class=\"line\">        myq = []</span><br><span class=\"line\">        node = root</span><br><span class=\"line\">        myq.append(node)</span><br><span class=\"line\">        while myq:</span><br><span class=\"line\">            node = myq.pop(0)</span><br><span class=\"line\">            print node.elem,</span><br><span class=\"line\">            if node.left != None:</span><br><span class=\"line\">                myq.append(node.left)</span><br><span class=\"line\">            if node.right != None:</span><br><span class=\"line\">                myq.append(node.right)</span><br><span class=\"line\">    </span><br><span class=\"line\">    # 求树的叶子节点</span><br><span class=\"line\">    def getYeJiedian(self,root):</span><br><span class=\"line\">        if root == None:</span><br><span class=\"line\">            return 0</span><br><span class=\"line\">        if root.left == None and root.right == None:</span><br><span class=\"line\">            return 1</span><br><span class=\"line\"></span><br><span class=\"line\">        return self.getYeJiedian(root.left) + self.getYeJiedian(root.right)</span><br><span class=\"line\"></span><br><span class=\"line\">    # 由先序和中序,还原二叉树</span><br><span class=\"line\">    def preMidToHou(self,pre,mid):</span><br><span class=\"line\">        if len(pre)==0:</span><br><span class=\"line\">            return None</span><br><span class=\"line\">        if len(pre)==1:</span><br><span class=\"line\">            Node(mid[0])</span><br><span class=\"line\">        root = Node(pre[0])</span><br><span class=\"line\">        root_index = mid.index(pre[0])</span><br><span class=\"line\">        root.left = self.preMidToHou(pre[1:root_index + 1],mid[:root_index])</span><br><span class=\"line\">        root.right = self.preMidToHou(pre[root_index + 1:],mid[root_index + 1:])</span><br><span class=\"line\">        return root</span><br><span class=\"line\"></span><br><span class=\"line\">    # 由后序和中序,还原二叉树</span><br><span class=\"line\">    def preMidToHou(self,mid,hou):</span><br><span class=\"line\">        if len(hou)==0:</span><br><span class=\"line\">            return None</span><br><span class=\"line\">        if len(hou)==1:</span><br><span class=\"line\">            Node(mid[0])</span><br><span class=\"line\">        root = Node(hou[-1])</span><br><span class=\"line\">        root_index = mid.index(hou[-1])</span><br><span class=\"line\">        root.left = self.preMidToHou(mid[:root_index],hou[:root_index])</span><br><span class=\"line\">        root.right = self.preMidToHou(mid[root_index + 1:],mid[root_index + 1:])</span><br><span class=\"line\">        return root</span><br><span class=\"line\"></span><br><span class=\"line\"># 创建一个树，添加节点</span><br><span class=\"line\">tree = Tree()</span><br><span class=\"line\">for i in range(10):</span><br><span class=\"line\">    tree.add(i)</span><br><span class=\"line\">    </span><br><span class=\"line\">print(&quot;二叉树的先序遍历:&quot;)</span><br><span class=\"line\">print(tree.xianxu(tree.root))</span><br><span class=\"line\"></span><br><span class=\"line\">print(&quot;二叉树的中序遍历:&quot;)</span><br><span class=\"line\">print(tree.zhongxu(tree.root))</span><br><span class=\"line\"></span><br><span class=\"line\">print(&quot;二叉树的后序遍历:&quot;)</span><br><span class=\"line\">print(tree.houxu(tree.root))</span><br><span class=\"line\"></span><br><span class=\"line\">print(&quot;二叉树的层次遍历&quot;)</span><br><span class=\"line\">print(tree.cengci(tree.root))</span><br><span class=\"line\"></span><br><span class=\"line\">print(&quot;\\n二叉树的叶子节点为:&quot;)</span><br><span class=\"line\">print(tree.getYeJiedian(tree.root))</span><br><span class=\"line\"></span><br><span class=\"line\">print(&quot;\\n已知二叉树先序遍历和中序遍历，求后序:&quot;)</span><br><span class=\"line\">print(&quot;先序:&quot;)</span><br><span class=\"line\">print(tree.xianxu(tree.root))</span><br><span class=\"line\">print(&quot;中序:&quot;)</span><br><span class=\"line\">print(tree.zhongxu(tree.root))</span><br><span class=\"line\">print(&quot;后序:&quot;)</span><br><span class=\"line\">root = tree.preMidToHou([0,1,3,7,8,4,9,2,5,6],[7,3,8,1,9,4,0,5,2,6])</span><br><span class=\"line\">print(tree.houxu(root))</span><br><span class=\"line\"></span><br><span class=\"line\">print(&quot;\\n已知二叉树后序遍历和中序遍历，求前序:&quot;)</span><br><span class=\"line\">print(&quot;后序:&quot;)</span><br><span class=\"line\">print(tree.houxu(tree.root))</span><br><span class=\"line\">print(&quot;中序:&quot;)</span><br><span class=\"line\">print(tree.zhongxu(tree.root))</span><br><span class=\"line\">print(&quot;前序:&quot;)</span><br><span class=\"line\">root = tree.preMidToHou([7,3,8,1,9,4,0,5,2,6],[7,8,3,9,4,1,5,6,2,0])</span><br><span class=\"line\">print(tree.xianxu(root))</span><br></pre></td></tr></table></figure>\n<p>运行结果为：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">二叉树的先序遍历:</span><br><span class=\"line\">0 1 3 7 8 4 9 2 5 6 None</span><br><span class=\"line\"></span><br><span class=\"line\">二叉树的中序遍历:</span><br><span class=\"line\">7 3 8 1 9 4 0 5 2 6 None</span><br><span class=\"line\"></span><br><span class=\"line\">二叉树的后序遍历:</span><br><span class=\"line\">7 8 3 9 4 1 5 6 2 0 None</span><br><span class=\"line\"></span><br><span class=\"line\">二叉树的层次遍历</span><br><span class=\"line\">0 1 2 3 4 5 6 7 8 9 None</span><br><span class=\"line\"></span><br><span class=\"line\">二叉树的叶子节点为:</span><br><span class=\"line\">5</span><br><span class=\"line\"></span><br><span class=\"line\">已知二叉树先序遍历和中序遍历，求后序:</span><br><span class=\"line\">先序:</span><br><span class=\"line\">0  1  3  7  8  4  9  2  5  6  None</span><br><span class=\"line\">中序:</span><br><span class=\"line\">7  3  8  1  9  4  0  5  2  6  None</span><br><span class=\"line\">后序:</span><br><span class=\"line\">1  3  7  8  4  9  0  5  2  6  None</span><br><span class=\"line\"></span><br><span class=\"line\">已知二叉树后序遍历和中序遍历，求前序:</span><br><span class=\"line\">后序:</span><br><span class=\"line\">7  8  3  9  4  1  5  6  2  0  None</span><br><span class=\"line\">中序:</span><br><span class=\"line\">7  3  8  1  9  4  0  5  2  6  None</span><br><span class=\"line\">前序:</span><br><span class=\"line\">0  1  3  7  8  4  9  6  2  5  None</span><br></pre></td></tr></table></figure></p>"},{"title":"数据结构算法之合并两个有序序列","date":"2017-11-12T16:55:29.000Z","_content":"\n有序序列的合并，python实现。\n<!--More-->\n\n```\n#coding:utf-8\n\na = [2,4,6,8,10]\nb = [3,5,7,9,11,13,15]\nc = []\n\ndef merge(a,b):\n    i,j = 0,0\n    while i<=len(a)-1 and j<=len(b)-1:\n        if a[i]<b[j]:\n            c.append(a[i])\n            i+=1\n        else:\n            c.append(b[j])\n            j+=1\n    if i<=len(a)-1:\n        for m in a[i:]:\n            c.append(m)\n    \n    if j<=len(b)-1:\n        for n in b[j:]:\n            c.append(n)\n    print(c)\n\nmerge(a,b)\n```\n\n运行结果为：\n```\n[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15]\n```","source":"_posts/数据结构/数据结构算法之合并两个有序序列.md","raw":"---\ntitle: 数据结构算法之合并两个有序序列\ndate: 2017-11-13 00:55:29\ntags: [数据结构]\ncategories: 技术篇\n---\n\n有序序列的合并，python实现。\n<!--More-->\n\n```\n#coding:utf-8\n\na = [2,4,6,8,10]\nb = [3,5,7,9,11,13,15]\nc = []\n\ndef merge(a,b):\n    i,j = 0,0\n    while i<=len(a)-1 and j<=len(b)-1:\n        if a[i]<b[j]:\n            c.append(a[i])\n            i+=1\n        else:\n            c.append(b[j])\n            j+=1\n    if i<=len(a)-1:\n        for m in a[i:]:\n            c.append(m)\n    \n    if j<=len(b)-1:\n        for n in b[j:]:\n            c.append(n)\n    print(c)\n\nmerge(a,b)\n```\n\n运行结果为：\n```\n[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15]\n```","slug":"数据结构/数据结构算法之合并两个有序序列","published":1,"updated":"2017-12-19T02:54:56.787Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjbd1r25z000ekxuw8u2wpn78","content":"<p>有序序列的合并，python实现。<br><a id=\"more\"></a></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#coding:utf-8</span><br><span class=\"line\"></span><br><span class=\"line\">a = [2,4,6,8,10]</span><br><span class=\"line\">b = [3,5,7,9,11,13,15]</span><br><span class=\"line\">c = []</span><br><span class=\"line\"></span><br><span class=\"line\">def merge(a,b):</span><br><span class=\"line\">    i,j = 0,0</span><br><span class=\"line\">    while i&lt;=len(a)-1 and j&lt;=len(b)-1:</span><br><span class=\"line\">        if a[i]&lt;b[j]:</span><br><span class=\"line\">            c.append(a[i])</span><br><span class=\"line\">            i+=1</span><br><span class=\"line\">        else:</span><br><span class=\"line\">            c.append(b[j])</span><br><span class=\"line\">            j+=1</span><br><span class=\"line\">    if i&lt;=len(a)-1:</span><br><span class=\"line\">        for m in a[i:]:</span><br><span class=\"line\">            c.append(m)</span><br><span class=\"line\">    </span><br><span class=\"line\">    if j&lt;=len(b)-1:</span><br><span class=\"line\">        for n in b[j:]:</span><br><span class=\"line\">            c.append(n)</span><br><span class=\"line\">    print(c)</span><br><span class=\"line\"></span><br><span class=\"line\">merge(a,b)</span><br></pre></td></tr></table></figure>\n<p>运行结果为：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15]</span><br></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"<p>有序序列的合并，python实现。<br>","more":"</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#coding:utf-8</span><br><span class=\"line\"></span><br><span class=\"line\">a = [2,4,6,8,10]</span><br><span class=\"line\">b = [3,5,7,9,11,13,15]</span><br><span class=\"line\">c = []</span><br><span class=\"line\"></span><br><span class=\"line\">def merge(a,b):</span><br><span class=\"line\">    i,j = 0,0</span><br><span class=\"line\">    while i&lt;=len(a)-1 and j&lt;=len(b)-1:</span><br><span class=\"line\">        if a[i]&lt;b[j]:</span><br><span class=\"line\">            c.append(a[i])</span><br><span class=\"line\">            i+=1</span><br><span class=\"line\">        else:</span><br><span class=\"line\">            c.append(b[j])</span><br><span class=\"line\">            j+=1</span><br><span class=\"line\">    if i&lt;=len(a)-1:</span><br><span class=\"line\">        for m in a[i:]:</span><br><span class=\"line\">            c.append(m)</span><br><span class=\"line\">    </span><br><span class=\"line\">    if j&lt;=len(b)-1:</span><br><span class=\"line\">        for n in b[j:]:</span><br><span class=\"line\">            c.append(n)</span><br><span class=\"line\">    print(c)</span><br><span class=\"line\"></span><br><span class=\"line\">merge(a,b)</span><br></pre></td></tr></table></figure>\n<p>运行结果为：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15]</span><br></pre></td></tr></table></figure></p>"},{"title":"数据结构算法之排序","date":"2017-11-12T16:51:28.000Z","_content":"数据结构面试中经常会被问到篇排序相关的问题，那么这篇文章会研究下怎么用python来实现排序。\n\n<!--More-->\n# 冒泡排序\n```\n#coding：utf-8\n\n# 冒泡排序\ndef maopao():\n    a = [2,1,4,3,9,5,6,8,7]\n    for i in range(len(a)-1):\n        for j in range(len(a)-1-i):\n            if a[j]>a[j+1]:\n                temp = a[j]\n                a[j] = a[j+1]\n                a[j+1] = temp\n    print(a)\nmaopao()\n```\n结果为：\n```\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n```\n---\n# 归并排序\n```\n# 归并排序\ndef merge(a,b):\n    i,j = 0,0\n    c = []\n    while i<=len(a)-1 and j<=len(b)-1:\n        if a[i]<b[j]:\n            c.append(a[i])\n            i+=1\n        else:\n            c.append(b[j])\n            j+=1\n    if i<=len(a)-1:\n        for m in a[i:]:\n            c.append(m)\n    \n    if j<=len(b)-1:\n        for n in b[j:]:\n            c.append(n)\n    return c\n\ndef guibing(a):\n    if len(a)<=1:\n        return a\n    center = int(len(a)/2)\n    left = guibing(a[:center])\n    right = guibing(a[center:])\n    return merge(left,right)\n\nprint(guibing([2,1,4,3,9,5,6,8,7]))\n```\n\n结果为：\n```\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n```\n---\n#　快速排序 \n```\n#　快速排序    \ndef kpsort(left,right,a):\n    based = a[left]\n    i = left\n    j = right\n    while i < j:\n        # 从数组右边开始遍历\n        while a[j]>=based and i<j:\n            j -= 1\n        a[i] = a[j]\n        while a[i]<=based and i<j:\n            i += 1\n        \n        a[j]= a[i]\n        a[i] = based\n\n    return i\n    \ndef kuaipai(left,right,a):\n    if left<right:\n        p = kpsort(left,right,a)\n        kuaipai(left,p-1,a)\n        kuaipai(p+1,right,a)\n\n    return a\n            \nprint(kuaipai(0,8,a =[2,1,4,3,9,5,6,8,7]))\n```\n结果为\n```\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n```","source":"_posts/数据结构/数据结构算法之排序.md","raw":"---\ntitle: 数据结构算法之排序\ndate: 2017-11-13 00:51:28\ntags: [数据结构]\ncategories: 技术篇\n---\n数据结构面试中经常会被问到篇排序相关的问题，那么这篇文章会研究下怎么用python来实现排序。\n\n<!--More-->\n# 冒泡排序\n```\n#coding：utf-8\n\n# 冒泡排序\ndef maopao():\n    a = [2,1,4,3,9,5,6,8,7]\n    for i in range(len(a)-1):\n        for j in range(len(a)-1-i):\n            if a[j]>a[j+1]:\n                temp = a[j]\n                a[j] = a[j+1]\n                a[j+1] = temp\n    print(a)\nmaopao()\n```\n结果为：\n```\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n```\n---\n# 归并排序\n```\n# 归并排序\ndef merge(a,b):\n    i,j = 0,0\n    c = []\n    while i<=len(a)-1 and j<=len(b)-1:\n        if a[i]<b[j]:\n            c.append(a[i])\n            i+=1\n        else:\n            c.append(b[j])\n            j+=1\n    if i<=len(a)-1:\n        for m in a[i:]:\n            c.append(m)\n    \n    if j<=len(b)-1:\n        for n in b[j:]:\n            c.append(n)\n    return c\n\ndef guibing(a):\n    if len(a)<=1:\n        return a\n    center = int(len(a)/2)\n    left = guibing(a[:center])\n    right = guibing(a[center:])\n    return merge(left,right)\n\nprint(guibing([2,1,4,3,9,5,6,8,7]))\n```\n\n结果为：\n```\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n```\n---\n#　快速排序 \n```\n#　快速排序    \ndef kpsort(left,right,a):\n    based = a[left]\n    i = left\n    j = right\n    while i < j:\n        # 从数组右边开始遍历\n        while a[j]>=based and i<j:\n            j -= 1\n        a[i] = a[j]\n        while a[i]<=based and i<j:\n            i += 1\n        \n        a[j]= a[i]\n        a[i] = based\n\n    return i\n    \ndef kuaipai(left,right,a):\n    if left<right:\n        p = kpsort(left,right,a)\n        kuaipai(left,p-1,a)\n        kuaipai(p+1,right,a)\n\n    return a\n            \nprint(kuaipai(0,8,a =[2,1,4,3,9,5,6,8,7]))\n```\n结果为\n```\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n```","slug":"数据结构/数据结构算法之排序","published":1,"updated":"2017-12-19T02:54:56.787Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjbd1r261000fkxuwnyz2tgey","content":"<p>数据结构面试中经常会被问到篇排序相关的问题，那么这篇文章会研究下怎么用python来实现排序。</p>\n<a id=\"more\"></a>\n<h1 id=\"冒泡排序\"><a href=\"#冒泡排序\" class=\"headerlink\" title=\"冒泡排序\"></a>冒泡排序</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#coding：utf-8</span><br><span class=\"line\"></span><br><span class=\"line\"># 冒泡排序</span><br><span class=\"line\">def maopao():</span><br><span class=\"line\">    a = [2,1,4,3,9,5,6,8,7]</span><br><span class=\"line\">    for i in range(len(a)-1):</span><br><span class=\"line\">        for j in range(len(a)-1-i):</span><br><span class=\"line\">            if a[j]&gt;a[j+1]:</span><br><span class=\"line\">                temp = a[j]</span><br><span class=\"line\">                a[j] = a[j+1]</span><br><span class=\"line\">                a[j+1] = temp</span><br><span class=\"line\">    print(a)</span><br><span class=\"line\">maopao()</span><br></pre></td></tr></table></figure>\n<p>结果为：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[1, 2, 3, 4, 5, 6, 7, 8, 9]</span><br></pre></td></tr></table></figure></p>\n<hr>\n<h1 id=\"归并排序\"><a href=\"#归并排序\" class=\"headerlink\" title=\"归并排序\"></a>归并排序</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 归并排序</span><br><span class=\"line\">def merge(a,b):</span><br><span class=\"line\">    i,j = 0,0</span><br><span class=\"line\">    c = []</span><br><span class=\"line\">    while i&lt;=len(a)-1 and j&lt;=len(b)-1:</span><br><span class=\"line\">        if a[i]&lt;b[j]:</span><br><span class=\"line\">            c.append(a[i])</span><br><span class=\"line\">            i+=1</span><br><span class=\"line\">        else:</span><br><span class=\"line\">            c.append(b[j])</span><br><span class=\"line\">            j+=1</span><br><span class=\"line\">    if i&lt;=len(a)-1:</span><br><span class=\"line\">        for m in a[i:]:</span><br><span class=\"line\">            c.append(m)</span><br><span class=\"line\">    </span><br><span class=\"line\">    if j&lt;=len(b)-1:</span><br><span class=\"line\">        for n in b[j:]:</span><br><span class=\"line\">            c.append(n)</span><br><span class=\"line\">    return c</span><br><span class=\"line\"></span><br><span class=\"line\">def guibing(a):</span><br><span class=\"line\">    if len(a)&lt;=1:</span><br><span class=\"line\">        return a</span><br><span class=\"line\">    center = int(len(a)/2)</span><br><span class=\"line\">    left = guibing(a[:center])</span><br><span class=\"line\">    right = guibing(a[center:])</span><br><span class=\"line\">    return merge(left,right)</span><br><span class=\"line\"></span><br><span class=\"line\">print(guibing([2,1,4,3,9,5,6,8,7]))</span><br></pre></td></tr></table></figure>\n<p>结果为：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[1, 2, 3, 4, 5, 6, 7, 8, 9]</span><br></pre></td></tr></table></figure></p>\n<hr>\n<h1 id=\"快速排序\"><a href=\"#快速排序\" class=\"headerlink\" title=\"　快速排序\"></a>　快速排序</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#　快速排序    </span><br><span class=\"line\">def kpsort(left,right,a):</span><br><span class=\"line\">    based = a[left]</span><br><span class=\"line\">    i = left</span><br><span class=\"line\">    j = right</span><br><span class=\"line\">    while i &lt; j:</span><br><span class=\"line\">        # 从数组右边开始遍历</span><br><span class=\"line\">        while a[j]&gt;=based and i&lt;j:</span><br><span class=\"line\">            j -= 1</span><br><span class=\"line\">        a[i] = a[j]</span><br><span class=\"line\">        while a[i]&lt;=based and i&lt;j:</span><br><span class=\"line\">            i += 1</span><br><span class=\"line\">        </span><br><span class=\"line\">        a[j]= a[i]</span><br><span class=\"line\">        a[i] = based</span><br><span class=\"line\"></span><br><span class=\"line\">    return i</span><br><span class=\"line\">    </span><br><span class=\"line\">def kuaipai(left,right,a):</span><br><span class=\"line\">    if left&lt;right:</span><br><span class=\"line\">        p = kpsort(left,right,a)</span><br><span class=\"line\">        kuaipai(left,p-1,a)</span><br><span class=\"line\">        kuaipai(p+1,right,a)</span><br><span class=\"line\"></span><br><span class=\"line\">    return a</span><br><span class=\"line\">            </span><br><span class=\"line\">print(kuaipai(0,8,a =[2,1,4,3,9,5,6,8,7]))</span><br></pre></td></tr></table></figure>\n<p>结果为<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[1, 2, 3, 4, 5, 6, 7, 8, 9]</span><br></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"<p>数据结构面试中经常会被问到篇排序相关的问题，那么这篇文章会研究下怎么用python来实现排序。</p>","more":"<h1 id=\"冒泡排序\"><a href=\"#冒泡排序\" class=\"headerlink\" title=\"冒泡排序\"></a>冒泡排序</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#coding：utf-8</span><br><span class=\"line\"></span><br><span class=\"line\"># 冒泡排序</span><br><span class=\"line\">def maopao():</span><br><span class=\"line\">    a = [2,1,4,3,9,5,6,8,7]</span><br><span class=\"line\">    for i in range(len(a)-1):</span><br><span class=\"line\">        for j in range(len(a)-1-i):</span><br><span class=\"line\">            if a[j]&gt;a[j+1]:</span><br><span class=\"line\">                temp = a[j]</span><br><span class=\"line\">                a[j] = a[j+1]</span><br><span class=\"line\">                a[j+1] = temp</span><br><span class=\"line\">    print(a)</span><br><span class=\"line\">maopao()</span><br></pre></td></tr></table></figure>\n<p>结果为：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[1, 2, 3, 4, 5, 6, 7, 8, 9]</span><br></pre></td></tr></table></figure></p>\n<hr>\n<h1 id=\"归并排序\"><a href=\"#归并排序\" class=\"headerlink\" title=\"归并排序\"></a>归并排序</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 归并排序</span><br><span class=\"line\">def merge(a,b):</span><br><span class=\"line\">    i,j = 0,0</span><br><span class=\"line\">    c = []</span><br><span class=\"line\">    while i&lt;=len(a)-1 and j&lt;=len(b)-1:</span><br><span class=\"line\">        if a[i]&lt;b[j]:</span><br><span class=\"line\">            c.append(a[i])</span><br><span class=\"line\">            i+=1</span><br><span class=\"line\">        else:</span><br><span class=\"line\">            c.append(b[j])</span><br><span class=\"line\">            j+=1</span><br><span class=\"line\">    if i&lt;=len(a)-1:</span><br><span class=\"line\">        for m in a[i:]:</span><br><span class=\"line\">            c.append(m)</span><br><span class=\"line\">    </span><br><span class=\"line\">    if j&lt;=len(b)-1:</span><br><span class=\"line\">        for n in b[j:]:</span><br><span class=\"line\">            c.append(n)</span><br><span class=\"line\">    return c</span><br><span class=\"line\"></span><br><span class=\"line\">def guibing(a):</span><br><span class=\"line\">    if len(a)&lt;=1:</span><br><span class=\"line\">        return a</span><br><span class=\"line\">    center = int(len(a)/2)</span><br><span class=\"line\">    left = guibing(a[:center])</span><br><span class=\"line\">    right = guibing(a[center:])</span><br><span class=\"line\">    return merge(left,right)</span><br><span class=\"line\"></span><br><span class=\"line\">print(guibing([2,1,4,3,9,5,6,8,7]))</span><br></pre></td></tr></table></figure>\n<p>结果为：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[1, 2, 3, 4, 5, 6, 7, 8, 9]</span><br></pre></td></tr></table></figure></p>\n<hr>\n<h1 id=\"快速排序\"><a href=\"#快速排序\" class=\"headerlink\" title=\"　快速排序\"></a>　快速排序</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#　快速排序    </span><br><span class=\"line\">def kpsort(left,right,a):</span><br><span class=\"line\">    based = a[left]</span><br><span class=\"line\">    i = left</span><br><span class=\"line\">    j = right</span><br><span class=\"line\">    while i &lt; j:</span><br><span class=\"line\">        # 从数组右边开始遍历</span><br><span class=\"line\">        while a[j]&gt;=based and i&lt;j:</span><br><span class=\"line\">            j -= 1</span><br><span class=\"line\">        a[i] = a[j]</span><br><span class=\"line\">        while a[i]&lt;=based and i&lt;j:</span><br><span class=\"line\">            i += 1</span><br><span class=\"line\">        </span><br><span class=\"line\">        a[j]= a[i]</span><br><span class=\"line\">        a[i] = based</span><br><span class=\"line\"></span><br><span class=\"line\">    return i</span><br><span class=\"line\">    </span><br><span class=\"line\">def kuaipai(left,right,a):</span><br><span class=\"line\">    if left&lt;right:</span><br><span class=\"line\">        p = kpsort(left,right,a)</span><br><span class=\"line\">        kuaipai(left,p-1,a)</span><br><span class=\"line\">        kuaipai(p+1,right,a)</span><br><span class=\"line\"></span><br><span class=\"line\">    return a</span><br><span class=\"line\">            </span><br><span class=\"line\">print(kuaipai(0,8,a =[2,1,4,3,9,5,6,8,7]))</span><br></pre></td></tr></table></figure>\n<p>结果为<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[1, 2, 3, 4, 5, 6, 7, 8, 9]</span><br></pre></td></tr></table></figure></p>"},{"title":"MachingLearning中的距离和相似性计算以及python实现","date":"2017-07-16T04:14:43.000Z","_content":"\n# 前言\n写这篇文章的目的不是说摘抄网上其他人的总结，刚才最近在看这方面的东西，为了让自己能够实际的去感受下每种求距离公式的差别，然后用python进行具体实现。\n<!--More-->\n在机器学习中，经常要用到距离和相似性的计算公式，我么要常计算个体之间的差异大小，继而评价个人之间的差异性和相似性，最常见的就是数据分析中的相关分析，数据挖掘中的分类和聚类算法。如利用k-means进行聚类时，判断个体所属的类别，要利用距离计算公式计算个体到簇心的距离，如利用KNN进行分类时，计算个体与已知类别之间的相似性，从而判断个体所属的类别等。\n\n\n文章编辑的过程中或许存在一个错误或者不合理的地方，欢迎指正。\n\n参考：http://www.cnblogs.com/heaad/archive/2011/03/08/1977733.html\n\n推荐：https://my.oschina.net/hunglish/blog/787596\n\n# 欧氏距离\n也称欧几里得距离，是指在m维空间中两个点之间的真实距离。欧式距离在ML中使用的范围比较广，也比较通用，就比如说利用k-Means对二维平面内的数据点进行聚类，对魔都房价的聚类分析（price/m^2 与平均房价）等。\n## 二维空间的欧氏距离 \n二维平面上两点a(x1,y1)与b(x2,y2)间的欧氏距离\n\n$$\nd12 =\\sqrt{(x_{1}-x_{2})^2+(y_{1}-y_{2})^2}\n$$\npython 实现为：\n\n```\n# coding: utf-8\n\nfrom numpy import *\n\ndef twoPointDistance(a,b):\n\td = sqrt( (a[0]-b[0])**2 + (a[1]-b[1])**2 )\n\treturn d\n\nprint 'a,b 二维距离为：',twoPointDistance((1,1),(2,2))\n```\n\n\n## 三维空间的欧氏距离\n三维空间两点a(x1,y1,z1)与b(x2,y2,z2)间的欧氏距离\n\n$$d12 =\\sqrt{(x_{1}-x_{2})^2+(y_{1}-y_{2})^2+(z_{1}-z_{2})^2}$$\npython 实现为：\n```\ndef threePointDistance(a,b):\n\td = sqrt( (a[0]-b[0])**2 + (a[1]-b[1])**2 + (a[2]-b[2])**2 )\n\treturn d\n\nprint 'a,b 三维距离为：',threePointDistance((1,1,1),(2,2,2))\n```\n\n## 多维空间的欧氏距离\n两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的欧氏距离\n\n$$ \n\\sqrt{\\sum_{n}^{k=1}(x_{1k}-x_{2k})^2 }\n$$\npython 实现为：\n\n```\ndef distance(a,b):\n\tsum = 0\n\tfor i in range(len(a)):\n\t\tsum += (a[i]-b[i])**2\n\treturn sqrt(sum)\n\nprint 'a,b 多维距离为：',distance((1,1,2,2),(2,2,4,4))\n```\n这里传入的参数可以是任意维的，该公式也适应上边的二维和三维\n\n# 标准欧氏距离\n标准化欧氏距离是针对简单欧氏距离的缺点而作的一种改进方案。标准欧氏距离的思路：既然数据各维分量的分布不一样，好吧！那我先将各个分量都“标准化”到均值、方差相等吧。均值和方差标准化到多少呢？这里先复习点统计学知识吧，假设样本集X的均值(mean)为m，标准差(standard deviation)为s，那么X的“标准化变量”表示为：\n\n　　而且标准化变量的数学期望为0，方差为1。因此样本集的标准化过程(standardization)用公式描述就是：\n　　\n$$\nX^* = \\frac{X-m}{s}\n$$\n标准化后的值 =  ( 标准化前的值  － 分量的均值 ) /分量的标准差\n\n经过简单的推导就可以得到两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的标准化欧氏距离的公式：\n\n$$\nd_{12} =\\sqrt {\\sum_{k=1}^{n} (\\frac{x_{1k}-x_{2k}}{s_{k}})^2}\n$$\n如果将方差的倒数看成是一个权重，这个公式可以看成是一种加权欧氏距离(Weighted Euclidean distance)。\n\npython 实现为\n```\ndef moreBZOSdis(a,b):\n    sumnum = 0\n    for i in range(len(a)):\n        # 计算si 分量标准差\n        avg = (a[i]-b[i])/2\n        si = sqrt( (a[i] - avg) ** 2 + (b[i] - avg) ** 2 )\n        sumnum += ((a[i]-b[i])/si ) ** 2\n\t\n    return sqrt(sumnum)\n\nprint 'a,b 标准欧式距离：',moreBZOSdis((1,2,1,2),(3,3,3,4))\n```\n\n# 曼哈顿距离\n又称为城市街区距离（City Block distance）, 想象你在曼哈顿要从一个十字路口开车到另外一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。实际驾驶距离就是这个“曼哈顿距离”。而这也是曼哈顿距离名称的来源。同样曼哈顿距离也分为二维，三维和多维。\n\n在计程车几何学中，一个圆是由从圆心向各个固定曼哈顿距离标示出来的点围成的区域，因此这种圆其实就是旋转了45度的正方形。如果有一群圆，且任两圆皆相交，则整群圆必在某点相交；因此曼哈顿距离会形成一个超凸度量空间。\n\n这里有一篇人脸表情分类的论文采用的曼哈顿距离进行计算的，[一种人脸表情分类的新方法——Manhattan距离](http://download.csdn.net/detail/gamer_gyt/9899825)\n\n## 二维曼哈顿距离\n二维平面两点a(x1,y1)与b(x2,y2)间的曼哈顿距离\n\n$$\nd12 =\\left | x_{1}-x_{2} \\right |  + \\left |y_{1}-y_{2}  \\right |\n$$\npython实现为\n```\ndef twoMHDdis(a,b):\n    return abs(a[0]-b[0])+abs(a[1]-b[1])\n\nprint 'a,b 二维曼哈顿距离为：', twoMHDdis((1,1),(2,2)) \n```\n\n\n## 三维曼哈顿距离\n三维平面两点a(x1,y1,z1)与b(x2,y2,z2)间的曼哈顿距离\n\n$$\nd12 =\\left | x_{1}-x_{2} \\right |  + \\left |y_{1}-y_{2}  \\right | + \\left |z_{1}-z_{2}  \\right |\n$$\npython实现为\n```\ndef threeMHDdis(a,b):\n\treturn abs(a[0]-b[0])+abs(a[1]-b[1]) + abs(a[2]-b[2])\n \nprint 'a,b 三维曼哈顿距离为：', threeMHDdis((1,1,1),(2,2,2)) \n```\n\n## 多维曼哈顿距离\n多维平面两点a(x1,y1)与b(x2,y2)间的曼哈顿距离\n$$\nd12 = \\sum_{k=1}^{n} \\left | x_{1k} - x_{2k} \\right |\n$$\npython实现为\n```\ndef moreMHDdis(a,b):\n    sum = 0 \n    for i in range(len(a)):\n        sum += abs(a[i]-b[i])\n    return sum\n\nprint 'a,b 多维曼哈顿距离为：', moreMHDdis((1,1,1,1),(2,2,2,2)) \n```\n由于维距离计算是比较灵活的，所以也同样适合二维和三维。\n\n# 切比雪夫距离\n切比雪夫距离（Chebyshev Distance）的定义为：max( | x2-x1 | , |y2-y1 | , ... ), 切比雪夫距离用的时候数据的维度必须是三个以上，这篇文章中[曼哈顿距离，欧式距离，明式距离，切比雪夫距离区别](http://blog.csdn.net/jerry81333/article/details/52632687) 给了一个很形象的解释如下：\n```\n比如，有同样两个人，在纽约准备到北京参拜天安门，同一个地点出发的话，按照欧式距离来计算，是完全一样的。\n\n但是按照切比雪夫距离，这是完全不同的概念了。\n\n譬如，其中一个人是土豪，另一个人是中产阶级，第一个人就能当晚直接头等舱走人，而第二个人可能就要等机票什么时候打折再去，或者选择坐船什么的。\n\n这样来看的话，距离是不是就不一样了呢？\n\n或者还是不清楚，我再说的详细点。\n\n同样是这两个人，欧式距离是直接算最短距离的，而切比雪夫距离可能还得加上财力，比如第一个人财富值100，第二个只有30，虽然物理距离一样，但是所包含的内容却是不同的。\n```\n\n## 二维切比雪夫距离\n\n二维平面两点a(x1,y1)与b(x2,y2)间的切比雪夫距离\n\n$$\nd_{12} = max( \\left | x_{1} - x_{2} \\right | , \\left | y_{1} - y_{2} \\right |)\n$$\npython 实现为\n\n```\ndef twoQBXFdis(a,b):\n    return max( abs(a[0]-b[0]), abs(a[1]-b[1]))\n\nprint 'a,b二维切比雪夫距离：' , twoQBXFdis((1,2),(3,4))\n```\n\n\n## 多维切比雪夫距离\n\n两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的切比雪夫距离\n$$\nd12 = max_{i\\epsilon n}( \\left | x_{1i} - x_{2i} \\right | )\n$$\n\npython 实现为\n```\ndef moreQBXFdis(a,b):\n    maxnum = 0\n    for i in range(len(a)):\n        if abs(a[i]-b[i]) > maxnum:\n            maxnum = abs(a[i]-b[i])\n    return maxnum\n\nprint 'a,b多维切比雪夫距离：' , moreQBXFdis((1,1,1,1),(3,4,3,4))\n```\n\n# 马氏距离\n有M个样本向量X1~Xm，协方差矩阵记为S，均值记为向量μ，则其中样本向量X到u的马氏距离表示为\n\n$$\nD(x) = \\sqrt{(X-\\mu )^TS^{-1}(X-\\mu)}\n$$\n而其中向量Xi与Xj之间的马氏距离定义为\n$$\nD(X_{i},X_{j}) = \\sqrt{(X_{i}-X_{j} )^TS^{-1}(X_{i}-X_{j} )}\n$$\n 若协方差矩阵是单位矩阵（各个样本向量之间独立同分布）,则公式就成了：\n$$\nD(X_{i},X_{j}) = \\sqrt{(X_{i}-X_{j} )^T(X_{i}-X_{j} )}\n$$\n也就是欧氏距离了。\n\n若协方差矩阵是对角矩阵，公式变成了标准化欧氏距离。\n\n马氏距离的优缺点：量纲无关，排除变量之间的相关性的干扰。\n\n\n# 夹角余弦\n几何中夹角余弦可用来衡量两个向量方向的差异，机器学习中借用这一概念来衡量样本向量之间的差异。\n\n## 二维空间向量的夹角余弦相似度\n在二维空间中向量A(x1,y1)与向量B(x2,y2)的夹角余弦公式：\n\n$$\n\\cos \\theta  = \\frac{x_{1}x_{2} + y_{1}y_{2}}{ \\sqrt{ x_{1}^2+x_{2}^2 }\\sqrt{ y_{1}^2+y_{2}^2 } }\n$$\npython 实现为\n```\ndef twoCos(a,b):\n    cos = (a[0]*b[0]+a[1]*b[1]) / (sqrt(a[0]**2 + b[0]**2) * sqrt(a[1]**2 + b[1]**2) )\n\n    return cos\nprint 'a,b 二维夹角余弦距离：',twoCos((1,1),(2,2))\n```\n\n\n## 多维空间向量的夹角余弦相似度\n两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)的夹角余弦\n\n类似的，对于两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)，可以使用类似于夹角余弦的概念来衡量它们间的相似程度。\n$$\n\\cos \\theta  = \\frac{a \\cdot  b}{\\left | a \\right | \\left | b \\right |}\n$$\n即：\n$$\n\\cos \\theta  = \\frac{ \\sum_{k=1}^{n} x_{1k}x_{2k} }{ \\sqrt{ \\sum_{k=1}^{n}x_{1k}^2 }\\sqrt{ \\sum_{k=1}^{n} x_{2k}^2 } }\n$$\npython实现为\n```\ndef moreCos(a,b):\n    sum_fenzi = 0.0\n    sum_fenmu_1,sum_fenmu_2 = 0,0\n    for i in range(len(a)):\n        sum_fenzi += a[i]*b[i]\n        sum_fenmu_1 += a[i]**2 \n        sum_fenmu_2 += b[i]**2 \n\n    return sum_fenzi/( sqrt(sum_fenmu_1) * sqrt(sum_fenmu_2) )\nprint 'a,b 多维夹角余弦距离：',moreCos((1,1,1,1),(2,2,2,2))\n```\n\n夹角余弦取值范围为[-1,1]。夹角余弦越大表示两个向量的夹角越小，夹角余弦越小表示两向量的夹角越大。当两个向量的方向重合时夹角余弦取最大值1，当两个向量的方向完全相反夹角余弦取最小值-1。\n\n# 闵可夫斯基距离\n\n闵氏距离不是一种距离，而是一组距离的定义\n## 定义\n两个n维变量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的闵可夫斯基距离定义为：\n$$\n\\sqrt[p]{ \\sum_{k=1}^{n} \\left | x_{1k}-x_{2k} \\right |^p} \n$$\n\n其中p是一个变参数。\n\n当p=1时，就是曼哈顿距离\n\n当p=2时，就是欧氏距离\n\n当p→∞时，就是切比雪夫距离\n\n根据变参数的不同，闵氏距离可以表示一类的距离。\n\n## 闵氏距离的缺点\n闵氏距离，包括曼哈顿距离、欧氏距离和切比雪夫距离都存在明显的缺点。\n\n举个例子：二维样本(身高,体重)，其中身高范围是150 ~ 190，体重范围是50 ~ 60，有三个样本：a(180,50)，b(190,50)，c(180,60)。那么a与b之间的闵氏距离（无论是曼哈顿距离、欧氏距离或切比雪夫距离）等于a与c之间的闵氏距离，但是身高的10cm真的等价于体重的10kg么？因此用闵氏距离来衡量这些样本间的相似度很有问题。\n\n简单说来，闵氏距离的缺点主要有两个：(1)将各个分量的量纲(scale)，也就是“单位”当作相同的看待了。(2)没有考虑各个分量的分布（期望，方差等)可能是不同的。\n\n\n# 汉明距离\n\n## 定义\n两个等长字符串s1与s2之间的汉明距离定义为将其中一个变为另外一个所需要作的最小替换次数。例如字符串“1111”与“1001”之间的汉明距离为2。\n\n应用：信息编码（为了增强容错性，应使得编码间的最小汉明距离尽可能大）。\n\n## python 实现\n```\ndef hanmingDis(a,b):\n    sumnum = 0\n    for i in range(len(a)):\n        if a[i]!=b[i]:\n            sumnum += 1\n    return sumnum\n\nprint 'a,b 汉明距离：',hanmingDis((1,1,2,3),(2,2,1,3))\n```\n\n# 杰卡德距离 & 杰卡德相似系数\n## 杰卡德距离\n与杰卡德相似系数相反的概念是杰卡德距离(Jaccard distance)。杰卡德距离可用如下公式表示：\n$$\nJ_{\\delta} (A,B) = \\frac{| A \\bigcup B | - | A \\bigcap B |}{| A \\bigcup B |}\n$$\n杰卡德距离用两个集合中不同元素占所有元素的比例来衡量两个集合的区分度。\n\npython 实现\n```\ndef jiekadeDis(a,b):\n    set_a = set(a)\n    set_b = set(b)\n    dis = float(len( (set_a | set_b) - (set_a & set_b) ) )/ len(set_a | set_b)\n    return dis\n\nprint 'a,b 杰卡德距离：', jiekadeDis((1,2,3),(2,3,4))\n```\n\n## 杰卡德相似系数\n两个集合A和B的交集元素在A，B的并集中所占的比例，称为两个集合的杰卡德相似系数，用符号J(A,B)表示。\n$$\nJ(A,B) = \\frac{| A \\bigcap B |}{| A \\bigcup B |}\n$$\n杰卡德相似系数是衡量两个集合的相似度一种指标。\n\npython 实现\n```\ndef jiekadeXSDis(a,b):\n    set_a = set(a)\n    set_b = set(b)\n    dis = float(len(set_a & set_b)  )/ len(set_a | set_b)\n    return dis\n\nprint 'a,b 杰卡德相似系数：', jiekadeXSDis((1,2,3),(2,3,4))\n```\n\n## 杰卡德相似系数与杰卡德距离的应用\n可将杰卡德相似系数用在衡量样本的相似度上。\n\n　　样本A与样本B是两个n维向量，而且所有维度的取值都是0或1。例如：A(0111)和B(1011)。我们将样本看成是一个集合，1表示集合包含该元素，0表示集合不包含该元素。\n\np ：样本A与B都是1的维度的个数\n\nq ：样本A是1，样本B是0的维度的个数\n\nr ：样本A是0，样本B是1的维度的个数\n\ns ：样本A与B都是0的维度的个数\n\n\n\n那么样本A与B的杰卡德相似系数可以表示为：\n\n这里p+q+r可理解为A与B的并集的元素个数，而p是A与B的交集的元素个数。\n\n而样本A与B的杰卡德距离表示为：\n$$\nJ= \\frac{p}{p+q+r}\n$$\n\n# 相关系数 & 相关距离\n## 相关系数\n$$\n\\rho_{XY} = \\frac{Cov(X,Y)}{\\sqrt{D(X)} \\sqrt{D(Y)}}=\\frac{ E( (X-EX) (Y-EY) ) }{ \\sqrt{D(X)} \\sqrt{D(Y)} }\n$$\n相关系数是衡量随机变量X与Y相关程度的一种方法，相关系数的取值范围是[-1,1]。相关系数的绝对值越大，则表明X与Y相关度越高。当X与Y线性相关时，相关系数取值为1（正线性相关）或-1（负线性相关）。\n\npython 实现\n相关系数可以利用numpy库中的corrcoef函数来计算\n例如 对于矩阵a,numpy.corrcoef(a)可计算行与行之间的相关系数，numpy.corrcoef(a,rowvar=0)用于计算各列之间的相关系数，输出为相关系数矩阵。\n```\nfrom numpy import  *\na = array([[1, 1, 2, 2, 3],  \n       [2, 2, 3, 3, 5],  \n       [1, 4, 2, 2, 3]]) \n\nprint corrcoef(a)\n\n>>array([[ 1.        ,  0.97590007,  0.10482848],\n       [ 0.97590007,  1.        ,  0.17902872],\n       [ 0.10482848,  0.17902872,  1.        ]])\n\nprint corrcoef(a,rowvar=0)\n\n>>array([[ 1.        , -0.18898224,  1.        ,  1.        ,  1.        ],\n       [-0.18898224,  1.        , -0.18898224, -0.18898224, -0.18898224],\n       [ 1.        , -0.18898224,  1.        ,  1.        ,  1.        ],\n       [ 1.        , -0.18898224,  1.        ,  1.        ,  1.        ],\n       [ 1.        , -0.18898224,  1.        ,  1.        ,  1.        ]])\n```\n\n## 相关距离\n\n$$\nD_{xy} = 1 - \\rho _{XY}\n$$\n\npython 实现（基于相关系数）\n同样针对矩阵a\n```\n# 行之间的相关距离\nones(shape(corrcoef(a)),int) - corrcoef(a)\n\n>>array([[ 0.        ,  0.02409993,  0.89517152],\n       [ 0.02409993,  0.        ,  0.82097128],\n       [ 0.89517152,  0.82097128,  0.        ]])\n       \n       \n# 列之间的相关距离\nones(shape(corrcoef(a,rowvar = 0)),int) - corrcoef(a,rowvar = 0)\n\n>>array([[ 0.        ,  1.18898224,  0.        ,  0.        ,  0.        ],\n       [ 1.18898224,  0.        ,  1.18898224,  1.18898224,  1.18898224],\n       [ 0.        ,  1.18898224,  0.        ,  0.        ,  0.        ],\n       [ 0.        ,  1.18898224,  0.        ,  0.        ,  0.        ],\n       [ 0.        ,  1.18898224,  0.        ,  0.        ,  0.        ]])\n\n```\n\n\n# 信息熵\n\n信息熵并不属于一种相似性度量，是衡量分布的混乱程度或分散程度的一种度量。分布越分散(或者说分布越平均)，信息熵就越大。分布越有序（或者说分布越集中），信息熵就越小。\n\n计算给定的样本集X的信息熵的公式：\n\n$$\nEntropy(X) = \\sum_{i=1}^{n} -p_{i} log_{2}p_{i} \n$$\n\n参数的含义：\n\nn：样本集X的分类数\n\npi：X中第i类元素出现的概率\n\n信息熵越大表明样本集S分类越分散，信息熵越小则表明样本集X分类越集中。。当S中n个分类出现的概率一样大时（都是1/n），信息熵取最大值log2(n)。当X只有一个分类时，信息熵取最小值0\n\npython进行计算和实现可参考：\nhttp://blog.csdn.net/autoliuweijie/article/details/52244246","source":"_posts/机器学习/MachingLearning中的距离和相似性计算以及python实现.md","raw":"---\ntitle: MachingLearning中的距离和相似性计算以及python实现\ndate: 2017-07-16 12:14:43\ntags: [距离计算]\ncategories: 技术篇\n---\n\n# 前言\n写这篇文章的目的不是说摘抄网上其他人的总结，刚才最近在看这方面的东西，为了让自己能够实际的去感受下每种求距离公式的差别，然后用python进行具体实现。\n<!--More-->\n在机器学习中，经常要用到距离和相似性的计算公式，我么要常计算个体之间的差异大小，继而评价个人之间的差异性和相似性，最常见的就是数据分析中的相关分析，数据挖掘中的分类和聚类算法。如利用k-means进行聚类时，判断个体所属的类别，要利用距离计算公式计算个体到簇心的距离，如利用KNN进行分类时，计算个体与已知类别之间的相似性，从而判断个体所属的类别等。\n\n\n文章编辑的过程中或许存在一个错误或者不合理的地方，欢迎指正。\n\n参考：http://www.cnblogs.com/heaad/archive/2011/03/08/1977733.html\n\n推荐：https://my.oschina.net/hunglish/blog/787596\n\n# 欧氏距离\n也称欧几里得距离，是指在m维空间中两个点之间的真实距离。欧式距离在ML中使用的范围比较广，也比较通用，就比如说利用k-Means对二维平面内的数据点进行聚类，对魔都房价的聚类分析（price/m^2 与平均房价）等。\n## 二维空间的欧氏距离 \n二维平面上两点a(x1,y1)与b(x2,y2)间的欧氏距离\n\n$$\nd12 =\\sqrt{(x_{1}-x_{2})^2+(y_{1}-y_{2})^2}\n$$\npython 实现为：\n\n```\n# coding: utf-8\n\nfrom numpy import *\n\ndef twoPointDistance(a,b):\n\td = sqrt( (a[0]-b[0])**2 + (a[1]-b[1])**2 )\n\treturn d\n\nprint 'a,b 二维距离为：',twoPointDistance((1,1),(2,2))\n```\n\n\n## 三维空间的欧氏距离\n三维空间两点a(x1,y1,z1)与b(x2,y2,z2)间的欧氏距离\n\n$$d12 =\\sqrt{(x_{1}-x_{2})^2+(y_{1}-y_{2})^2+(z_{1}-z_{2})^2}$$\npython 实现为：\n```\ndef threePointDistance(a,b):\n\td = sqrt( (a[0]-b[0])**2 + (a[1]-b[1])**2 + (a[2]-b[2])**2 )\n\treturn d\n\nprint 'a,b 三维距离为：',threePointDistance((1,1,1),(2,2,2))\n```\n\n## 多维空间的欧氏距离\n两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的欧氏距离\n\n$$ \n\\sqrt{\\sum_{n}^{k=1}(x_{1k}-x_{2k})^2 }\n$$\npython 实现为：\n\n```\ndef distance(a,b):\n\tsum = 0\n\tfor i in range(len(a)):\n\t\tsum += (a[i]-b[i])**2\n\treturn sqrt(sum)\n\nprint 'a,b 多维距离为：',distance((1,1,2,2),(2,2,4,4))\n```\n这里传入的参数可以是任意维的，该公式也适应上边的二维和三维\n\n# 标准欧氏距离\n标准化欧氏距离是针对简单欧氏距离的缺点而作的一种改进方案。标准欧氏距离的思路：既然数据各维分量的分布不一样，好吧！那我先将各个分量都“标准化”到均值、方差相等吧。均值和方差标准化到多少呢？这里先复习点统计学知识吧，假设样本集X的均值(mean)为m，标准差(standard deviation)为s，那么X的“标准化变量”表示为：\n\n　　而且标准化变量的数学期望为0，方差为1。因此样本集的标准化过程(standardization)用公式描述就是：\n　　\n$$\nX^* = \\frac{X-m}{s}\n$$\n标准化后的值 =  ( 标准化前的值  － 分量的均值 ) /分量的标准差\n\n经过简单的推导就可以得到两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的标准化欧氏距离的公式：\n\n$$\nd_{12} =\\sqrt {\\sum_{k=1}^{n} (\\frac{x_{1k}-x_{2k}}{s_{k}})^2}\n$$\n如果将方差的倒数看成是一个权重，这个公式可以看成是一种加权欧氏距离(Weighted Euclidean distance)。\n\npython 实现为\n```\ndef moreBZOSdis(a,b):\n    sumnum = 0\n    for i in range(len(a)):\n        # 计算si 分量标准差\n        avg = (a[i]-b[i])/2\n        si = sqrt( (a[i] - avg) ** 2 + (b[i] - avg) ** 2 )\n        sumnum += ((a[i]-b[i])/si ) ** 2\n\t\n    return sqrt(sumnum)\n\nprint 'a,b 标准欧式距离：',moreBZOSdis((1,2,1,2),(3,3,3,4))\n```\n\n# 曼哈顿距离\n又称为城市街区距离（City Block distance）, 想象你在曼哈顿要从一个十字路口开车到另外一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。实际驾驶距离就是这个“曼哈顿距离”。而这也是曼哈顿距离名称的来源。同样曼哈顿距离也分为二维，三维和多维。\n\n在计程车几何学中，一个圆是由从圆心向各个固定曼哈顿距离标示出来的点围成的区域，因此这种圆其实就是旋转了45度的正方形。如果有一群圆，且任两圆皆相交，则整群圆必在某点相交；因此曼哈顿距离会形成一个超凸度量空间。\n\n这里有一篇人脸表情分类的论文采用的曼哈顿距离进行计算的，[一种人脸表情分类的新方法——Manhattan距离](http://download.csdn.net/detail/gamer_gyt/9899825)\n\n## 二维曼哈顿距离\n二维平面两点a(x1,y1)与b(x2,y2)间的曼哈顿距离\n\n$$\nd12 =\\left | x_{1}-x_{2} \\right |  + \\left |y_{1}-y_{2}  \\right |\n$$\npython实现为\n```\ndef twoMHDdis(a,b):\n    return abs(a[0]-b[0])+abs(a[1]-b[1])\n\nprint 'a,b 二维曼哈顿距离为：', twoMHDdis((1,1),(2,2)) \n```\n\n\n## 三维曼哈顿距离\n三维平面两点a(x1,y1,z1)与b(x2,y2,z2)间的曼哈顿距离\n\n$$\nd12 =\\left | x_{1}-x_{2} \\right |  + \\left |y_{1}-y_{2}  \\right | + \\left |z_{1}-z_{2}  \\right |\n$$\npython实现为\n```\ndef threeMHDdis(a,b):\n\treturn abs(a[0]-b[0])+abs(a[1]-b[1]) + abs(a[2]-b[2])\n \nprint 'a,b 三维曼哈顿距离为：', threeMHDdis((1,1,1),(2,2,2)) \n```\n\n## 多维曼哈顿距离\n多维平面两点a(x1,y1)与b(x2,y2)间的曼哈顿距离\n$$\nd12 = \\sum_{k=1}^{n} \\left | x_{1k} - x_{2k} \\right |\n$$\npython实现为\n```\ndef moreMHDdis(a,b):\n    sum = 0 \n    for i in range(len(a)):\n        sum += abs(a[i]-b[i])\n    return sum\n\nprint 'a,b 多维曼哈顿距离为：', moreMHDdis((1,1,1,1),(2,2,2,2)) \n```\n由于维距离计算是比较灵活的，所以也同样适合二维和三维。\n\n# 切比雪夫距离\n切比雪夫距离（Chebyshev Distance）的定义为：max( | x2-x1 | , |y2-y1 | , ... ), 切比雪夫距离用的时候数据的维度必须是三个以上，这篇文章中[曼哈顿距离，欧式距离，明式距离，切比雪夫距离区别](http://blog.csdn.net/jerry81333/article/details/52632687) 给了一个很形象的解释如下：\n```\n比如，有同样两个人，在纽约准备到北京参拜天安门，同一个地点出发的话，按照欧式距离来计算，是完全一样的。\n\n但是按照切比雪夫距离，这是完全不同的概念了。\n\n譬如，其中一个人是土豪，另一个人是中产阶级，第一个人就能当晚直接头等舱走人，而第二个人可能就要等机票什么时候打折再去，或者选择坐船什么的。\n\n这样来看的话，距离是不是就不一样了呢？\n\n或者还是不清楚，我再说的详细点。\n\n同样是这两个人，欧式距离是直接算最短距离的，而切比雪夫距离可能还得加上财力，比如第一个人财富值100，第二个只有30，虽然物理距离一样，但是所包含的内容却是不同的。\n```\n\n## 二维切比雪夫距离\n\n二维平面两点a(x1,y1)与b(x2,y2)间的切比雪夫距离\n\n$$\nd_{12} = max( \\left | x_{1} - x_{2} \\right | , \\left | y_{1} - y_{2} \\right |)\n$$\npython 实现为\n\n```\ndef twoQBXFdis(a,b):\n    return max( abs(a[0]-b[0]), abs(a[1]-b[1]))\n\nprint 'a,b二维切比雪夫距离：' , twoQBXFdis((1,2),(3,4))\n```\n\n\n## 多维切比雪夫距离\n\n两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的切比雪夫距离\n$$\nd12 = max_{i\\epsilon n}( \\left | x_{1i} - x_{2i} \\right | )\n$$\n\npython 实现为\n```\ndef moreQBXFdis(a,b):\n    maxnum = 0\n    for i in range(len(a)):\n        if abs(a[i]-b[i]) > maxnum:\n            maxnum = abs(a[i]-b[i])\n    return maxnum\n\nprint 'a,b多维切比雪夫距离：' , moreQBXFdis((1,1,1,1),(3,4,3,4))\n```\n\n# 马氏距离\n有M个样本向量X1~Xm，协方差矩阵记为S，均值记为向量μ，则其中样本向量X到u的马氏距离表示为\n\n$$\nD(x) = \\sqrt{(X-\\mu )^TS^{-1}(X-\\mu)}\n$$\n而其中向量Xi与Xj之间的马氏距离定义为\n$$\nD(X_{i},X_{j}) = \\sqrt{(X_{i}-X_{j} )^TS^{-1}(X_{i}-X_{j} )}\n$$\n 若协方差矩阵是单位矩阵（各个样本向量之间独立同分布）,则公式就成了：\n$$\nD(X_{i},X_{j}) = \\sqrt{(X_{i}-X_{j} )^T(X_{i}-X_{j} )}\n$$\n也就是欧氏距离了。\n\n若协方差矩阵是对角矩阵，公式变成了标准化欧氏距离。\n\n马氏距离的优缺点：量纲无关，排除变量之间的相关性的干扰。\n\n\n# 夹角余弦\n几何中夹角余弦可用来衡量两个向量方向的差异，机器学习中借用这一概念来衡量样本向量之间的差异。\n\n## 二维空间向量的夹角余弦相似度\n在二维空间中向量A(x1,y1)与向量B(x2,y2)的夹角余弦公式：\n\n$$\n\\cos \\theta  = \\frac{x_{1}x_{2} + y_{1}y_{2}}{ \\sqrt{ x_{1}^2+x_{2}^2 }\\sqrt{ y_{1}^2+y_{2}^2 } }\n$$\npython 实现为\n```\ndef twoCos(a,b):\n    cos = (a[0]*b[0]+a[1]*b[1]) / (sqrt(a[0]**2 + b[0]**2) * sqrt(a[1]**2 + b[1]**2) )\n\n    return cos\nprint 'a,b 二维夹角余弦距离：',twoCos((1,1),(2,2))\n```\n\n\n## 多维空间向量的夹角余弦相似度\n两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)的夹角余弦\n\n类似的，对于两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)，可以使用类似于夹角余弦的概念来衡量它们间的相似程度。\n$$\n\\cos \\theta  = \\frac{a \\cdot  b}{\\left | a \\right | \\left | b \\right |}\n$$\n即：\n$$\n\\cos \\theta  = \\frac{ \\sum_{k=1}^{n} x_{1k}x_{2k} }{ \\sqrt{ \\sum_{k=1}^{n}x_{1k}^2 }\\sqrt{ \\sum_{k=1}^{n} x_{2k}^2 } }\n$$\npython实现为\n```\ndef moreCos(a,b):\n    sum_fenzi = 0.0\n    sum_fenmu_1,sum_fenmu_2 = 0,0\n    for i in range(len(a)):\n        sum_fenzi += a[i]*b[i]\n        sum_fenmu_1 += a[i]**2 \n        sum_fenmu_2 += b[i]**2 \n\n    return sum_fenzi/( sqrt(sum_fenmu_1) * sqrt(sum_fenmu_2) )\nprint 'a,b 多维夹角余弦距离：',moreCos((1,1,1,1),(2,2,2,2))\n```\n\n夹角余弦取值范围为[-1,1]。夹角余弦越大表示两个向量的夹角越小，夹角余弦越小表示两向量的夹角越大。当两个向量的方向重合时夹角余弦取最大值1，当两个向量的方向完全相反夹角余弦取最小值-1。\n\n# 闵可夫斯基距离\n\n闵氏距离不是一种距离，而是一组距离的定义\n## 定义\n两个n维变量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的闵可夫斯基距离定义为：\n$$\n\\sqrt[p]{ \\sum_{k=1}^{n} \\left | x_{1k}-x_{2k} \\right |^p} \n$$\n\n其中p是一个变参数。\n\n当p=1时，就是曼哈顿距离\n\n当p=2时，就是欧氏距离\n\n当p→∞时，就是切比雪夫距离\n\n根据变参数的不同，闵氏距离可以表示一类的距离。\n\n## 闵氏距离的缺点\n闵氏距离，包括曼哈顿距离、欧氏距离和切比雪夫距离都存在明显的缺点。\n\n举个例子：二维样本(身高,体重)，其中身高范围是150 ~ 190，体重范围是50 ~ 60，有三个样本：a(180,50)，b(190,50)，c(180,60)。那么a与b之间的闵氏距离（无论是曼哈顿距离、欧氏距离或切比雪夫距离）等于a与c之间的闵氏距离，但是身高的10cm真的等价于体重的10kg么？因此用闵氏距离来衡量这些样本间的相似度很有问题。\n\n简单说来，闵氏距离的缺点主要有两个：(1)将各个分量的量纲(scale)，也就是“单位”当作相同的看待了。(2)没有考虑各个分量的分布（期望，方差等)可能是不同的。\n\n\n# 汉明距离\n\n## 定义\n两个等长字符串s1与s2之间的汉明距离定义为将其中一个变为另外一个所需要作的最小替换次数。例如字符串“1111”与“1001”之间的汉明距离为2。\n\n应用：信息编码（为了增强容错性，应使得编码间的最小汉明距离尽可能大）。\n\n## python 实现\n```\ndef hanmingDis(a,b):\n    sumnum = 0\n    for i in range(len(a)):\n        if a[i]!=b[i]:\n            sumnum += 1\n    return sumnum\n\nprint 'a,b 汉明距离：',hanmingDis((1,1,2,3),(2,2,1,3))\n```\n\n# 杰卡德距离 & 杰卡德相似系数\n## 杰卡德距离\n与杰卡德相似系数相反的概念是杰卡德距离(Jaccard distance)。杰卡德距离可用如下公式表示：\n$$\nJ_{\\delta} (A,B) = \\frac{| A \\bigcup B | - | A \\bigcap B |}{| A \\bigcup B |}\n$$\n杰卡德距离用两个集合中不同元素占所有元素的比例来衡量两个集合的区分度。\n\npython 实现\n```\ndef jiekadeDis(a,b):\n    set_a = set(a)\n    set_b = set(b)\n    dis = float(len( (set_a | set_b) - (set_a & set_b) ) )/ len(set_a | set_b)\n    return dis\n\nprint 'a,b 杰卡德距离：', jiekadeDis((1,2,3),(2,3,4))\n```\n\n## 杰卡德相似系数\n两个集合A和B的交集元素在A，B的并集中所占的比例，称为两个集合的杰卡德相似系数，用符号J(A,B)表示。\n$$\nJ(A,B) = \\frac{| A \\bigcap B |}{| A \\bigcup B |}\n$$\n杰卡德相似系数是衡量两个集合的相似度一种指标。\n\npython 实现\n```\ndef jiekadeXSDis(a,b):\n    set_a = set(a)\n    set_b = set(b)\n    dis = float(len(set_a & set_b)  )/ len(set_a | set_b)\n    return dis\n\nprint 'a,b 杰卡德相似系数：', jiekadeXSDis((1,2,3),(2,3,4))\n```\n\n## 杰卡德相似系数与杰卡德距离的应用\n可将杰卡德相似系数用在衡量样本的相似度上。\n\n　　样本A与样本B是两个n维向量，而且所有维度的取值都是0或1。例如：A(0111)和B(1011)。我们将样本看成是一个集合，1表示集合包含该元素，0表示集合不包含该元素。\n\np ：样本A与B都是1的维度的个数\n\nq ：样本A是1，样本B是0的维度的个数\n\nr ：样本A是0，样本B是1的维度的个数\n\ns ：样本A与B都是0的维度的个数\n\n\n\n那么样本A与B的杰卡德相似系数可以表示为：\n\n这里p+q+r可理解为A与B的并集的元素个数，而p是A与B的交集的元素个数。\n\n而样本A与B的杰卡德距离表示为：\n$$\nJ= \\frac{p}{p+q+r}\n$$\n\n# 相关系数 & 相关距离\n## 相关系数\n$$\n\\rho_{XY} = \\frac{Cov(X,Y)}{\\sqrt{D(X)} \\sqrt{D(Y)}}=\\frac{ E( (X-EX) (Y-EY) ) }{ \\sqrt{D(X)} \\sqrt{D(Y)} }\n$$\n相关系数是衡量随机变量X与Y相关程度的一种方法，相关系数的取值范围是[-1,1]。相关系数的绝对值越大，则表明X与Y相关度越高。当X与Y线性相关时，相关系数取值为1（正线性相关）或-1（负线性相关）。\n\npython 实现\n相关系数可以利用numpy库中的corrcoef函数来计算\n例如 对于矩阵a,numpy.corrcoef(a)可计算行与行之间的相关系数，numpy.corrcoef(a,rowvar=0)用于计算各列之间的相关系数，输出为相关系数矩阵。\n```\nfrom numpy import  *\na = array([[1, 1, 2, 2, 3],  \n       [2, 2, 3, 3, 5],  \n       [1, 4, 2, 2, 3]]) \n\nprint corrcoef(a)\n\n>>array([[ 1.        ,  0.97590007,  0.10482848],\n       [ 0.97590007,  1.        ,  0.17902872],\n       [ 0.10482848,  0.17902872,  1.        ]])\n\nprint corrcoef(a,rowvar=0)\n\n>>array([[ 1.        , -0.18898224,  1.        ,  1.        ,  1.        ],\n       [-0.18898224,  1.        , -0.18898224, -0.18898224, -0.18898224],\n       [ 1.        , -0.18898224,  1.        ,  1.        ,  1.        ],\n       [ 1.        , -0.18898224,  1.        ,  1.        ,  1.        ],\n       [ 1.        , -0.18898224,  1.        ,  1.        ,  1.        ]])\n```\n\n## 相关距离\n\n$$\nD_{xy} = 1 - \\rho _{XY}\n$$\n\npython 实现（基于相关系数）\n同样针对矩阵a\n```\n# 行之间的相关距离\nones(shape(corrcoef(a)),int) - corrcoef(a)\n\n>>array([[ 0.        ,  0.02409993,  0.89517152],\n       [ 0.02409993,  0.        ,  0.82097128],\n       [ 0.89517152,  0.82097128,  0.        ]])\n       \n       \n# 列之间的相关距离\nones(shape(corrcoef(a,rowvar = 0)),int) - corrcoef(a,rowvar = 0)\n\n>>array([[ 0.        ,  1.18898224,  0.        ,  0.        ,  0.        ],\n       [ 1.18898224,  0.        ,  1.18898224,  1.18898224,  1.18898224],\n       [ 0.        ,  1.18898224,  0.        ,  0.        ,  0.        ],\n       [ 0.        ,  1.18898224,  0.        ,  0.        ,  0.        ],\n       [ 0.        ,  1.18898224,  0.        ,  0.        ,  0.        ]])\n\n```\n\n\n# 信息熵\n\n信息熵并不属于一种相似性度量，是衡量分布的混乱程度或分散程度的一种度量。分布越分散(或者说分布越平均)，信息熵就越大。分布越有序（或者说分布越集中），信息熵就越小。\n\n计算给定的样本集X的信息熵的公式：\n\n$$\nEntropy(X) = \\sum_{i=1}^{n} -p_{i} log_{2}p_{i} \n$$\n\n参数的含义：\n\nn：样本集X的分类数\n\npi：X中第i类元素出现的概率\n\n信息熵越大表明样本集S分类越分散，信息熵越小则表明样本集X分类越集中。。当S中n个分类出现的概率一样大时（都是1/n），信息熵取最大值log2(n)。当X只有一个分类时，信息熵取最小值0\n\npython进行计算和实现可参考：\nhttp://blog.csdn.net/autoliuweijie/article/details/52244246","slug":"机器学习/MachingLearning中的距离和相似性计算以及python实现","published":1,"updated":"2017-12-19T02:54:56.787Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjbd1r265000jkxuwh92ok2ta","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><p>写这篇文章的目的不是说摘抄网上其他人的总结，刚才最近在看这方面的东西，为了让自己能够实际的去感受下每种求距离公式的差别，然后用python进行具体实现。<br><a id=\"more\"></a><br>在机器学习中，经常要用到距离和相似性的计算公式，我么要常计算个体之间的差异大小，继而评价个人之间的差异性和相似性，最常见的就是数据分析中的相关分析，数据挖掘中的分类和聚类算法。如利用k-means进行聚类时，判断个体所属的类别，要利用距离计算公式计算个体到簇心的距离，如利用KNN进行分类时，计算个体与已知类别之间的相似性，从而判断个体所属的类别等。</p>\n<p>文章编辑的过程中或许存在一个错误或者不合理的地方，欢迎指正。</p>\n<p>参考：<a href=\"http://www.cnblogs.com/heaad/archive/2011/03/08/1977733.html\" target=\"_blank\" rel=\"external\">http://www.cnblogs.com/heaad/archive/2011/03/08/1977733.html</a></p>\n<p>推荐：<a href=\"https://my.oschina.net/hunglish/blog/787596\" target=\"_blank\" rel=\"external\">https://my.oschina.net/hunglish/blog/787596</a></p>\n<h1 id=\"欧氏距离\"><a href=\"#欧氏距离\" class=\"headerlink\" title=\"欧氏距离\"></a>欧氏距离</h1><p>也称欧几里得距离，是指在m维空间中两个点之间的真实距离。欧式距离在ML中使用的范围比较广，也比较通用，就比如说利用k-Means对二维平面内的数据点进行聚类，对魔都房价的聚类分析（price/m^2 与平均房价）等。</p>\n<h2 id=\"二维空间的欧氏距离\"><a href=\"#二维空间的欧氏距离\" class=\"headerlink\" title=\"二维空间的欧氏距离\"></a>二维空间的欧氏距离</h2><p>二维平面上两点a(x1,y1)与b(x2,y2)间的欧氏距离</p>\n<script type=\"math/tex; mode=display\">\nd12 =\\sqrt{(x_{1}-x_{2})^2+(y_{1}-y_{2})^2}</script><p>python 实现为：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># coding: utf-8</span><br><span class=\"line\"></span><br><span class=\"line\">from numpy import *</span><br><span class=\"line\"></span><br><span class=\"line\">def twoPointDistance(a,b):</span><br><span class=\"line\">\td = sqrt( (a[0]-b[0])**2 + (a[1]-b[1])**2 )</span><br><span class=\"line\">\treturn d</span><br><span class=\"line\"></span><br><span class=\"line\">print &apos;a,b 二维距离为：&apos;,twoPointDistance((1,1),(2,2))</span><br></pre></td></tr></table></figure>\n<h2 id=\"三维空间的欧氏距离\"><a href=\"#三维空间的欧氏距离\" class=\"headerlink\" title=\"三维空间的欧氏距离\"></a>三维空间的欧氏距离</h2><p>三维空间两点a(x1,y1,z1)与b(x2,y2,z2)间的欧氏距离</p>\n<script type=\"math/tex; mode=display\">d12 =\\sqrt{(x_{1}-x_{2})^2+(y_{1}-y_{2})^2+(z_{1}-z_{2})^2}</script><p>python 实现为：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def threePointDistance(a,b):</span><br><span class=\"line\">\td = sqrt( (a[0]-b[0])**2 + (a[1]-b[1])**2 + (a[2]-b[2])**2 )</span><br><span class=\"line\">\treturn d</span><br><span class=\"line\"></span><br><span class=\"line\">print &apos;a,b 三维距离为：&apos;,threePointDistance((1,1,1),(2,2,2))</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"多维空间的欧氏距离\"><a href=\"#多维空间的欧氏距离\" class=\"headerlink\" title=\"多维空间的欧氏距离\"></a>多维空间的欧氏距离</h2><p>两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的欧氏距离</p>\n<script type=\"math/tex; mode=display\">\n\\sqrt{\\sum_{n}^{k=1}(x_{1k}-x_{2k})^2 }</script><p>python 实现为：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def distance(a,b):</span><br><span class=\"line\">\tsum = 0</span><br><span class=\"line\">\tfor i in range(len(a)):</span><br><span class=\"line\">\t\tsum += (a[i]-b[i])**2</span><br><span class=\"line\">\treturn sqrt(sum)</span><br><span class=\"line\"></span><br><span class=\"line\">print &apos;a,b 多维距离为：&apos;,distance((1,1,2,2),(2,2,4,4))</span><br></pre></td></tr></table></figure>\n<p>这里传入的参数可以是任意维的，该公式也适应上边的二维和三维</p>\n<h1 id=\"标准欧氏距离\"><a href=\"#标准欧氏距离\" class=\"headerlink\" title=\"标准欧氏距离\"></a>标准欧氏距离</h1><p>标准化欧氏距离是针对简单欧氏距离的缺点而作的一种改进方案。标准欧氏距离的思路：既然数据各维分量的分布不一样，好吧！那我先将各个分量都“标准化”到均值、方差相等吧。均值和方差标准化到多少呢？这里先复习点统计学知识吧，假设样本集X的均值(mean)为m，标准差(standard deviation)为s，那么X的“标准化变量”表示为：</p>\n<p>　　而且标准化变量的数学期望为0，方差为1。因此样本集的标准化过程(standardization)用公式描述就是：\n　　</p>\n<script type=\"math/tex; mode=display\">\nX^* = \\frac{X-m}{s}</script><p>标准化后的值 =  ( 标准化前的值  － 分量的均值 ) /分量的标准差</p>\n<p>经过简单的推导就可以得到两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的标准化欧氏距离的公式：</p>\n<script type=\"math/tex; mode=display\">\nd_{12} =\\sqrt {\\sum_{k=1}^{n} (\\frac{x_{1k}-x_{2k}}{s_{k}})^2}</script><p>如果将方差的倒数看成是一个权重，这个公式可以看成是一种加权欧氏距离(Weighted Euclidean distance)。</p>\n<p>python 实现为<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def moreBZOSdis(a,b):</span><br><span class=\"line\">    sumnum = 0</span><br><span class=\"line\">    for i in range(len(a)):</span><br><span class=\"line\">        # 计算si 分量标准差</span><br><span class=\"line\">        avg = (a[i]-b[i])/2</span><br><span class=\"line\">        si = sqrt( (a[i] - avg) ** 2 + (b[i] - avg) ** 2 )</span><br><span class=\"line\">        sumnum += ((a[i]-b[i])/si ) ** 2</span><br><span class=\"line\">\t</span><br><span class=\"line\">    return sqrt(sumnum)</span><br><span class=\"line\"></span><br><span class=\"line\">print &apos;a,b 标准欧式距离：&apos;,moreBZOSdis((1,2,1,2),(3,3,3,4))</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"曼哈顿距离\"><a href=\"#曼哈顿距离\" class=\"headerlink\" title=\"曼哈顿距离\"></a>曼哈顿距离</h1><p>又称为城市街区距离（City Block distance）, 想象你在曼哈顿要从一个十字路口开车到另外一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。实际驾驶距离就是这个“曼哈顿距离”。而这也是曼哈顿距离名称的来源。同样曼哈顿距离也分为二维，三维和多维。</p>\n<p>在计程车几何学中，一个圆是由从圆心向各个固定曼哈顿距离标示出来的点围成的区域，因此这种圆其实就是旋转了45度的正方形。如果有一群圆，且任两圆皆相交，则整群圆必在某点相交；因此曼哈顿距离会形成一个超凸度量空间。</p>\n<p>这里有一篇人脸表情分类的论文采用的曼哈顿距离进行计算的，<a href=\"http://download.csdn.net/detail/gamer_gyt/9899825\" target=\"_blank\" rel=\"external\">一种人脸表情分类的新方法——Manhattan距离</a></p>\n<h2 id=\"二维曼哈顿距离\"><a href=\"#二维曼哈顿距离\" class=\"headerlink\" title=\"二维曼哈顿距离\"></a>二维曼哈顿距离</h2><p>二维平面两点a(x1,y1)与b(x2,y2)间的曼哈顿距离</p>\n<script type=\"math/tex; mode=display\">\nd12 =\\left | x_{1}-x_{2} \\right |  + \\left |y_{1}-y_{2}  \\right |</script><p>python实现为<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def twoMHDdis(a,b):</span><br><span class=\"line\">    return abs(a[0]-b[0])+abs(a[1]-b[1])</span><br><span class=\"line\"></span><br><span class=\"line\">print &apos;a,b 二维曼哈顿距离为：&apos;, twoMHDdis((1,1),(2,2))</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"三维曼哈顿距离\"><a href=\"#三维曼哈顿距离\" class=\"headerlink\" title=\"三维曼哈顿距离\"></a>三维曼哈顿距离</h2><p>三维平面两点a(x1,y1,z1)与b(x2,y2,z2)间的曼哈顿距离</p>\n<script type=\"math/tex; mode=display\">\nd12 =\\left | x_{1}-x_{2} \\right |  + \\left |y_{1}-y_{2}  \\right | + \\left |z_{1}-z_{2}  \\right |</script><p>python实现为<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def threeMHDdis(a,b):</span><br><span class=\"line\">\treturn abs(a[0]-b[0])+abs(a[1]-b[1]) + abs(a[2]-b[2])</span><br><span class=\"line\"> </span><br><span class=\"line\">print &apos;a,b 三维曼哈顿距离为：&apos;, threeMHDdis((1,1,1),(2,2,2))</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"多维曼哈顿距离\"><a href=\"#多维曼哈顿距离\" class=\"headerlink\" title=\"多维曼哈顿距离\"></a>多维曼哈顿距离</h2><p>多维平面两点a(x1,y1)与b(x2,y2)间的曼哈顿距离</p>\n<script type=\"math/tex; mode=display\">\nd12 = \\sum_{k=1}^{n} \\left | x_{1k} - x_{2k} \\right |</script><p>python实现为<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def moreMHDdis(a,b):</span><br><span class=\"line\">    sum = 0 </span><br><span class=\"line\">    for i in range(len(a)):</span><br><span class=\"line\">        sum += abs(a[i]-b[i])</span><br><span class=\"line\">    return sum</span><br><span class=\"line\"></span><br><span class=\"line\">print &apos;a,b 多维曼哈顿距离为：&apos;, moreMHDdis((1,1,1,1),(2,2,2,2))</span><br></pre></td></tr></table></figure></p>\n<p>由于维距离计算是比较灵活的，所以也同样适合二维和三维。</p>\n<h1 id=\"切比雪夫距离\"><a href=\"#切比雪夫距离\" class=\"headerlink\" title=\"切比雪夫距离\"></a>切比雪夫距离</h1><p>切比雪夫距离（Chebyshev Distance）的定义为：max( | x2-x1 | , |y2-y1 | , … ), 切比雪夫距离用的时候数据的维度必须是三个以上，这篇文章中<a href=\"http://blog.csdn.net/jerry81333/article/details/52632687\" target=\"_blank\" rel=\"external\">曼哈顿距离，欧式距离，明式距离，切比雪夫距离区别</a> 给了一个很形象的解释如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">比如，有同样两个人，在纽约准备到北京参拜天安门，同一个地点出发的话，按照欧式距离来计算，是完全一样的。</span><br><span class=\"line\"></span><br><span class=\"line\">但是按照切比雪夫距离，这是完全不同的概念了。</span><br><span class=\"line\"></span><br><span class=\"line\">譬如，其中一个人是土豪，另一个人是中产阶级，第一个人就能当晚直接头等舱走人，而第二个人可能就要等机票什么时候打折再去，或者选择坐船什么的。</span><br><span class=\"line\"></span><br><span class=\"line\">这样来看的话，距离是不是就不一样了呢？</span><br><span class=\"line\"></span><br><span class=\"line\">或者还是不清楚，我再说的详细点。</span><br><span class=\"line\"></span><br><span class=\"line\">同样是这两个人，欧式距离是直接算最短距离的，而切比雪夫距离可能还得加上财力，比如第一个人财富值100，第二个只有30，虽然物理距离一样，但是所包含的内容却是不同的。</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"二维切比雪夫距离\"><a href=\"#二维切比雪夫距离\" class=\"headerlink\" title=\"二维切比雪夫距离\"></a>二维切比雪夫距离</h2><p>二维平面两点a(x1,y1)与b(x2,y2)间的切比雪夫距离</p>\n<script type=\"math/tex; mode=display\">\nd_{12} = max( \\left | x_{1} - x_{2} \\right | , \\left | y_{1} - y_{2} \\right |)</script><p>python 实现为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def twoQBXFdis(a,b):</span><br><span class=\"line\">    return max( abs(a[0]-b[0]), abs(a[1]-b[1]))</span><br><span class=\"line\"></span><br><span class=\"line\">print &apos;a,b二维切比雪夫距离：&apos; , twoQBXFdis((1,2),(3,4))</span><br></pre></td></tr></table></figure>\n<h2 id=\"多维切比雪夫距离\"><a href=\"#多维切比雪夫距离\" class=\"headerlink\" title=\"多维切比雪夫距离\"></a>多维切比雪夫距离</h2><p>两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的切比雪夫距离</p>\n<script type=\"math/tex; mode=display\">\nd12 = max_{i\\epsilon n}( \\left | x_{1i} - x_{2i} \\right | )</script><p>python 实现为<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def moreQBXFdis(a,b):</span><br><span class=\"line\">    maxnum = 0</span><br><span class=\"line\">    for i in range(len(a)):</span><br><span class=\"line\">        if abs(a[i]-b[i]) &gt; maxnum:</span><br><span class=\"line\">            maxnum = abs(a[i]-b[i])</span><br><span class=\"line\">    return maxnum</span><br><span class=\"line\"></span><br><span class=\"line\">print &apos;a,b多维切比雪夫距离：&apos; , moreQBXFdis((1,1,1,1),(3,4,3,4))</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"马氏距离\"><a href=\"#马氏距离\" class=\"headerlink\" title=\"马氏距离\"></a>马氏距离</h1><p>有M个样本向量X1~Xm，协方差矩阵记为S，均值记为向量μ，则其中样本向量X到u的马氏距离表示为</p>\n<script type=\"math/tex; mode=display\">\nD(x) = \\sqrt{(X-\\mu )^TS^{-1}(X-\\mu)}</script><p>而其中向量Xi与Xj之间的马氏距离定义为</p>\n<script type=\"math/tex; mode=display\">\nD(X_{i},X_{j}) = \\sqrt{(X_{i}-X_{j} )^TS^{-1}(X_{i}-X_{j} )}</script><p> 若协方差矩阵是单位矩阵（各个样本向量之间独立同分布）,则公式就成了：</p>\n<script type=\"math/tex; mode=display\">\nD(X_{i},X_{j}) = \\sqrt{(X_{i}-X_{j} )^T(X_{i}-X_{j} )}</script><p>也就是欧氏距离了。</p>\n<p>若协方差矩阵是对角矩阵，公式变成了标准化欧氏距离。</p>\n<p>马氏距离的优缺点：量纲无关，排除变量之间的相关性的干扰。</p>\n<h1 id=\"夹角余弦\"><a href=\"#夹角余弦\" class=\"headerlink\" title=\"夹角余弦\"></a>夹角余弦</h1><p>几何中夹角余弦可用来衡量两个向量方向的差异，机器学习中借用这一概念来衡量样本向量之间的差异。</p>\n<h2 id=\"二维空间向量的夹角余弦相似度\"><a href=\"#二维空间向量的夹角余弦相似度\" class=\"headerlink\" title=\"二维空间向量的夹角余弦相似度\"></a>二维空间向量的夹角余弦相似度</h2><p>在二维空间中向量A(x1,y1)与向量B(x2,y2)的夹角余弦公式：</p>\n<script type=\"math/tex; mode=display\">\n\\cos \\theta  = \\frac{x_{1}x_{2} + y_{1}y_{2}}{ \\sqrt{ x_{1}^2+x_{2}^2 }\\sqrt{ y_{1}^2+y_{2}^2 } }</script><p>python 实现为<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def twoCos(a,b):</span><br><span class=\"line\">    cos = (a[0]*b[0]+a[1]*b[1]) / (sqrt(a[0]**2 + b[0]**2) * sqrt(a[1]**2 + b[1]**2) )</span><br><span class=\"line\"></span><br><span class=\"line\">    return cos</span><br><span class=\"line\">print &apos;a,b 二维夹角余弦距离：&apos;,twoCos((1,1),(2,2))</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"多维空间向量的夹角余弦相似度\"><a href=\"#多维空间向量的夹角余弦相似度\" class=\"headerlink\" title=\"多维空间向量的夹角余弦相似度\"></a>多维空间向量的夹角余弦相似度</h2><p>两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)的夹角余弦</p>\n<p>类似的，对于两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)，可以使用类似于夹角余弦的概念来衡量它们间的相似程度。</p>\n<script type=\"math/tex; mode=display\">\n\\cos \\theta  = \\frac{a \\cdot  b}{\\left | a \\right | \\left | b \\right |}</script><p>即：</p>\n<script type=\"math/tex; mode=display\">\n\\cos \\theta  = \\frac{ \\sum_{k=1}^{n} x_{1k}x_{2k} }{ \\sqrt{ \\sum_{k=1}^{n}x_{1k}^2 }\\sqrt{ \\sum_{k=1}^{n} x_{2k}^2 } }</script><p>python实现为<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def moreCos(a,b):</span><br><span class=\"line\">    sum_fenzi = 0.0</span><br><span class=\"line\">    sum_fenmu_1,sum_fenmu_2 = 0,0</span><br><span class=\"line\">    for i in range(len(a)):</span><br><span class=\"line\">        sum_fenzi += a[i]*b[i]</span><br><span class=\"line\">        sum_fenmu_1 += a[i]**2 </span><br><span class=\"line\">        sum_fenmu_2 += b[i]**2 </span><br><span class=\"line\"></span><br><span class=\"line\">    return sum_fenzi/( sqrt(sum_fenmu_1) * sqrt(sum_fenmu_2) )</span><br><span class=\"line\">print &apos;a,b 多维夹角余弦距离：&apos;,moreCos((1,1,1,1),(2,2,2,2))</span><br></pre></td></tr></table></figure></p>\n<p>夹角余弦取值范围为[-1,1]。夹角余弦越大表示两个向量的夹角越小，夹角余弦越小表示两向量的夹角越大。当两个向量的方向重合时夹角余弦取最大值1，当两个向量的方向完全相反夹角余弦取最小值-1。</p>\n<h1 id=\"闵可夫斯基距离\"><a href=\"#闵可夫斯基距离\" class=\"headerlink\" title=\"闵可夫斯基距离\"></a>闵可夫斯基距离</h1><p>闵氏距离不是一种距离，而是一组距离的定义</p>\n<h2 id=\"定义\"><a href=\"#定义\" class=\"headerlink\" title=\"定义\"></a>定义</h2><p>两个n维变量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的闵可夫斯基距离定义为：</p>\n<script type=\"math/tex; mode=display\">\n\\sqrt[p]{ \\sum_{k=1}^{n} \\left | x_{1k}-x_{2k} \\right |^p}</script><p>其中p是一个变参数。</p>\n<p>当p=1时，就是曼哈顿距离</p>\n<p>当p=2时，就是欧氏距离</p>\n<p>当p→∞时，就是切比雪夫距离</p>\n<p>根据变参数的不同，闵氏距离可以表示一类的距离。</p>\n<h2 id=\"闵氏距离的缺点\"><a href=\"#闵氏距离的缺点\" class=\"headerlink\" title=\"闵氏距离的缺点\"></a>闵氏距离的缺点</h2><p>闵氏距离，包括曼哈顿距离、欧氏距离和切比雪夫距离都存在明显的缺点。</p>\n<p>举个例子：二维样本(身高,体重)，其中身高范围是150 ~ 190，体重范围是50 ~ 60，有三个样本：a(180,50)，b(190,50)，c(180,60)。那么a与b之间的闵氏距离（无论是曼哈顿距离、欧氏距离或切比雪夫距离）等于a与c之间的闵氏距离，但是身高的10cm真的等价于体重的10kg么？因此用闵氏距离来衡量这些样本间的相似度很有问题。</p>\n<p>简单说来，闵氏距离的缺点主要有两个：(1)将各个分量的量纲(scale)，也就是“单位”当作相同的看待了。(2)没有考虑各个分量的分布（期望，方差等)可能是不同的。</p>\n<h1 id=\"汉明距离\"><a href=\"#汉明距离\" class=\"headerlink\" title=\"汉明距离\"></a>汉明距离</h1><h2 id=\"定义-1\"><a href=\"#定义-1\" class=\"headerlink\" title=\"定义\"></a>定义</h2><p>两个等长字符串s1与s2之间的汉明距离定义为将其中一个变为另外一个所需要作的最小替换次数。例如字符串“1111”与“1001”之间的汉明距离为2。</p>\n<p>应用：信息编码（为了增强容错性，应使得编码间的最小汉明距离尽可能大）。</p>\n<h2 id=\"python-实现\"><a href=\"#python-实现\" class=\"headerlink\" title=\"python 实现\"></a>python 实现</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def hanmingDis(a,b):</span><br><span class=\"line\">    sumnum = 0</span><br><span class=\"line\">    for i in range(len(a)):</span><br><span class=\"line\">        if a[i]!=b[i]:</span><br><span class=\"line\">            sumnum += 1</span><br><span class=\"line\">    return sumnum</span><br><span class=\"line\"></span><br><span class=\"line\">print &apos;a,b 汉明距离：&apos;,hanmingDis((1,1,2,3),(2,2,1,3))</span><br></pre></td></tr></table></figure>\n<h1 id=\"杰卡德距离-amp-杰卡德相似系数\"><a href=\"#杰卡德距离-amp-杰卡德相似系数\" class=\"headerlink\" title=\"杰卡德距离 &amp; 杰卡德相似系数\"></a>杰卡德距离 &amp; 杰卡德相似系数</h1><h2 id=\"杰卡德距离\"><a href=\"#杰卡德距离\" class=\"headerlink\" title=\"杰卡德距离\"></a>杰卡德距离</h2><p>与杰卡德相似系数相反的概念是杰卡德距离(Jaccard distance)。杰卡德距离可用如下公式表示：</p>\n<script type=\"math/tex; mode=display\">\nJ_{\\delta} (A,B) = \\frac{| A \\bigcup B | - | A \\bigcap B |}{| A \\bigcup B |}</script><p>杰卡德距离用两个集合中不同元素占所有元素的比例来衡量两个集合的区分度。</p>\n<p>python 实现<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def jiekadeDis(a,b):</span><br><span class=\"line\">    set_a = set(a)</span><br><span class=\"line\">    set_b = set(b)</span><br><span class=\"line\">    dis = float(len( (set_a | set_b) - (set_a &amp; set_b) ) )/ len(set_a | set_b)</span><br><span class=\"line\">    return dis</span><br><span class=\"line\"></span><br><span class=\"line\">print &apos;a,b 杰卡德距离：&apos;, jiekadeDis((1,2,3),(2,3,4))</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"杰卡德相似系数\"><a href=\"#杰卡德相似系数\" class=\"headerlink\" title=\"杰卡德相似系数\"></a>杰卡德相似系数</h2><p>两个集合A和B的交集元素在A，B的并集中所占的比例，称为两个集合的杰卡德相似系数，用符号J(A,B)表示。</p>\n<script type=\"math/tex; mode=display\">\nJ(A,B) = \\frac{| A \\bigcap B |}{| A \\bigcup B |}</script><p>杰卡德相似系数是衡量两个集合的相似度一种指标。</p>\n<p>python 实现<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def jiekadeXSDis(a,b):</span><br><span class=\"line\">    set_a = set(a)</span><br><span class=\"line\">    set_b = set(b)</span><br><span class=\"line\">    dis = float(len(set_a &amp; set_b)  )/ len(set_a | set_b)</span><br><span class=\"line\">    return dis</span><br><span class=\"line\"></span><br><span class=\"line\">print &apos;a,b 杰卡德相似系数：&apos;, jiekadeXSDis((1,2,3),(2,3,4))</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"杰卡德相似系数与杰卡德距离的应用\"><a href=\"#杰卡德相似系数与杰卡德距离的应用\" class=\"headerlink\" title=\"杰卡德相似系数与杰卡德距离的应用\"></a>杰卡德相似系数与杰卡德距离的应用</h2><p>可将杰卡德相似系数用在衡量样本的相似度上。</p>\n<p>　　样本A与样本B是两个n维向量，而且所有维度的取值都是0或1。例如：A(0111)和B(1011)。我们将样本看成是一个集合，1表示集合包含该元素，0表示集合不包含该元素。</p>\n<p>p ：样本A与B都是1的维度的个数</p>\n<p>q ：样本A是1，样本B是0的维度的个数</p>\n<p>r ：样本A是0，样本B是1的维度的个数</p>\n<p>s ：样本A与B都是0的维度的个数</p>\n<p>那么样本A与B的杰卡德相似系数可以表示为：</p>\n<p>这里p+q+r可理解为A与B的并集的元素个数，而p是A与B的交集的元素个数。</p>\n<p>而样本A与B的杰卡德距离表示为：</p>\n<script type=\"math/tex; mode=display\">\nJ= \\frac{p}{p+q+r}</script><h1 id=\"相关系数-amp-相关距离\"><a href=\"#相关系数-amp-相关距离\" class=\"headerlink\" title=\"相关系数 &amp; 相关距离\"></a>相关系数 &amp; 相关距离</h1><h2 id=\"相关系数\"><a href=\"#相关系数\" class=\"headerlink\" title=\"相关系数\"></a>相关系数</h2><script type=\"math/tex; mode=display\">\n\\rho_{XY} = \\frac{Cov(X,Y)}{\\sqrt{D(X)} \\sqrt{D(Y)}}=\\frac{ E( (X-EX) (Y-EY) ) }{ \\sqrt{D(X)} \\sqrt{D(Y)} }</script><p>相关系数是衡量随机变量X与Y相关程度的一种方法，相关系数的取值范围是[-1,1]。相关系数的绝对值越大，则表明X与Y相关度越高。当X与Y线性相关时，相关系数取值为1（正线性相关）或-1（负线性相关）。</p>\n<p>python 实现<br>相关系数可以利用numpy库中的corrcoef函数来计算<br>例如 对于矩阵a,numpy.corrcoef(a)可计算行与行之间的相关系数，numpy.corrcoef(a,rowvar=0)用于计算各列之间的相关系数，输出为相关系数矩阵。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from numpy import  *</span><br><span class=\"line\">a = array([[1, 1, 2, 2, 3],  </span><br><span class=\"line\">       [2, 2, 3, 3, 5],  </span><br><span class=\"line\">       [1, 4, 2, 2, 3]]) </span><br><span class=\"line\"></span><br><span class=\"line\">print corrcoef(a)</span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;array([[ 1.        ,  0.97590007,  0.10482848],</span><br><span class=\"line\">       [ 0.97590007,  1.        ,  0.17902872],</span><br><span class=\"line\">       [ 0.10482848,  0.17902872,  1.        ]])</span><br><span class=\"line\"></span><br><span class=\"line\">print corrcoef(a,rowvar=0)</span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;array([[ 1.        , -0.18898224,  1.        ,  1.        ,  1.        ],</span><br><span class=\"line\">       [-0.18898224,  1.        , -0.18898224, -0.18898224, -0.18898224],</span><br><span class=\"line\">       [ 1.        , -0.18898224,  1.        ,  1.        ,  1.        ],</span><br><span class=\"line\">       [ 1.        , -0.18898224,  1.        ,  1.        ,  1.        ],</span><br><span class=\"line\">       [ 1.        , -0.18898224,  1.        ,  1.        ,  1.        ]])</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"相关距离\"><a href=\"#相关距离\" class=\"headerlink\" title=\"相关距离\"></a>相关距离</h2><script type=\"math/tex; mode=display\">\nD_{xy} = 1 - \\rho _{XY}</script><p>python 实现（基于相关系数）<br>同样针对矩阵a<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 行之间的相关距离</span><br><span class=\"line\">ones(shape(corrcoef(a)),int) - corrcoef(a)</span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;array([[ 0.        ,  0.02409993,  0.89517152],</span><br><span class=\"line\">       [ 0.02409993,  0.        ,  0.82097128],</span><br><span class=\"line\">       [ 0.89517152,  0.82097128,  0.        ]])</span><br><span class=\"line\">       </span><br><span class=\"line\">       </span><br><span class=\"line\"># 列之间的相关距离</span><br><span class=\"line\">ones(shape(corrcoef(a,rowvar = 0)),int) - corrcoef(a,rowvar = 0)</span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;array([[ 0.        ,  1.18898224,  0.        ,  0.        ,  0.        ],</span><br><span class=\"line\">       [ 1.18898224,  0.        ,  1.18898224,  1.18898224,  1.18898224],</span><br><span class=\"line\">       [ 0.        ,  1.18898224,  0.        ,  0.        ,  0.        ],</span><br><span class=\"line\">       [ 0.        ,  1.18898224,  0.        ,  0.        ,  0.        ],</span><br><span class=\"line\">       [ 0.        ,  1.18898224,  0.        ,  0.        ,  0.        ]])</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"信息熵\"><a href=\"#信息熵\" class=\"headerlink\" title=\"信息熵\"></a>信息熵</h1><p>信息熵并不属于一种相似性度量，是衡量分布的混乱程度或分散程度的一种度量。分布越分散(或者说分布越平均)，信息熵就越大。分布越有序（或者说分布越集中），信息熵就越小。</p>\n<p>计算给定的样本集X的信息熵的公式：</p>\n<script type=\"math/tex; mode=display\">\nEntropy(X) = \\sum_{i=1}^{n} -p_{i} log_{2}p_{i}</script><p>参数的含义：</p>\n<p>n：样本集X的分类数</p>\n<p>pi：X中第i类元素出现的概率</p>\n<p>信息熵越大表明样本集S分类越分散，信息熵越小则表明样本集X分类越集中。。当S中n个分类出现的概率一样大时（都是1/n），信息熵取最大值log2(n)。当X只有一个分类时，信息熵取最小值0</p>\n<p>python进行计算和实现可参考：<br><a href=\"http://blog.csdn.net/autoliuweijie/article/details/52244246\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/autoliuweijie/article/details/52244246</a></p>\n","site":{"data":{}},"excerpt":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><p>写这篇文章的目的不是说摘抄网上其他人的总结，刚才最近在看这方面的东西，为了让自己能够实际的去感受下每种求距离公式的差别，然后用python进行具体实现。<br>","more":"<br>在机器学习中，经常要用到距离和相似性的计算公式，我么要常计算个体之间的差异大小，继而评价个人之间的差异性和相似性，最常见的就是数据分析中的相关分析，数据挖掘中的分类和聚类算法。如利用k-means进行聚类时，判断个体所属的类别，要利用距离计算公式计算个体到簇心的距离，如利用KNN进行分类时，计算个体与已知类别之间的相似性，从而判断个体所属的类别等。</p>\n<p>文章编辑的过程中或许存在一个错误或者不合理的地方，欢迎指正。</p>\n<p>参考：<a href=\"http://www.cnblogs.com/heaad/archive/2011/03/08/1977733.html\" target=\"_blank\" rel=\"external\">http://www.cnblogs.com/heaad/archive/2011/03/08/1977733.html</a></p>\n<p>推荐：<a href=\"https://my.oschina.net/hunglish/blog/787596\" target=\"_blank\" rel=\"external\">https://my.oschina.net/hunglish/blog/787596</a></p>\n<h1 id=\"欧氏距离\"><a href=\"#欧氏距离\" class=\"headerlink\" title=\"欧氏距离\"></a>欧氏距离</h1><p>也称欧几里得距离，是指在m维空间中两个点之间的真实距离。欧式距离在ML中使用的范围比较广，也比较通用，就比如说利用k-Means对二维平面内的数据点进行聚类，对魔都房价的聚类分析（price/m^2 与平均房价）等。</p>\n<h2 id=\"二维空间的欧氏距离\"><a href=\"#二维空间的欧氏距离\" class=\"headerlink\" title=\"二维空间的欧氏距离\"></a>二维空间的欧氏距离</h2><p>二维平面上两点a(x1,y1)与b(x2,y2)间的欧氏距离</p>\n<script type=\"math/tex; mode=display\">\nd12 =\\sqrt{(x_{1}-x_{2})^2+(y_{1}-y_{2})^2}</script><p>python 实现为：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># coding: utf-8</span><br><span class=\"line\"></span><br><span class=\"line\">from numpy import *</span><br><span class=\"line\"></span><br><span class=\"line\">def twoPointDistance(a,b):</span><br><span class=\"line\">\td = sqrt( (a[0]-b[0])**2 + (a[1]-b[1])**2 )</span><br><span class=\"line\">\treturn d</span><br><span class=\"line\"></span><br><span class=\"line\">print &apos;a,b 二维距离为：&apos;,twoPointDistance((1,1),(2,2))</span><br></pre></td></tr></table></figure>\n<h2 id=\"三维空间的欧氏距离\"><a href=\"#三维空间的欧氏距离\" class=\"headerlink\" title=\"三维空间的欧氏距离\"></a>三维空间的欧氏距离</h2><p>三维空间两点a(x1,y1,z1)与b(x2,y2,z2)间的欧氏距离</p>\n<script type=\"math/tex; mode=display\">d12 =\\sqrt{(x_{1}-x_{2})^2+(y_{1}-y_{2})^2+(z_{1}-z_{2})^2}</script><p>python 实现为：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def threePointDistance(a,b):</span><br><span class=\"line\">\td = sqrt( (a[0]-b[0])**2 + (a[1]-b[1])**2 + (a[2]-b[2])**2 )</span><br><span class=\"line\">\treturn d</span><br><span class=\"line\"></span><br><span class=\"line\">print &apos;a,b 三维距离为：&apos;,threePointDistance((1,1,1),(2,2,2))</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"多维空间的欧氏距离\"><a href=\"#多维空间的欧氏距离\" class=\"headerlink\" title=\"多维空间的欧氏距离\"></a>多维空间的欧氏距离</h2><p>两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的欧氏距离</p>\n<script type=\"math/tex; mode=display\">\n\\sqrt{\\sum_{n}^{k=1}(x_{1k}-x_{2k})^2 }</script><p>python 实现为：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def distance(a,b):</span><br><span class=\"line\">\tsum = 0</span><br><span class=\"line\">\tfor i in range(len(a)):</span><br><span class=\"line\">\t\tsum += (a[i]-b[i])**2</span><br><span class=\"line\">\treturn sqrt(sum)</span><br><span class=\"line\"></span><br><span class=\"line\">print &apos;a,b 多维距离为：&apos;,distance((1,1,2,2),(2,2,4,4))</span><br></pre></td></tr></table></figure>\n<p>这里传入的参数可以是任意维的，该公式也适应上边的二维和三维</p>\n<h1 id=\"标准欧氏距离\"><a href=\"#标准欧氏距离\" class=\"headerlink\" title=\"标准欧氏距离\"></a>标准欧氏距离</h1><p>标准化欧氏距离是针对简单欧氏距离的缺点而作的一种改进方案。标准欧氏距离的思路：既然数据各维分量的分布不一样，好吧！那我先将各个分量都“标准化”到均值、方差相等吧。均值和方差标准化到多少呢？这里先复习点统计学知识吧，假设样本集X的均值(mean)为m，标准差(standard deviation)为s，那么X的“标准化变量”表示为：</p>\n<p>　　而且标准化变量的数学期望为0，方差为1。因此样本集的标准化过程(standardization)用公式描述就是：\n　　</p>\n<script type=\"math/tex; mode=display\">\nX^* = \\frac{X-m}{s}</script><p>标准化后的值 =  ( 标准化前的值  － 分量的均值 ) /分量的标准差</p>\n<p>经过简单的推导就可以得到两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的标准化欧氏距离的公式：</p>\n<script type=\"math/tex; mode=display\">\nd_{12} =\\sqrt {\\sum_{k=1}^{n} (\\frac{x_{1k}-x_{2k}}{s_{k}})^2}</script><p>如果将方差的倒数看成是一个权重，这个公式可以看成是一种加权欧氏距离(Weighted Euclidean distance)。</p>\n<p>python 实现为<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def moreBZOSdis(a,b):</span><br><span class=\"line\">    sumnum = 0</span><br><span class=\"line\">    for i in range(len(a)):</span><br><span class=\"line\">        # 计算si 分量标准差</span><br><span class=\"line\">        avg = (a[i]-b[i])/2</span><br><span class=\"line\">        si = sqrt( (a[i] - avg) ** 2 + (b[i] - avg) ** 2 )</span><br><span class=\"line\">        sumnum += ((a[i]-b[i])/si ) ** 2</span><br><span class=\"line\">\t</span><br><span class=\"line\">    return sqrt(sumnum)</span><br><span class=\"line\"></span><br><span class=\"line\">print &apos;a,b 标准欧式距离：&apos;,moreBZOSdis((1,2,1,2),(3,3,3,4))</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"曼哈顿距离\"><a href=\"#曼哈顿距离\" class=\"headerlink\" title=\"曼哈顿距离\"></a>曼哈顿距离</h1><p>又称为城市街区距离（City Block distance）, 想象你在曼哈顿要从一个十字路口开车到另外一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。实际驾驶距离就是这个“曼哈顿距离”。而这也是曼哈顿距离名称的来源。同样曼哈顿距离也分为二维，三维和多维。</p>\n<p>在计程车几何学中，一个圆是由从圆心向各个固定曼哈顿距离标示出来的点围成的区域，因此这种圆其实就是旋转了45度的正方形。如果有一群圆，且任两圆皆相交，则整群圆必在某点相交；因此曼哈顿距离会形成一个超凸度量空间。</p>\n<p>这里有一篇人脸表情分类的论文采用的曼哈顿距离进行计算的，<a href=\"http://download.csdn.net/detail/gamer_gyt/9899825\" target=\"_blank\" rel=\"external\">一种人脸表情分类的新方法——Manhattan距离</a></p>\n<h2 id=\"二维曼哈顿距离\"><a href=\"#二维曼哈顿距离\" class=\"headerlink\" title=\"二维曼哈顿距离\"></a>二维曼哈顿距离</h2><p>二维平面两点a(x1,y1)与b(x2,y2)间的曼哈顿距离</p>\n<script type=\"math/tex; mode=display\">\nd12 =\\left | x_{1}-x_{2} \\right |  + \\left |y_{1}-y_{2}  \\right |</script><p>python实现为<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def twoMHDdis(a,b):</span><br><span class=\"line\">    return abs(a[0]-b[0])+abs(a[1]-b[1])</span><br><span class=\"line\"></span><br><span class=\"line\">print &apos;a,b 二维曼哈顿距离为：&apos;, twoMHDdis((1,1),(2,2))</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"三维曼哈顿距离\"><a href=\"#三维曼哈顿距离\" class=\"headerlink\" title=\"三维曼哈顿距离\"></a>三维曼哈顿距离</h2><p>三维平面两点a(x1,y1,z1)与b(x2,y2,z2)间的曼哈顿距离</p>\n<script type=\"math/tex; mode=display\">\nd12 =\\left | x_{1}-x_{2} \\right |  + \\left |y_{1}-y_{2}  \\right | + \\left |z_{1}-z_{2}  \\right |</script><p>python实现为<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def threeMHDdis(a,b):</span><br><span class=\"line\">\treturn abs(a[0]-b[0])+abs(a[1]-b[1]) + abs(a[2]-b[2])</span><br><span class=\"line\"> </span><br><span class=\"line\">print &apos;a,b 三维曼哈顿距离为：&apos;, threeMHDdis((1,1,1),(2,2,2))</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"多维曼哈顿距离\"><a href=\"#多维曼哈顿距离\" class=\"headerlink\" title=\"多维曼哈顿距离\"></a>多维曼哈顿距离</h2><p>多维平面两点a(x1,y1)与b(x2,y2)间的曼哈顿距离</p>\n<script type=\"math/tex; mode=display\">\nd12 = \\sum_{k=1}^{n} \\left | x_{1k} - x_{2k} \\right |</script><p>python实现为<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def moreMHDdis(a,b):</span><br><span class=\"line\">    sum = 0 </span><br><span class=\"line\">    for i in range(len(a)):</span><br><span class=\"line\">        sum += abs(a[i]-b[i])</span><br><span class=\"line\">    return sum</span><br><span class=\"line\"></span><br><span class=\"line\">print &apos;a,b 多维曼哈顿距离为：&apos;, moreMHDdis((1,1,1,1),(2,2,2,2))</span><br></pre></td></tr></table></figure></p>\n<p>由于维距离计算是比较灵活的，所以也同样适合二维和三维。</p>\n<h1 id=\"切比雪夫距离\"><a href=\"#切比雪夫距离\" class=\"headerlink\" title=\"切比雪夫距离\"></a>切比雪夫距离</h1><p>切比雪夫距离（Chebyshev Distance）的定义为：max( | x2-x1 | , |y2-y1 | , … ), 切比雪夫距离用的时候数据的维度必须是三个以上，这篇文章中<a href=\"http://blog.csdn.net/jerry81333/article/details/52632687\" target=\"_blank\" rel=\"external\">曼哈顿距离，欧式距离，明式距离，切比雪夫距离区别</a> 给了一个很形象的解释如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">比如，有同样两个人，在纽约准备到北京参拜天安门，同一个地点出发的话，按照欧式距离来计算，是完全一样的。</span><br><span class=\"line\"></span><br><span class=\"line\">但是按照切比雪夫距离，这是完全不同的概念了。</span><br><span class=\"line\"></span><br><span class=\"line\">譬如，其中一个人是土豪，另一个人是中产阶级，第一个人就能当晚直接头等舱走人，而第二个人可能就要等机票什么时候打折再去，或者选择坐船什么的。</span><br><span class=\"line\"></span><br><span class=\"line\">这样来看的话，距离是不是就不一样了呢？</span><br><span class=\"line\"></span><br><span class=\"line\">或者还是不清楚，我再说的详细点。</span><br><span class=\"line\"></span><br><span class=\"line\">同样是这两个人，欧式距离是直接算最短距离的，而切比雪夫距离可能还得加上财力，比如第一个人财富值100，第二个只有30，虽然物理距离一样，但是所包含的内容却是不同的。</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"二维切比雪夫距离\"><a href=\"#二维切比雪夫距离\" class=\"headerlink\" title=\"二维切比雪夫距离\"></a>二维切比雪夫距离</h2><p>二维平面两点a(x1,y1)与b(x2,y2)间的切比雪夫距离</p>\n<script type=\"math/tex; mode=display\">\nd_{12} = max( \\left | x_{1} - x_{2} \\right | , \\left | y_{1} - y_{2} \\right |)</script><p>python 实现为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def twoQBXFdis(a,b):</span><br><span class=\"line\">    return max( abs(a[0]-b[0]), abs(a[1]-b[1]))</span><br><span class=\"line\"></span><br><span class=\"line\">print &apos;a,b二维切比雪夫距离：&apos; , twoQBXFdis((1,2),(3,4))</span><br></pre></td></tr></table></figure>\n<h2 id=\"多维切比雪夫距离\"><a href=\"#多维切比雪夫距离\" class=\"headerlink\" title=\"多维切比雪夫距离\"></a>多维切比雪夫距离</h2><p>两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的切比雪夫距离</p>\n<script type=\"math/tex; mode=display\">\nd12 = max_{i\\epsilon n}( \\left | x_{1i} - x_{2i} \\right | )</script><p>python 实现为<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def moreQBXFdis(a,b):</span><br><span class=\"line\">    maxnum = 0</span><br><span class=\"line\">    for i in range(len(a)):</span><br><span class=\"line\">        if abs(a[i]-b[i]) &gt; maxnum:</span><br><span class=\"line\">            maxnum = abs(a[i]-b[i])</span><br><span class=\"line\">    return maxnum</span><br><span class=\"line\"></span><br><span class=\"line\">print &apos;a,b多维切比雪夫距离：&apos; , moreQBXFdis((1,1,1,1),(3,4,3,4))</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"马氏距离\"><a href=\"#马氏距离\" class=\"headerlink\" title=\"马氏距离\"></a>马氏距离</h1><p>有M个样本向量X1~Xm，协方差矩阵记为S，均值记为向量μ，则其中样本向量X到u的马氏距离表示为</p>\n<script type=\"math/tex; mode=display\">\nD(x) = \\sqrt{(X-\\mu )^TS^{-1}(X-\\mu)}</script><p>而其中向量Xi与Xj之间的马氏距离定义为</p>\n<script type=\"math/tex; mode=display\">\nD(X_{i},X_{j}) = \\sqrt{(X_{i}-X_{j} )^TS^{-1}(X_{i}-X_{j} )}</script><p> 若协方差矩阵是单位矩阵（各个样本向量之间独立同分布）,则公式就成了：</p>\n<script type=\"math/tex; mode=display\">\nD(X_{i},X_{j}) = \\sqrt{(X_{i}-X_{j} )^T(X_{i}-X_{j} )}</script><p>也就是欧氏距离了。</p>\n<p>若协方差矩阵是对角矩阵，公式变成了标准化欧氏距离。</p>\n<p>马氏距离的优缺点：量纲无关，排除变量之间的相关性的干扰。</p>\n<h1 id=\"夹角余弦\"><a href=\"#夹角余弦\" class=\"headerlink\" title=\"夹角余弦\"></a>夹角余弦</h1><p>几何中夹角余弦可用来衡量两个向量方向的差异，机器学习中借用这一概念来衡量样本向量之间的差异。</p>\n<h2 id=\"二维空间向量的夹角余弦相似度\"><a href=\"#二维空间向量的夹角余弦相似度\" class=\"headerlink\" title=\"二维空间向量的夹角余弦相似度\"></a>二维空间向量的夹角余弦相似度</h2><p>在二维空间中向量A(x1,y1)与向量B(x2,y2)的夹角余弦公式：</p>\n<script type=\"math/tex; mode=display\">\n\\cos \\theta  = \\frac{x_{1}x_{2} + y_{1}y_{2}}{ \\sqrt{ x_{1}^2+x_{2}^2 }\\sqrt{ y_{1}^2+y_{2}^2 } }</script><p>python 实现为<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def twoCos(a,b):</span><br><span class=\"line\">    cos = (a[0]*b[0]+a[1]*b[1]) / (sqrt(a[0]**2 + b[0]**2) * sqrt(a[1]**2 + b[1]**2) )</span><br><span class=\"line\"></span><br><span class=\"line\">    return cos</span><br><span class=\"line\">print &apos;a,b 二维夹角余弦距离：&apos;,twoCos((1,1),(2,2))</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"多维空间向量的夹角余弦相似度\"><a href=\"#多维空间向量的夹角余弦相似度\" class=\"headerlink\" title=\"多维空间向量的夹角余弦相似度\"></a>多维空间向量的夹角余弦相似度</h2><p>两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)的夹角余弦</p>\n<p>类似的，对于两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)，可以使用类似于夹角余弦的概念来衡量它们间的相似程度。</p>\n<script type=\"math/tex; mode=display\">\n\\cos \\theta  = \\frac{a \\cdot  b}{\\left | a \\right | \\left | b \\right |}</script><p>即：</p>\n<script type=\"math/tex; mode=display\">\n\\cos \\theta  = \\frac{ \\sum_{k=1}^{n} x_{1k}x_{2k} }{ \\sqrt{ \\sum_{k=1}^{n}x_{1k}^2 }\\sqrt{ \\sum_{k=1}^{n} x_{2k}^2 } }</script><p>python实现为<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def moreCos(a,b):</span><br><span class=\"line\">    sum_fenzi = 0.0</span><br><span class=\"line\">    sum_fenmu_1,sum_fenmu_2 = 0,0</span><br><span class=\"line\">    for i in range(len(a)):</span><br><span class=\"line\">        sum_fenzi += a[i]*b[i]</span><br><span class=\"line\">        sum_fenmu_1 += a[i]**2 </span><br><span class=\"line\">        sum_fenmu_2 += b[i]**2 </span><br><span class=\"line\"></span><br><span class=\"line\">    return sum_fenzi/( sqrt(sum_fenmu_1) * sqrt(sum_fenmu_2) )</span><br><span class=\"line\">print &apos;a,b 多维夹角余弦距离：&apos;,moreCos((1,1,1,1),(2,2,2,2))</span><br></pre></td></tr></table></figure></p>\n<p>夹角余弦取值范围为[-1,1]。夹角余弦越大表示两个向量的夹角越小，夹角余弦越小表示两向量的夹角越大。当两个向量的方向重合时夹角余弦取最大值1，当两个向量的方向完全相反夹角余弦取最小值-1。</p>\n<h1 id=\"闵可夫斯基距离\"><a href=\"#闵可夫斯基距离\" class=\"headerlink\" title=\"闵可夫斯基距离\"></a>闵可夫斯基距离</h1><p>闵氏距离不是一种距离，而是一组距离的定义</p>\n<h2 id=\"定义\"><a href=\"#定义\" class=\"headerlink\" title=\"定义\"></a>定义</h2><p>两个n维变量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的闵可夫斯基距离定义为：</p>\n<script type=\"math/tex; mode=display\">\n\\sqrt[p]{ \\sum_{k=1}^{n} \\left | x_{1k}-x_{2k} \\right |^p}</script><p>其中p是一个变参数。</p>\n<p>当p=1时，就是曼哈顿距离</p>\n<p>当p=2时，就是欧氏距离</p>\n<p>当p→∞时，就是切比雪夫距离</p>\n<p>根据变参数的不同，闵氏距离可以表示一类的距离。</p>\n<h2 id=\"闵氏距离的缺点\"><a href=\"#闵氏距离的缺点\" class=\"headerlink\" title=\"闵氏距离的缺点\"></a>闵氏距离的缺点</h2><p>闵氏距离，包括曼哈顿距离、欧氏距离和切比雪夫距离都存在明显的缺点。</p>\n<p>举个例子：二维样本(身高,体重)，其中身高范围是150 ~ 190，体重范围是50 ~ 60，有三个样本：a(180,50)，b(190,50)，c(180,60)。那么a与b之间的闵氏距离（无论是曼哈顿距离、欧氏距离或切比雪夫距离）等于a与c之间的闵氏距离，但是身高的10cm真的等价于体重的10kg么？因此用闵氏距离来衡量这些样本间的相似度很有问题。</p>\n<p>简单说来，闵氏距离的缺点主要有两个：(1)将各个分量的量纲(scale)，也就是“单位”当作相同的看待了。(2)没有考虑各个分量的分布（期望，方差等)可能是不同的。</p>\n<h1 id=\"汉明距离\"><a href=\"#汉明距离\" class=\"headerlink\" title=\"汉明距离\"></a>汉明距离</h1><h2 id=\"定义-1\"><a href=\"#定义-1\" class=\"headerlink\" title=\"定义\"></a>定义</h2><p>两个等长字符串s1与s2之间的汉明距离定义为将其中一个变为另外一个所需要作的最小替换次数。例如字符串“1111”与“1001”之间的汉明距离为2。</p>\n<p>应用：信息编码（为了增强容错性，应使得编码间的最小汉明距离尽可能大）。</p>\n<h2 id=\"python-实现\"><a href=\"#python-实现\" class=\"headerlink\" title=\"python 实现\"></a>python 实现</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def hanmingDis(a,b):</span><br><span class=\"line\">    sumnum = 0</span><br><span class=\"line\">    for i in range(len(a)):</span><br><span class=\"line\">        if a[i]!=b[i]:</span><br><span class=\"line\">            sumnum += 1</span><br><span class=\"line\">    return sumnum</span><br><span class=\"line\"></span><br><span class=\"line\">print &apos;a,b 汉明距离：&apos;,hanmingDis((1,1,2,3),(2,2,1,3))</span><br></pre></td></tr></table></figure>\n<h1 id=\"杰卡德距离-amp-杰卡德相似系数\"><a href=\"#杰卡德距离-amp-杰卡德相似系数\" class=\"headerlink\" title=\"杰卡德距离 &amp; 杰卡德相似系数\"></a>杰卡德距离 &amp; 杰卡德相似系数</h1><h2 id=\"杰卡德距离\"><a href=\"#杰卡德距离\" class=\"headerlink\" title=\"杰卡德距离\"></a>杰卡德距离</h2><p>与杰卡德相似系数相反的概念是杰卡德距离(Jaccard distance)。杰卡德距离可用如下公式表示：</p>\n<script type=\"math/tex; mode=display\">\nJ_{\\delta} (A,B) = \\frac{| A \\bigcup B | - | A \\bigcap B |}{| A \\bigcup B |}</script><p>杰卡德距离用两个集合中不同元素占所有元素的比例来衡量两个集合的区分度。</p>\n<p>python 实现<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def jiekadeDis(a,b):</span><br><span class=\"line\">    set_a = set(a)</span><br><span class=\"line\">    set_b = set(b)</span><br><span class=\"line\">    dis = float(len( (set_a | set_b) - (set_a &amp; set_b) ) )/ len(set_a | set_b)</span><br><span class=\"line\">    return dis</span><br><span class=\"line\"></span><br><span class=\"line\">print &apos;a,b 杰卡德距离：&apos;, jiekadeDis((1,2,3),(2,3,4))</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"杰卡德相似系数\"><a href=\"#杰卡德相似系数\" class=\"headerlink\" title=\"杰卡德相似系数\"></a>杰卡德相似系数</h2><p>两个集合A和B的交集元素在A，B的并集中所占的比例，称为两个集合的杰卡德相似系数，用符号J(A,B)表示。</p>\n<script type=\"math/tex; mode=display\">\nJ(A,B) = \\frac{| A \\bigcap B |}{| A \\bigcup B |}</script><p>杰卡德相似系数是衡量两个集合的相似度一种指标。</p>\n<p>python 实现<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def jiekadeXSDis(a,b):</span><br><span class=\"line\">    set_a = set(a)</span><br><span class=\"line\">    set_b = set(b)</span><br><span class=\"line\">    dis = float(len(set_a &amp; set_b)  )/ len(set_a | set_b)</span><br><span class=\"line\">    return dis</span><br><span class=\"line\"></span><br><span class=\"line\">print &apos;a,b 杰卡德相似系数：&apos;, jiekadeXSDis((1,2,3),(2,3,4))</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"杰卡德相似系数与杰卡德距离的应用\"><a href=\"#杰卡德相似系数与杰卡德距离的应用\" class=\"headerlink\" title=\"杰卡德相似系数与杰卡德距离的应用\"></a>杰卡德相似系数与杰卡德距离的应用</h2><p>可将杰卡德相似系数用在衡量样本的相似度上。</p>\n<p>　　样本A与样本B是两个n维向量，而且所有维度的取值都是0或1。例如：A(0111)和B(1011)。我们将样本看成是一个集合，1表示集合包含该元素，0表示集合不包含该元素。</p>\n<p>p ：样本A与B都是1的维度的个数</p>\n<p>q ：样本A是1，样本B是0的维度的个数</p>\n<p>r ：样本A是0，样本B是1的维度的个数</p>\n<p>s ：样本A与B都是0的维度的个数</p>\n<p>那么样本A与B的杰卡德相似系数可以表示为：</p>\n<p>这里p+q+r可理解为A与B的并集的元素个数，而p是A与B的交集的元素个数。</p>\n<p>而样本A与B的杰卡德距离表示为：</p>\n<script type=\"math/tex; mode=display\">\nJ= \\frac{p}{p+q+r}</script><h1 id=\"相关系数-amp-相关距离\"><a href=\"#相关系数-amp-相关距离\" class=\"headerlink\" title=\"相关系数 &amp; 相关距离\"></a>相关系数 &amp; 相关距离</h1><h2 id=\"相关系数\"><a href=\"#相关系数\" class=\"headerlink\" title=\"相关系数\"></a>相关系数</h2><script type=\"math/tex; mode=display\">\n\\rho_{XY} = \\frac{Cov(X,Y)}{\\sqrt{D(X)} \\sqrt{D(Y)}}=\\frac{ E( (X-EX) (Y-EY) ) }{ \\sqrt{D(X)} \\sqrt{D(Y)} }</script><p>相关系数是衡量随机变量X与Y相关程度的一种方法，相关系数的取值范围是[-1,1]。相关系数的绝对值越大，则表明X与Y相关度越高。当X与Y线性相关时，相关系数取值为1（正线性相关）或-1（负线性相关）。</p>\n<p>python 实现<br>相关系数可以利用numpy库中的corrcoef函数来计算<br>例如 对于矩阵a,numpy.corrcoef(a)可计算行与行之间的相关系数，numpy.corrcoef(a,rowvar=0)用于计算各列之间的相关系数，输出为相关系数矩阵。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from numpy import  *</span><br><span class=\"line\">a = array([[1, 1, 2, 2, 3],  </span><br><span class=\"line\">       [2, 2, 3, 3, 5],  </span><br><span class=\"line\">       [1, 4, 2, 2, 3]]) </span><br><span class=\"line\"></span><br><span class=\"line\">print corrcoef(a)</span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;array([[ 1.        ,  0.97590007,  0.10482848],</span><br><span class=\"line\">       [ 0.97590007,  1.        ,  0.17902872],</span><br><span class=\"line\">       [ 0.10482848,  0.17902872,  1.        ]])</span><br><span class=\"line\"></span><br><span class=\"line\">print corrcoef(a,rowvar=0)</span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;array([[ 1.        , -0.18898224,  1.        ,  1.        ,  1.        ],</span><br><span class=\"line\">       [-0.18898224,  1.        , -0.18898224, -0.18898224, -0.18898224],</span><br><span class=\"line\">       [ 1.        , -0.18898224,  1.        ,  1.        ,  1.        ],</span><br><span class=\"line\">       [ 1.        , -0.18898224,  1.        ,  1.        ,  1.        ],</span><br><span class=\"line\">       [ 1.        , -0.18898224,  1.        ,  1.        ,  1.        ]])</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"相关距离\"><a href=\"#相关距离\" class=\"headerlink\" title=\"相关距离\"></a>相关距离</h2><script type=\"math/tex; mode=display\">\nD_{xy} = 1 - \\rho _{XY}</script><p>python 实现（基于相关系数）<br>同样针对矩阵a<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 行之间的相关距离</span><br><span class=\"line\">ones(shape(corrcoef(a)),int) - corrcoef(a)</span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;array([[ 0.        ,  0.02409993,  0.89517152],</span><br><span class=\"line\">       [ 0.02409993,  0.        ,  0.82097128],</span><br><span class=\"line\">       [ 0.89517152,  0.82097128,  0.        ]])</span><br><span class=\"line\">       </span><br><span class=\"line\">       </span><br><span class=\"line\"># 列之间的相关距离</span><br><span class=\"line\">ones(shape(corrcoef(a,rowvar = 0)),int) - corrcoef(a,rowvar = 0)</span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;array([[ 0.        ,  1.18898224,  0.        ,  0.        ,  0.        ],</span><br><span class=\"line\">       [ 1.18898224,  0.        ,  1.18898224,  1.18898224,  1.18898224],</span><br><span class=\"line\">       [ 0.        ,  1.18898224,  0.        ,  0.        ,  0.        ],</span><br><span class=\"line\">       [ 0.        ,  1.18898224,  0.        ,  0.        ,  0.        ],</span><br><span class=\"line\">       [ 0.        ,  1.18898224,  0.        ,  0.        ,  0.        ]])</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"信息熵\"><a href=\"#信息熵\" class=\"headerlink\" title=\"信息熵\"></a>信息熵</h1><p>信息熵并不属于一种相似性度量，是衡量分布的混乱程度或分散程度的一种度量。分布越分散(或者说分布越平均)，信息熵就越大。分布越有序（或者说分布越集中），信息熵就越小。</p>\n<p>计算给定的样本集X的信息熵的公式：</p>\n<script type=\"math/tex; mode=display\">\nEntropy(X) = \\sum_{i=1}^{n} -p_{i} log_{2}p_{i}</script><p>参数的含义：</p>\n<p>n：样本集X的分类数</p>\n<p>pi：X中第i类元素出现的概率</p>\n<p>信息熵越大表明样本集S分类越分散，信息熵越小则表明样本集X分类越集中。。当S中n个分类出现的概率一样大时（都是1/n），信息熵取最大值log2(n)。当X只有一个分类时，信息熵取最小值0</p>\n<p>python进行计算和实现可参考：<br><a href=\"http://blog.csdn.net/autoliuweijie/article/details/52244246\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/autoliuweijie/article/details/52244246</a></p>"},{"title":"数据结构算法之链表","date":"2017-11-12T16:58:37.000Z","_content":"\n链表面试总结，使用python实现，参考：https://www.cnblogs.com/lixiaohui-ambition/archive/2012/09/25/2703195.html\n<!--More-->\n\n```\n#coding:utf-8\n\n# 定义链表\nclass ListNode:\n    def __init__(self):\n        self.data = None\n        self.pnext = None\n\n# 链表操作类\nclass ListNode_handle:\n    def __init__(self):\n        self.cur_node = None\n    \n    # 链表添加元素\n    def add(self,data):\n        ln = ListNode()\n        ln.data = data\n        \n        ln.pnext = self.cur_node\n        self.cur_node = ln\n        return ln\n    \n    # 打印链表\n    def prt(self,ln):\n        while ln:\n            print(ln.data,end=\"  \")\n            ln = ln.pnext\n    # 逆序输出\n    def _reverse(self,ln):\n        _list = []\n        while ln:\n            _list.append(ln.data)\n            ln = ln.pnext\n        ln_2 = ListNode()\n        ln_h = ListNode_handle()\n        for i in _list:\n            ln_2 = ln_h.add(i)\n        return ln_2\n    \n    # 求链表的长度\n    def _length(self,ln):\n        _len = 0\n        while ln:\n            _len += 1\n            ln = ln.pnext\n        return _len\n    \n    # 查找指定位置的节点\n    def _find_loc(self,ln,loc):\n        _sum = 0\n        while ln and _sum != loc:\n            _sum += 1\n            ln = ln.pnext\n        return ln.data\n    \n    # 判断某个节点是否在链表中\n    def _exist(self,ln,data):\n        flag = False\n        while ln and data != ln.data:\n            ln = ln.pnext\n        return flag\n\n# 创建链表   \nln = ListNode()\nln_h = ListNode_handle()\na = [1,4,2,5,8,5,7,9]\nfor i in a:\n    ln = ln_h.add(i)\n\nprint(\"正序输出...\")\nln_h.prt(ln)\n\nprint(\"\\n\\n逆序输出...\")\nln_2 = ln_h._reverse(ln)\nln_h.prt(ln_2)\n\n# 求链表ln的长度\nlength = ln_h._length(ln)\nprint(\"\\n\\nln的长度为:\",length)\n\n# 查找链表ln中的倒数第３个节点\ndata = ln_h._find_loc(ln,ln_h._length(ln)-3)\nprint(\"\\n\\n倒数第三个节点为:\",data)\n\n# 返回某个节点在链表中的位置\nloc = ln_h._loc(ln,5)\n\n#　判断某个节点是否在链表中\nflag = ln_h._exist(ln,5)\nprint(\"\\n\\n５是否存在与链表ln中:\",end=\" \")\nif flag:\n    print(\"Yes\")\nelse:\n    print(\"No\")\n```","source":"_posts/数据结构/数据结构算法之链表.md","raw":"---\ntitle: 数据结构算法之链表\ndate: 2017-11-13 00:58:37\ntags: [数据结构]\ncategories: 技术篇\n---\n\n链表面试总结，使用python实现，参考：https://www.cnblogs.com/lixiaohui-ambition/archive/2012/09/25/2703195.html\n<!--More-->\n\n```\n#coding:utf-8\n\n# 定义链表\nclass ListNode:\n    def __init__(self):\n        self.data = None\n        self.pnext = None\n\n# 链表操作类\nclass ListNode_handle:\n    def __init__(self):\n        self.cur_node = None\n    \n    # 链表添加元素\n    def add(self,data):\n        ln = ListNode()\n        ln.data = data\n        \n        ln.pnext = self.cur_node\n        self.cur_node = ln\n        return ln\n    \n    # 打印链表\n    def prt(self,ln):\n        while ln:\n            print(ln.data,end=\"  \")\n            ln = ln.pnext\n    # 逆序输出\n    def _reverse(self,ln):\n        _list = []\n        while ln:\n            _list.append(ln.data)\n            ln = ln.pnext\n        ln_2 = ListNode()\n        ln_h = ListNode_handle()\n        for i in _list:\n            ln_2 = ln_h.add(i)\n        return ln_2\n    \n    # 求链表的长度\n    def _length(self,ln):\n        _len = 0\n        while ln:\n            _len += 1\n            ln = ln.pnext\n        return _len\n    \n    # 查找指定位置的节点\n    def _find_loc(self,ln,loc):\n        _sum = 0\n        while ln and _sum != loc:\n            _sum += 1\n            ln = ln.pnext\n        return ln.data\n    \n    # 判断某个节点是否在链表中\n    def _exist(self,ln,data):\n        flag = False\n        while ln and data != ln.data:\n            ln = ln.pnext\n        return flag\n\n# 创建链表   \nln = ListNode()\nln_h = ListNode_handle()\na = [1,4,2,5,8,5,7,9]\nfor i in a:\n    ln = ln_h.add(i)\n\nprint(\"正序输出...\")\nln_h.prt(ln)\n\nprint(\"\\n\\n逆序输出...\")\nln_2 = ln_h._reverse(ln)\nln_h.prt(ln_2)\n\n# 求链表ln的长度\nlength = ln_h._length(ln)\nprint(\"\\n\\nln的长度为:\",length)\n\n# 查找链表ln中的倒数第３个节点\ndata = ln_h._find_loc(ln,ln_h._length(ln)-3)\nprint(\"\\n\\n倒数第三个节点为:\",data)\n\n# 返回某个节点在链表中的位置\nloc = ln_h._loc(ln,5)\n\n#　判断某个节点是否在链表中\nflag = ln_h._exist(ln,5)\nprint(\"\\n\\n５是否存在与链表ln中:\",end=\" \")\nif flag:\n    print(\"Yes\")\nelse:\n    print(\"No\")\n```","slug":"数据结构/数据结构算法之链表","published":1,"updated":"2017-12-19T02:54:56.787Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjbd1r267000mkxuwc1j2j1ed","content":"<p>链表面试总结，使用python实现，参考：<a href=\"https://www.cnblogs.com/lixiaohui-ambition/archive/2012/09/25/2703195.html\" target=\"_blank\" rel=\"external\">https://www.cnblogs.com/lixiaohui-ambition/archive/2012/09/25/2703195.html</a><br><a id=\"more\"></a></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#coding:utf-8</span><br><span class=\"line\"></span><br><span class=\"line\"># 定义链表</span><br><span class=\"line\">class ListNode:</span><br><span class=\"line\">    def __init__(self):</span><br><span class=\"line\">        self.data = None</span><br><span class=\"line\">        self.pnext = None</span><br><span class=\"line\"></span><br><span class=\"line\"># 链表操作类</span><br><span class=\"line\">class ListNode_handle:</span><br><span class=\"line\">    def __init__(self):</span><br><span class=\"line\">        self.cur_node = None</span><br><span class=\"line\">    </span><br><span class=\"line\">    # 链表添加元素</span><br><span class=\"line\">    def add(self,data):</span><br><span class=\"line\">        ln = ListNode()</span><br><span class=\"line\">        ln.data = data</span><br><span class=\"line\">        </span><br><span class=\"line\">        ln.pnext = self.cur_node</span><br><span class=\"line\">        self.cur_node = ln</span><br><span class=\"line\">        return ln</span><br><span class=\"line\">    </span><br><span class=\"line\">    # 打印链表</span><br><span class=\"line\">    def prt(self,ln):</span><br><span class=\"line\">        while ln:</span><br><span class=\"line\">            print(ln.data,end=&quot;  &quot;)</span><br><span class=\"line\">            ln = ln.pnext</span><br><span class=\"line\">    # 逆序输出</span><br><span class=\"line\">    def _reverse(self,ln):</span><br><span class=\"line\">        _list = []</span><br><span class=\"line\">        while ln:</span><br><span class=\"line\">            _list.append(ln.data)</span><br><span class=\"line\">            ln = ln.pnext</span><br><span class=\"line\">        ln_2 = ListNode()</span><br><span class=\"line\">        ln_h = ListNode_handle()</span><br><span class=\"line\">        for i in _list:</span><br><span class=\"line\">            ln_2 = ln_h.add(i)</span><br><span class=\"line\">        return ln_2</span><br><span class=\"line\">    </span><br><span class=\"line\">    # 求链表的长度</span><br><span class=\"line\">    def _length(self,ln):</span><br><span class=\"line\">        _len = 0</span><br><span class=\"line\">        while ln:</span><br><span class=\"line\">            _len += 1</span><br><span class=\"line\">            ln = ln.pnext</span><br><span class=\"line\">        return _len</span><br><span class=\"line\">    </span><br><span class=\"line\">    # 查找指定位置的节点</span><br><span class=\"line\">    def _find_loc(self,ln,loc):</span><br><span class=\"line\">        _sum = 0</span><br><span class=\"line\">        while ln and _sum != loc:</span><br><span class=\"line\">            _sum += 1</span><br><span class=\"line\">            ln = ln.pnext</span><br><span class=\"line\">        return ln.data</span><br><span class=\"line\">    </span><br><span class=\"line\">    # 判断某个节点是否在链表中</span><br><span class=\"line\">    def _exist(self,ln,data):</span><br><span class=\"line\">        flag = False</span><br><span class=\"line\">        while ln and data != ln.data:</span><br><span class=\"line\">            ln = ln.pnext</span><br><span class=\"line\">        return flag</span><br><span class=\"line\"></span><br><span class=\"line\"># 创建链表   </span><br><span class=\"line\">ln = ListNode()</span><br><span class=\"line\">ln_h = ListNode_handle()</span><br><span class=\"line\">a = [1,4,2,5,8,5,7,9]</span><br><span class=\"line\">for i in a:</span><br><span class=\"line\">    ln = ln_h.add(i)</span><br><span class=\"line\"></span><br><span class=\"line\">print(&quot;正序输出...&quot;)</span><br><span class=\"line\">ln_h.prt(ln)</span><br><span class=\"line\"></span><br><span class=\"line\">print(&quot;\\n\\n逆序输出...&quot;)</span><br><span class=\"line\">ln_2 = ln_h._reverse(ln)</span><br><span class=\"line\">ln_h.prt(ln_2)</span><br><span class=\"line\"></span><br><span class=\"line\"># 求链表ln的长度</span><br><span class=\"line\">length = ln_h._length(ln)</span><br><span class=\"line\">print(&quot;\\n\\nln的长度为:&quot;,length)</span><br><span class=\"line\"></span><br><span class=\"line\"># 查找链表ln中的倒数第３个节点</span><br><span class=\"line\">data = ln_h._find_loc(ln,ln_h._length(ln)-3)</span><br><span class=\"line\">print(&quot;\\n\\n倒数第三个节点为:&quot;,data)</span><br><span class=\"line\"></span><br><span class=\"line\"># 返回某个节点在链表中的位置</span><br><span class=\"line\">loc = ln_h._loc(ln,5)</span><br><span class=\"line\"></span><br><span class=\"line\">#　判断某个节点是否在链表中</span><br><span class=\"line\">flag = ln_h._exist(ln,5)</span><br><span class=\"line\">print(&quot;\\n\\n５是否存在与链表ln中:&quot;,end=&quot; &quot;)</span><br><span class=\"line\">if flag:</span><br><span class=\"line\">    print(&quot;Yes&quot;)</span><br><span class=\"line\">else:</span><br><span class=\"line\">    print(&quot;No&quot;)</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"<p>链表面试总结，使用python实现，参考：<a href=\"https://www.cnblogs.com/lixiaohui-ambition/archive/2012/09/25/2703195.html\" target=\"_blank\" rel=\"external\">https://www.cnblogs.com/lixiaohui-ambition/archive/2012/09/25/2703195.html</a><br>","more":"</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#coding:utf-8</span><br><span class=\"line\"></span><br><span class=\"line\"># 定义链表</span><br><span class=\"line\">class ListNode:</span><br><span class=\"line\">    def __init__(self):</span><br><span class=\"line\">        self.data = None</span><br><span class=\"line\">        self.pnext = None</span><br><span class=\"line\"></span><br><span class=\"line\"># 链表操作类</span><br><span class=\"line\">class ListNode_handle:</span><br><span class=\"line\">    def __init__(self):</span><br><span class=\"line\">        self.cur_node = None</span><br><span class=\"line\">    </span><br><span class=\"line\">    # 链表添加元素</span><br><span class=\"line\">    def add(self,data):</span><br><span class=\"line\">        ln = ListNode()</span><br><span class=\"line\">        ln.data = data</span><br><span class=\"line\">        </span><br><span class=\"line\">        ln.pnext = self.cur_node</span><br><span class=\"line\">        self.cur_node = ln</span><br><span class=\"line\">        return ln</span><br><span class=\"line\">    </span><br><span class=\"line\">    # 打印链表</span><br><span class=\"line\">    def prt(self,ln):</span><br><span class=\"line\">        while ln:</span><br><span class=\"line\">            print(ln.data,end=&quot;  &quot;)</span><br><span class=\"line\">            ln = ln.pnext</span><br><span class=\"line\">    # 逆序输出</span><br><span class=\"line\">    def _reverse(self,ln):</span><br><span class=\"line\">        _list = []</span><br><span class=\"line\">        while ln:</span><br><span class=\"line\">            _list.append(ln.data)</span><br><span class=\"line\">            ln = ln.pnext</span><br><span class=\"line\">        ln_2 = ListNode()</span><br><span class=\"line\">        ln_h = ListNode_handle()</span><br><span class=\"line\">        for i in _list:</span><br><span class=\"line\">            ln_2 = ln_h.add(i)</span><br><span class=\"line\">        return ln_2</span><br><span class=\"line\">    </span><br><span class=\"line\">    # 求链表的长度</span><br><span class=\"line\">    def _length(self,ln):</span><br><span class=\"line\">        _len = 0</span><br><span class=\"line\">        while ln:</span><br><span class=\"line\">            _len += 1</span><br><span class=\"line\">            ln = ln.pnext</span><br><span class=\"line\">        return _len</span><br><span class=\"line\">    </span><br><span class=\"line\">    # 查找指定位置的节点</span><br><span class=\"line\">    def _find_loc(self,ln,loc):</span><br><span class=\"line\">        _sum = 0</span><br><span class=\"line\">        while ln and _sum != loc:</span><br><span class=\"line\">            _sum += 1</span><br><span class=\"line\">            ln = ln.pnext</span><br><span class=\"line\">        return ln.data</span><br><span class=\"line\">    </span><br><span class=\"line\">    # 判断某个节点是否在链表中</span><br><span class=\"line\">    def _exist(self,ln,data):</span><br><span class=\"line\">        flag = False</span><br><span class=\"line\">        while ln and data != ln.data:</span><br><span class=\"line\">            ln = ln.pnext</span><br><span class=\"line\">        return flag</span><br><span class=\"line\"></span><br><span class=\"line\"># 创建链表   </span><br><span class=\"line\">ln = ListNode()</span><br><span class=\"line\">ln_h = ListNode_handle()</span><br><span class=\"line\">a = [1,4,2,5,8,5,7,9]</span><br><span class=\"line\">for i in a:</span><br><span class=\"line\">    ln = ln_h.add(i)</span><br><span class=\"line\"></span><br><span class=\"line\">print(&quot;正序输出...&quot;)</span><br><span class=\"line\">ln_h.prt(ln)</span><br><span class=\"line\"></span><br><span class=\"line\">print(&quot;\\n\\n逆序输出...&quot;)</span><br><span class=\"line\">ln_2 = ln_h._reverse(ln)</span><br><span class=\"line\">ln_h.prt(ln_2)</span><br><span class=\"line\"></span><br><span class=\"line\"># 求链表ln的长度</span><br><span class=\"line\">length = ln_h._length(ln)</span><br><span class=\"line\">print(&quot;\\n\\nln的长度为:&quot;,length)</span><br><span class=\"line\"></span><br><span class=\"line\"># 查找链表ln中的倒数第３个节点</span><br><span class=\"line\">data = ln_h._find_loc(ln,ln_h._length(ln)-3)</span><br><span class=\"line\">print(&quot;\\n\\n倒数第三个节点为:&quot;,data)</span><br><span class=\"line\"></span><br><span class=\"line\"># 返回某个节点在链表中的位置</span><br><span class=\"line\">loc = ln_h._loc(ln,5)</span><br><span class=\"line\"></span><br><span class=\"line\">#　判断某个节点是否在链表中</span><br><span class=\"line\">flag = ln_h._exist(ln,5)</span><br><span class=\"line\">print(&quot;\\n\\n５是否存在与链表ln中:&quot;,end=&quot; &quot;)</span><br><span class=\"line\">if flag:</span><br><span class=\"line\">    print(&quot;Yes&quot;)</span><br><span class=\"line\">else:</span><br><span class=\"line\">    print(&quot;No&quot;)</span><br></pre></td></tr></table></figure>"},{"title":"几种距离计算公式在数据挖掘中的应用场景分析","date":"2017-09-20T02:23:39.000Z","_content":"\n本文涉及以下几种距离计算公式的分析，参考资料为《面向程序员的数据挖掘指南》\n\n- 曼哈顿距离\n- 欧几里得距离\n- 闵可夫斯基距离\n- 皮尔逊相关系数\n- 余弦相似度\n\n<!--More-->\n\n之前整理过一篇关于距离相关的文章：[机器学习算法中的距离和相似性计算公式，分析以及python实现]()\n\n\n\n# 闵可夫斯基距离\n两个n维变量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的闵可夫斯基距离定义为：\n$$ \\sqrt[p]{ \\sum_{k=1}^{n} \\left | x_{1k}-x_{2k} \\right |^p}  $$\n\n其中p是一个变参数。\n\n当p=1时，就是曼哈顿距离\n\n当p=2时，就是欧氏距离\n\n当p→∞时，就是切比雪夫距离\n\n根据变参数的不同，闵氏距离可以表示一类的距离。\n\np值越大，单个维度的差值大小会对整体距离有更大的影响\n\n# 曼哈顿距离／欧几里得距离的瑕疵\n在《面向程序员的数据挖掘指南》中给出了这样一组样例数据, 下图为一个在线音乐网站的的用户评分情况，用户可以用1-5星来评价一个乐队，下边是8位用户对8个乐队的评价：\n![这里写图片描述](http://img.blog.csdn.net/20170920102356159?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n表中的横线表示用户没有对乐队进行评价，我们在计算两个用户的距离时，只采用他们都评价过的乐队。\n\n现在来求Angelica和Bill的距离，因为他们共同评分过的乐队有5个，所以使用其对该5个乐队的评分进行曼哈顿距离的计算为：\n\n```\nDis_1 = |3.5-2| + |2-3.5| + |5-2| + |1.5-3.5| + |2-3| = 9\n```\n\n同样使用欧式距离计算为：\n```\nDis_2 = sqrt( (3.5-2)^2 + (2-3.5)^2 + (5-2)^2 + (1.5-3.5)^2 + (2-3)^2 ) = 4.3\n```\n当对Angelica和Bill，Bill和Chan进行距离对比时，由于两者的共同评分过的乐队均为5，数据都在一个5维空间里，是公平的，如果现在要计算Angelica和Hailey与Bill的距离时，会发现，Angelica与Bill共同评分的有5个乐队，Hailey与Bill共同评分的有3个乐队，也就是说两者数据一个在5维空间里，一个在三维空间里，这样明显是不公平的。这将会对我们进行计算时产生不好的影响，所以曼哈顿距离和欧几里得距离在数据完整的情况下效果最好。\n\n---\n# 用户问题／皮尔逊相关系数／分数膨胀\n\n## 现象——用户问题\n仔细观察用户对乐队的评分数据，可以发现每个用户的评分标准不同：\n\n- Bill没有打出极端的分数，都在2-4分之间\n- Jordyn似乎喜欢所有的乐队，打分都在4-5之间\n- Hailey是一个有趣的人，他的评分不是1就是4\n\n那么如何比较这些用户呢？比如说Hailey的4分是相当于Jordyn的4分还是5分呢？我觉得更接近5分，这样一来，就影响推荐系统的准确性了！\n\n## 解决该现象\n解决该现象的办法之一就是 使用皮尔逊相关系数，例如下边这样的数据样例（Clara和Robert对五个乐队的评分）：\n\n![这里写图片描述](http://img.blog.csdn.net/20170920111425007?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n\n这种现象在数据挖掘领域被称为“分数膨胀“。我们将其评分画成图，如下：\n![这里写图片描述](http://img.blog.csdn.net/20170920112804525?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n一条直线-完全吻合，代表着Clara和Robert的喜好完全一致。\n\n皮尔逊相关系数用于衡量两个变量之间的相关性，他的值在-1～1，1代表完全一致，-1代表完全相悖。所以我们可以利用皮尔逊相关系数来找到相似的用户。\n\n皮尔逊相关系数的计算公式为：\n![这里写图片描述](http://img.blog.csdn.net/20170920114015874?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n该公式除了看起来比较复杂，另外需要对数据进行两次遍历，第一次遍历求出 x平均值和y平均值，第二次遍历才能出现结果，这里提供另外一个计算公式，能够计算皮尔逊相关系数的近似值：\n![这里写图片描述](http://img.blog.csdn.net/20170920114326883?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n---\n\n# 余弦相似度／稀疏数据\n假设这样一个数据集，一个在线音乐网站，有10000w首音乐（这里不考虑音乐类型，年代等因素），每个用户常听的也就其中的几十首，这种情况下使用曼哈顿或者欧几里得或者皮尔逊相关系数进行计算用户之间相似性，计算相似值会非常小，因为用户之间的交集本来就很少，这样对于计算结果来讲是很不准确的，这个时候就需要余弦相似度了，余弦相似度进行计算时会自动略过这些非零值。\n\n# 总结\n这里只是简答的介绍了这几种相似性距离度量的方法和场景，但是在实际环境中远比这个复杂许多。这里总结下：\n\n- 如果数据存在“分数膨胀“问题，就使用皮尔逊相关系数\n- 如果数据比较密集，变量之间基本都存在共有值，且这些距离数据都是非常重要的，那就使用欧几里得或者曼哈顿距离\n- 如果数据是稀疏的，就使用余弦相似度\n\n","source":"_posts/机器学习/几种距离计算公式在数据挖掘中的应用场景分析.md","raw":"---\ntitle: 几种距离计算公式在数据挖掘中的应用场景分析\ndate: 2017-09-20 10:23:39\ntags: [距离计算]\ncategories: 技术篇\n---\n\n本文涉及以下几种距离计算公式的分析，参考资料为《面向程序员的数据挖掘指南》\n\n- 曼哈顿距离\n- 欧几里得距离\n- 闵可夫斯基距离\n- 皮尔逊相关系数\n- 余弦相似度\n\n<!--More-->\n\n之前整理过一篇关于距离相关的文章：[机器学习算法中的距离和相似性计算公式，分析以及python实现]()\n\n\n\n# 闵可夫斯基距离\n两个n维变量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的闵可夫斯基距离定义为：\n$$ \\sqrt[p]{ \\sum_{k=1}^{n} \\left | x_{1k}-x_{2k} \\right |^p}  $$\n\n其中p是一个变参数。\n\n当p=1时，就是曼哈顿距离\n\n当p=2时，就是欧氏距离\n\n当p→∞时，就是切比雪夫距离\n\n根据变参数的不同，闵氏距离可以表示一类的距离。\n\np值越大，单个维度的差值大小会对整体距离有更大的影响\n\n# 曼哈顿距离／欧几里得距离的瑕疵\n在《面向程序员的数据挖掘指南》中给出了这样一组样例数据, 下图为一个在线音乐网站的的用户评分情况，用户可以用1-5星来评价一个乐队，下边是8位用户对8个乐队的评价：\n![这里写图片描述](http://img.blog.csdn.net/20170920102356159?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n表中的横线表示用户没有对乐队进行评价，我们在计算两个用户的距离时，只采用他们都评价过的乐队。\n\n现在来求Angelica和Bill的距离，因为他们共同评分过的乐队有5个，所以使用其对该5个乐队的评分进行曼哈顿距离的计算为：\n\n```\nDis_1 = |3.5-2| + |2-3.5| + |5-2| + |1.5-3.5| + |2-3| = 9\n```\n\n同样使用欧式距离计算为：\n```\nDis_2 = sqrt( (3.5-2)^2 + (2-3.5)^2 + (5-2)^2 + (1.5-3.5)^2 + (2-3)^2 ) = 4.3\n```\n当对Angelica和Bill，Bill和Chan进行距离对比时，由于两者的共同评分过的乐队均为5，数据都在一个5维空间里，是公平的，如果现在要计算Angelica和Hailey与Bill的距离时，会发现，Angelica与Bill共同评分的有5个乐队，Hailey与Bill共同评分的有3个乐队，也就是说两者数据一个在5维空间里，一个在三维空间里，这样明显是不公平的。这将会对我们进行计算时产生不好的影响，所以曼哈顿距离和欧几里得距离在数据完整的情况下效果最好。\n\n---\n# 用户问题／皮尔逊相关系数／分数膨胀\n\n## 现象——用户问题\n仔细观察用户对乐队的评分数据，可以发现每个用户的评分标准不同：\n\n- Bill没有打出极端的分数，都在2-4分之间\n- Jordyn似乎喜欢所有的乐队，打分都在4-5之间\n- Hailey是一个有趣的人，他的评分不是1就是4\n\n那么如何比较这些用户呢？比如说Hailey的4分是相当于Jordyn的4分还是5分呢？我觉得更接近5分，这样一来，就影响推荐系统的准确性了！\n\n## 解决该现象\n解决该现象的办法之一就是 使用皮尔逊相关系数，例如下边这样的数据样例（Clara和Robert对五个乐队的评分）：\n\n![这里写图片描述](http://img.blog.csdn.net/20170920111425007?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n\n这种现象在数据挖掘领域被称为“分数膨胀“。我们将其评分画成图，如下：\n![这里写图片描述](http://img.blog.csdn.net/20170920112804525?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n一条直线-完全吻合，代表着Clara和Robert的喜好完全一致。\n\n皮尔逊相关系数用于衡量两个变量之间的相关性，他的值在-1～1，1代表完全一致，-1代表完全相悖。所以我们可以利用皮尔逊相关系数来找到相似的用户。\n\n皮尔逊相关系数的计算公式为：\n![这里写图片描述](http://img.blog.csdn.net/20170920114015874?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n该公式除了看起来比较复杂，另外需要对数据进行两次遍历，第一次遍历求出 x平均值和y平均值，第二次遍历才能出现结果，这里提供另外一个计算公式，能够计算皮尔逊相关系数的近似值：\n![这里写图片描述](http://img.blog.csdn.net/20170920114326883?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n---\n\n# 余弦相似度／稀疏数据\n假设这样一个数据集，一个在线音乐网站，有10000w首音乐（这里不考虑音乐类型，年代等因素），每个用户常听的也就其中的几十首，这种情况下使用曼哈顿或者欧几里得或者皮尔逊相关系数进行计算用户之间相似性，计算相似值会非常小，因为用户之间的交集本来就很少，这样对于计算结果来讲是很不准确的，这个时候就需要余弦相似度了，余弦相似度进行计算时会自动略过这些非零值。\n\n# 总结\n这里只是简答的介绍了这几种相似性距离度量的方法和场景，但是在实际环境中远比这个复杂许多。这里总结下：\n\n- 如果数据存在“分数膨胀“问题，就使用皮尔逊相关系数\n- 如果数据比较密集，变量之间基本都存在共有值，且这些距离数据都是非常重要的，那就使用欧几里得或者曼哈顿距离\n- 如果数据是稀疏的，就使用余弦相似度\n\n","slug":"机器学习/几种距离计算公式在数据挖掘中的应用场景分析","published":1,"updated":"2017-12-19T02:54:56.787Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjbd1r269000rkxuw12stlgj9","content":"<p>本文涉及以下几种距离计算公式的分析，参考资料为《面向程序员的数据挖掘指南》</p>\n<ul>\n<li>曼哈顿距离</li>\n<li>欧几里得距离</li>\n<li>闵可夫斯基距离</li>\n<li>皮尔逊相关系数</li>\n<li>余弦相似度</li>\n</ul>\n<a id=\"more\"></a>\n<p>之前整理过一篇关于距离相关的文章：<a href=\"\">机器学习算法中的距离和相似性计算公式，分析以及python实现</a></p>\n<h1 id=\"闵可夫斯基距离\"><a href=\"#闵可夫斯基距离\" class=\"headerlink\" title=\"闵可夫斯基距离\"></a>闵可夫斯基距离</h1><p>两个n维变量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的闵可夫斯基距离定义为：</p>\n<script type=\"math/tex; mode=display\">\\sqrt[p]{ \\sum_{k=1}^{n} \\left | x_{1k}-x_{2k} \\right |^p}</script><p>其中p是一个变参数。</p>\n<p>当p=1时，就是曼哈顿距离</p>\n<p>当p=2时，就是欧氏距离</p>\n<p>当p→∞时，就是切比雪夫距离</p>\n<p>根据变参数的不同，闵氏距离可以表示一类的距离。</p>\n<p>p值越大，单个维度的差值大小会对整体距离有更大的影响</p>\n<h1 id=\"曼哈顿距离／欧几里得距离的瑕疵\"><a href=\"#曼哈顿距离／欧几里得距离的瑕疵\" class=\"headerlink\" title=\"曼哈顿距离／欧几里得距离的瑕疵\"></a>曼哈顿距离／欧几里得距离的瑕疵</h1><p>在《面向程序员的数据挖掘指南》中给出了这样一组样例数据, 下图为一个在线音乐网站的的用户评分情况，用户可以用1-5星来评价一个乐队，下边是8位用户对8个乐队的评价：<br><img src=\"http://img.blog.csdn.net/20170920102356159?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"这里写图片描述\"></p>\n<p>表中的横线表示用户没有对乐队进行评价，我们在计算两个用户的距离时，只采用他们都评价过的乐队。</p>\n<p>现在来求Angelica和Bill的距离，因为他们共同评分过的乐队有5个，所以使用其对该5个乐队的评分进行曼哈顿距离的计算为：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Dis_1 = |3.5-2| + |2-3.5| + |5-2| + |1.5-3.5| + |2-3| = 9</span><br></pre></td></tr></table></figure>\n<p>同样使用欧式距离计算为：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Dis_2 = sqrt( (3.5-2)^2 + (2-3.5)^2 + (5-2)^2 + (1.5-3.5)^2 + (2-3)^2 ) = 4.3</span><br></pre></td></tr></table></figure></p>\n<p>当对Angelica和Bill，Bill和Chan进行距离对比时，由于两者的共同评分过的乐队均为5，数据都在一个5维空间里，是公平的，如果现在要计算Angelica和Hailey与Bill的距离时，会发现，Angelica与Bill共同评分的有5个乐队，Hailey与Bill共同评分的有3个乐队，也就是说两者数据一个在5维空间里，一个在三维空间里，这样明显是不公平的。这将会对我们进行计算时产生不好的影响，所以曼哈顿距离和欧几里得距离在数据完整的情况下效果最好。</p>\n<hr>\n<h1 id=\"用户问题／皮尔逊相关系数／分数膨胀\"><a href=\"#用户问题／皮尔逊相关系数／分数膨胀\" class=\"headerlink\" title=\"用户问题／皮尔逊相关系数／分数膨胀\"></a>用户问题／皮尔逊相关系数／分数膨胀</h1><h2 id=\"现象——用户问题\"><a href=\"#现象——用户问题\" class=\"headerlink\" title=\"现象——用户问题\"></a>现象——用户问题</h2><p>仔细观察用户对乐队的评分数据，可以发现每个用户的评分标准不同：</p>\n<ul>\n<li>Bill没有打出极端的分数，都在2-4分之间</li>\n<li>Jordyn似乎喜欢所有的乐队，打分都在4-5之间</li>\n<li>Hailey是一个有趣的人，他的评分不是1就是4</li>\n</ul>\n<p>那么如何比较这些用户呢？比如说Hailey的4分是相当于Jordyn的4分还是5分呢？我觉得更接近5分，这样一来，就影响推荐系统的准确性了！</p>\n<h2 id=\"解决该现象\"><a href=\"#解决该现象\" class=\"headerlink\" title=\"解决该现象\"></a>解决该现象</h2><p>解决该现象的办法之一就是 使用皮尔逊相关系数，例如下边这样的数据样例（Clara和Robert对五个乐队的评分）：</p>\n<p><img src=\"http://img.blog.csdn.net/20170920111425007?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"这里写图片描述\"></p>\n<p>这种现象在数据挖掘领域被称为“分数膨胀“。我们将其评分画成图，如下：<br><img src=\"http://img.blog.csdn.net/20170920112804525?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"这里写图片描述\"></p>\n<p>一条直线-完全吻合，代表着Clara和Robert的喜好完全一致。</p>\n<p>皮尔逊相关系数用于衡量两个变量之间的相关性，他的值在-1～1，1代表完全一致，-1代表完全相悖。所以我们可以利用皮尔逊相关系数来找到相似的用户。</p>\n<p>皮尔逊相关系数的计算公式为：<br><img src=\"http://img.blog.csdn.net/20170920114015874?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"这里写图片描述\"><br>该公式除了看起来比较复杂，另外需要对数据进行两次遍历，第一次遍历求出 x平均值和y平均值，第二次遍历才能出现结果，这里提供另外一个计算公式，能够计算皮尔逊相关系数的近似值：<br><img src=\"http://img.blog.csdn.net/20170920114326883?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"这里写图片描述\"></p>\n<hr>\n<h1 id=\"余弦相似度／稀疏数据\"><a href=\"#余弦相似度／稀疏数据\" class=\"headerlink\" title=\"余弦相似度／稀疏数据\"></a>余弦相似度／稀疏数据</h1><p>假设这样一个数据集，一个在线音乐网站，有10000w首音乐（这里不考虑音乐类型，年代等因素），每个用户常听的也就其中的几十首，这种情况下使用曼哈顿或者欧几里得或者皮尔逊相关系数进行计算用户之间相似性，计算相似值会非常小，因为用户之间的交集本来就很少，这样对于计算结果来讲是很不准确的，这个时候就需要余弦相似度了，余弦相似度进行计算时会自动略过这些非零值。</p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>这里只是简答的介绍了这几种相似性距离度量的方法和场景，但是在实际环境中远比这个复杂许多。这里总结下：</p>\n<ul>\n<li>如果数据存在“分数膨胀“问题，就使用皮尔逊相关系数</li>\n<li>如果数据比较密集，变量之间基本都存在共有值，且这些距离数据都是非常重要的，那就使用欧几里得或者曼哈顿距离</li>\n<li>如果数据是稀疏的，就使用余弦相似度</li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>本文涉及以下几种距离计算公式的分析，参考资料为《面向程序员的数据挖掘指南》</p>\n<ul>\n<li>曼哈顿距离</li>\n<li>欧几里得距离</li>\n<li>闵可夫斯基距离</li>\n<li>皮尔逊相关系数</li>\n<li>余弦相似度</li>\n</ul>","more":"<p>之前整理过一篇关于距离相关的文章：<a href=\"\">机器学习算法中的距离和相似性计算公式，分析以及python实现</a></p>\n<h1 id=\"闵可夫斯基距离\"><a href=\"#闵可夫斯基距离\" class=\"headerlink\" title=\"闵可夫斯基距离\"></a>闵可夫斯基距离</h1><p>两个n维变量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的闵可夫斯基距离定义为：</p>\n<script type=\"math/tex; mode=display\">\\sqrt[p]{ \\sum_{k=1}^{n} \\left | x_{1k}-x_{2k} \\right |^p}</script><p>其中p是一个变参数。</p>\n<p>当p=1时，就是曼哈顿距离</p>\n<p>当p=2时，就是欧氏距离</p>\n<p>当p→∞时，就是切比雪夫距离</p>\n<p>根据变参数的不同，闵氏距离可以表示一类的距离。</p>\n<p>p值越大，单个维度的差值大小会对整体距离有更大的影响</p>\n<h1 id=\"曼哈顿距离／欧几里得距离的瑕疵\"><a href=\"#曼哈顿距离／欧几里得距离的瑕疵\" class=\"headerlink\" title=\"曼哈顿距离／欧几里得距离的瑕疵\"></a>曼哈顿距离／欧几里得距离的瑕疵</h1><p>在《面向程序员的数据挖掘指南》中给出了这样一组样例数据, 下图为一个在线音乐网站的的用户评分情况，用户可以用1-5星来评价一个乐队，下边是8位用户对8个乐队的评价：<br><img src=\"http://img.blog.csdn.net/20170920102356159?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"这里写图片描述\"></p>\n<p>表中的横线表示用户没有对乐队进行评价，我们在计算两个用户的距离时，只采用他们都评价过的乐队。</p>\n<p>现在来求Angelica和Bill的距离，因为他们共同评分过的乐队有5个，所以使用其对该5个乐队的评分进行曼哈顿距离的计算为：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Dis_1 = |3.5-2| + |2-3.5| + |5-2| + |1.5-3.5| + |2-3| = 9</span><br></pre></td></tr></table></figure>\n<p>同样使用欧式距离计算为：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Dis_2 = sqrt( (3.5-2)^2 + (2-3.5)^2 + (5-2)^2 + (1.5-3.5)^2 + (2-3)^2 ) = 4.3</span><br></pre></td></tr></table></figure></p>\n<p>当对Angelica和Bill，Bill和Chan进行距离对比时，由于两者的共同评分过的乐队均为5，数据都在一个5维空间里，是公平的，如果现在要计算Angelica和Hailey与Bill的距离时，会发现，Angelica与Bill共同评分的有5个乐队，Hailey与Bill共同评分的有3个乐队，也就是说两者数据一个在5维空间里，一个在三维空间里，这样明显是不公平的。这将会对我们进行计算时产生不好的影响，所以曼哈顿距离和欧几里得距离在数据完整的情况下效果最好。</p>\n<hr>\n<h1 id=\"用户问题／皮尔逊相关系数／分数膨胀\"><a href=\"#用户问题／皮尔逊相关系数／分数膨胀\" class=\"headerlink\" title=\"用户问题／皮尔逊相关系数／分数膨胀\"></a>用户问题／皮尔逊相关系数／分数膨胀</h1><h2 id=\"现象——用户问题\"><a href=\"#现象——用户问题\" class=\"headerlink\" title=\"现象——用户问题\"></a>现象——用户问题</h2><p>仔细观察用户对乐队的评分数据，可以发现每个用户的评分标准不同：</p>\n<ul>\n<li>Bill没有打出极端的分数，都在2-4分之间</li>\n<li>Jordyn似乎喜欢所有的乐队，打分都在4-5之间</li>\n<li>Hailey是一个有趣的人，他的评分不是1就是4</li>\n</ul>\n<p>那么如何比较这些用户呢？比如说Hailey的4分是相当于Jordyn的4分还是5分呢？我觉得更接近5分，这样一来，就影响推荐系统的准确性了！</p>\n<h2 id=\"解决该现象\"><a href=\"#解决该现象\" class=\"headerlink\" title=\"解决该现象\"></a>解决该现象</h2><p>解决该现象的办法之一就是 使用皮尔逊相关系数，例如下边这样的数据样例（Clara和Robert对五个乐队的评分）：</p>\n<p><img src=\"http://img.blog.csdn.net/20170920111425007?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"这里写图片描述\"></p>\n<p>这种现象在数据挖掘领域被称为“分数膨胀“。我们将其评分画成图，如下：<br><img src=\"http://img.blog.csdn.net/20170920112804525?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"这里写图片描述\"></p>\n<p>一条直线-完全吻合，代表着Clara和Robert的喜好完全一致。</p>\n<p>皮尔逊相关系数用于衡量两个变量之间的相关性，他的值在-1～1，1代表完全一致，-1代表完全相悖。所以我们可以利用皮尔逊相关系数来找到相似的用户。</p>\n<p>皮尔逊相关系数的计算公式为：<br><img src=\"http://img.blog.csdn.net/20170920114015874?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"这里写图片描述\"><br>该公式除了看起来比较复杂，另外需要对数据进行两次遍历，第一次遍历求出 x平均值和y平均值，第二次遍历才能出现结果，这里提供另外一个计算公式，能够计算皮尔逊相关系数的近似值：<br><img src=\"http://img.blog.csdn.net/20170920114326883?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"这里写图片描述\"></p>\n<hr>\n<h1 id=\"余弦相似度／稀疏数据\"><a href=\"#余弦相似度／稀疏数据\" class=\"headerlink\" title=\"余弦相似度／稀疏数据\"></a>余弦相似度／稀疏数据</h1><p>假设这样一个数据集，一个在线音乐网站，有10000w首音乐（这里不考虑音乐类型，年代等因素），每个用户常听的也就其中的几十首，这种情况下使用曼哈顿或者欧几里得或者皮尔逊相关系数进行计算用户之间相似性，计算相似值会非常小，因为用户之间的交集本来就很少，这样对于计算结果来讲是很不准确的，这个时候就需要余弦相似度了，余弦相似度进行计算时会自动略过这些非零值。</p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>这里只是简答的介绍了这几种相似性距离度量的方法和场景，但是在实际环境中远比这个复杂许多。这里总结下：</p>\n<ul>\n<li>如果数据存在“分数膨胀“问题，就使用皮尔逊相关系数</li>\n<li>如果数据比较密集，变量之间基本都存在共有值，且这些距离数据都是非常重要的，那就使用欧几里得或者曼哈顿距离</li>\n<li>如果数据是稀疏的，就使用余弦相似度</li>\n</ul>"},{"title":"回归分析之Sklearn实现电力预测","date":"2017-11-07T05:39:15.000Z","_content":"\n参考原文：http://www.cnblogs.com/pinard/p/6016029.html\n这里进行了手动实现，增强记忆。\n<!--More-->\n# 1：数据集介绍\n使用的数据是UCI大学公开的机器学习数据\n\n数据的介绍在这： http://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant\n\n数据的下载地址在这：http://archive.ics.uci.edu/ml/machine-learning-databases/00294/\n\n里面是一个循环发电场的数据，共有9568个样本数据，每个数据有5列，分别是:AT（温度）, V（压力）, AP（湿度）, RH（压强）, PE（输出电力)。我们不用纠结于每项具体的意思。\n\n我们的问题是得到一个线性的关系，对应PE是样本输出，而AT/V/AP/RH这4个是样本特征， 机器学习的目的就是得到一个线性回归模型，即:\n\n$$\nPE = \\theta _{0} + \\theta _{0} * AT + \\theta _{0} * V +\\theta _{0} * AP +\\theta _{0}*RH\n$$\n\n而需要学习的，就是θ0,θ1,θ2,θ3,θ4这5个参数。\n\n---\n# 2：准备数据\n下载源数据之后，解压会得到一个xlsx的文件，打开另存为csv文件，数据已经整理好，没有非法数据，但是数据并没有进行归一化，不过这里我们可以使用sklearn来帮我处理\n\nsklearn的归一化处理参考：http://blog.csdn.net/gamer_gyt/article/details/77761884\n\n---\n\n# 3：使用pandas来进行数据的读取\n\n```\nimport pandas as pd\n# pandas 读取数据\ndata = pd.read_csv(\"Folds5x2_pp.csv\")\ndata.head()\n```\n然后会看到如下结果，说明数据读取成功：\n\n```\n\tAT\tV\tAP\tRH\tPE\n0\t8.34\t40.77\t1010.84\t90.01\t480.48\n1\t23.64\t58.49\t1011.40\t74.20\t445.75\n2\t29.74\t56.90\t1007.15\t41.91\t438.76\n3\t19.07\t49.69\t1007.22\t76.79\t453.09\n4\t11.80\t40.66\t1017.13\t97.20\t464.43\n```\n\n---\n\n# 4：准备运行算法的数据\n```\nX = data[[\"AT\",\"V\",\"AP\",\"RH\"]]\nprint X.shape\ny = data[[\"PE\"]]\nprint y.shape\n```\n\n```\n(9568, 4)\n(9568, 1)\n```\n\n说明有9658条数据，其中\"AT\",\"V\",\"AP\",\"RH\" 四列作为样本特征，\"PE\"列作为样本输出。\n\n---\n# 5：划分训练集和测试集\n\n```\nfrom sklearn.cross_validation import train_test_split\n\n# 划分训练集和测试集\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=1)\nprint X_train.shape\nprint y_train.shape\nprint X_test.shape\nprint y_test.shape\n```\n```\n(7176, 4)\n(7176, 1)\n(2392, 4)\n(2392, 1)\n```\n75%的数据被划分为训练集，25的数据划分为测试集。\n\n---\n# 6：运行sklearn 线性模型\n```\nfrom sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nlinreg.fit(X_train,y_train)\n\n# 训练模型完毕，查看结果\nprint linreg.intercept_\nprint linreg.coef_\n```\n\n```\n[ 447.06297099]\n[[-1.97376045 -0.23229086  0.0693515  -0.15806957]]\n```\n\n即我们得到的模型结果为：\n$$\nPE = 447.06297099 - 1.97376045*AT - 0.23229086*V + 0.0693515*AP -0.15806957*RH\n$$\n\n---\n# 7：模型评价\n我们需要评价模型的好坏，通常对于线性回归来讲，我么一般使用均方差（MSE，Mean Squared Error）或者均方根差（RMSE，Root Mean Squared Error）来评价模型的好坏\n\n```\ny_pred = linreg.predict(X_test)\nfrom sklearn import metrics\n\n# 使用sklearn来计算mse和Rmse\nprint \"MSE:\",metrics.mean_squared_error(y_test, y_pred)\nprint \"RMSE:\",np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n```\n\n```\nMSE: 20.0804012021\nRMSE: 4.48111606657\n```\n得到了MSE或者RMSE，如果我们用其他方法得到了不同的系数，需要选择模型时，就用MSE小的时候对应的参数。\n\n---\n# 8：交叉验证\n\n我们可以通过交叉验证来持续优化模型，代码如下，我们采用10折交叉验证，即cross_val_predict中的cv参数为10：\n\n```\n# 交叉验证\nfrom sklearn.model_selection import cross_val_predict\npredicted = cross_val_predict(linreg,X,y,cv=10)\nprint \"MSE:\",metrics.mean_squared_error(y, predicted)\nprint \"RMSE:\",np.sqrt(metrics.mean_squared_error(y, predicted))\n```\n\n```\nMSE: 20.7955974619\nRMSE: 4.56021901469\n```\n\n可以看出，采用交叉验证模型的MSE比第6节的大，主要原因是我们这里是对所有折的样本做测试集对应的预测值的MSE，而第6节仅仅对25%的测试集做了MSE。两者的先决条件并不同。\n\n---\n# 9：画图查看结果\n```\n# 画图查看结果\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nax.scatter(y, predicted)\nax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\nax.set_xlabel('Measured')\nax.set_ylabel('Predicted')\nplt.show()\n```\n![这里写图片描述](http://img.blog.csdn.net/20171107133222238?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n","source":"_posts/机器学习/回归分析之Sklearn实现电力预测.md","raw":"---\ntitle: 回归分析之Sklearn实现电力预测\ndate: 2017-11-07 13:39:15\ntags: [回归分析,sklearn]\ncategories: 技术篇\n---\n\n参考原文：http://www.cnblogs.com/pinard/p/6016029.html\n这里进行了手动实现，增强记忆。\n<!--More-->\n# 1：数据集介绍\n使用的数据是UCI大学公开的机器学习数据\n\n数据的介绍在这： http://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant\n\n数据的下载地址在这：http://archive.ics.uci.edu/ml/machine-learning-databases/00294/\n\n里面是一个循环发电场的数据，共有9568个样本数据，每个数据有5列，分别是:AT（温度）, V（压力）, AP（湿度）, RH（压强）, PE（输出电力)。我们不用纠结于每项具体的意思。\n\n我们的问题是得到一个线性的关系，对应PE是样本输出，而AT/V/AP/RH这4个是样本特征， 机器学习的目的就是得到一个线性回归模型，即:\n\n$$\nPE = \\theta _{0} + \\theta _{0} * AT + \\theta _{0} * V +\\theta _{0} * AP +\\theta _{0}*RH\n$$\n\n而需要学习的，就是θ0,θ1,θ2,θ3,θ4这5个参数。\n\n---\n# 2：准备数据\n下载源数据之后，解压会得到一个xlsx的文件，打开另存为csv文件，数据已经整理好，没有非法数据，但是数据并没有进行归一化，不过这里我们可以使用sklearn来帮我处理\n\nsklearn的归一化处理参考：http://blog.csdn.net/gamer_gyt/article/details/77761884\n\n---\n\n# 3：使用pandas来进行数据的读取\n\n```\nimport pandas as pd\n# pandas 读取数据\ndata = pd.read_csv(\"Folds5x2_pp.csv\")\ndata.head()\n```\n然后会看到如下结果，说明数据读取成功：\n\n```\n\tAT\tV\tAP\tRH\tPE\n0\t8.34\t40.77\t1010.84\t90.01\t480.48\n1\t23.64\t58.49\t1011.40\t74.20\t445.75\n2\t29.74\t56.90\t1007.15\t41.91\t438.76\n3\t19.07\t49.69\t1007.22\t76.79\t453.09\n4\t11.80\t40.66\t1017.13\t97.20\t464.43\n```\n\n---\n\n# 4：准备运行算法的数据\n```\nX = data[[\"AT\",\"V\",\"AP\",\"RH\"]]\nprint X.shape\ny = data[[\"PE\"]]\nprint y.shape\n```\n\n```\n(9568, 4)\n(9568, 1)\n```\n\n说明有9658条数据，其中\"AT\",\"V\",\"AP\",\"RH\" 四列作为样本特征，\"PE\"列作为样本输出。\n\n---\n# 5：划分训练集和测试集\n\n```\nfrom sklearn.cross_validation import train_test_split\n\n# 划分训练集和测试集\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=1)\nprint X_train.shape\nprint y_train.shape\nprint X_test.shape\nprint y_test.shape\n```\n```\n(7176, 4)\n(7176, 1)\n(2392, 4)\n(2392, 1)\n```\n75%的数据被划分为训练集，25的数据划分为测试集。\n\n---\n# 6：运行sklearn 线性模型\n```\nfrom sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nlinreg.fit(X_train,y_train)\n\n# 训练模型完毕，查看结果\nprint linreg.intercept_\nprint linreg.coef_\n```\n\n```\n[ 447.06297099]\n[[-1.97376045 -0.23229086  0.0693515  -0.15806957]]\n```\n\n即我们得到的模型结果为：\n$$\nPE = 447.06297099 - 1.97376045*AT - 0.23229086*V + 0.0693515*AP -0.15806957*RH\n$$\n\n---\n# 7：模型评价\n我们需要评价模型的好坏，通常对于线性回归来讲，我么一般使用均方差（MSE，Mean Squared Error）或者均方根差（RMSE，Root Mean Squared Error）来评价模型的好坏\n\n```\ny_pred = linreg.predict(X_test)\nfrom sklearn import metrics\n\n# 使用sklearn来计算mse和Rmse\nprint \"MSE:\",metrics.mean_squared_error(y_test, y_pred)\nprint \"RMSE:\",np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n```\n\n```\nMSE: 20.0804012021\nRMSE: 4.48111606657\n```\n得到了MSE或者RMSE，如果我们用其他方法得到了不同的系数，需要选择模型时，就用MSE小的时候对应的参数。\n\n---\n# 8：交叉验证\n\n我们可以通过交叉验证来持续优化模型，代码如下，我们采用10折交叉验证，即cross_val_predict中的cv参数为10：\n\n```\n# 交叉验证\nfrom sklearn.model_selection import cross_val_predict\npredicted = cross_val_predict(linreg,X,y,cv=10)\nprint \"MSE:\",metrics.mean_squared_error(y, predicted)\nprint \"RMSE:\",np.sqrt(metrics.mean_squared_error(y, predicted))\n```\n\n```\nMSE: 20.7955974619\nRMSE: 4.56021901469\n```\n\n可以看出，采用交叉验证模型的MSE比第6节的大，主要原因是我们这里是对所有折的样本做测试集对应的预测值的MSE，而第6节仅仅对25%的测试集做了MSE。两者的先决条件并不同。\n\n---\n# 9：画图查看结果\n```\n# 画图查看结果\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nax.scatter(y, predicted)\nax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\nax.set_xlabel('Measured')\nax.set_ylabel('Predicted')\nplt.show()\n```\n![这里写图片描述](http://img.blog.csdn.net/20171107133222238?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n","slug":"机器学习/回归分析之Sklearn实现电力预测","published":1,"updated":"2017-12-19T02:54:56.787Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjbd1r26c000tkxuwbq4jhgt4","content":"<p>参考原文：<a href=\"http://www.cnblogs.com/pinard/p/6016029.html\" target=\"_blank\" rel=\"external\">http://www.cnblogs.com/pinard/p/6016029.html</a><br>这里进行了手动实现，增强记忆。<br><a id=\"more\"></a></p>\n<h1 id=\"1：数据集介绍\"><a href=\"#1：数据集介绍\" class=\"headerlink\" title=\"1：数据集介绍\"></a>1：数据集介绍</h1><p>使用的数据是UCI大学公开的机器学习数据</p>\n<p>数据的介绍在这： <a href=\"http://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant\" target=\"_blank\" rel=\"external\">http://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant</a></p>\n<p>数据的下载地址在这：<a href=\"http://archive.ics.uci.edu/ml/machine-learning-databases/00294/\" target=\"_blank\" rel=\"external\">http://archive.ics.uci.edu/ml/machine-learning-databases/00294/</a></p>\n<p>里面是一个循环发电场的数据，共有9568个样本数据，每个数据有5列，分别是:AT（温度）, V（压力）, AP（湿度）, RH（压强）, PE（输出电力)。我们不用纠结于每项具体的意思。</p>\n<p>我们的问题是得到一个线性的关系，对应PE是样本输出，而AT/V/AP/RH这4个是样本特征， 机器学习的目的就是得到一个线性回归模型，即:</p>\n<script type=\"math/tex; mode=display\">\nPE = \\theta _{0} + \\theta _{0} * AT + \\theta _{0} * V +\\theta _{0} * AP +\\theta _{0}*RH</script><p>而需要学习的，就是θ0,θ1,θ2,θ3,θ4这5个参数。</p>\n<hr>\n<h1 id=\"2：准备数据\"><a href=\"#2：准备数据\" class=\"headerlink\" title=\"2：准备数据\"></a>2：准备数据</h1><p>下载源数据之后，解压会得到一个xlsx的文件，打开另存为csv文件，数据已经整理好，没有非法数据，但是数据并没有进行归一化，不过这里我们可以使用sklearn来帮我处理</p>\n<p>sklearn的归一化处理参考：<a href=\"http://blog.csdn.net/gamer_gyt/article/details/77761884\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/gamer_gyt/article/details/77761884</a></p>\n<hr>\n<h1 id=\"3：使用pandas来进行数据的读取\"><a href=\"#3：使用pandas来进行数据的读取\" class=\"headerlink\" title=\"3：使用pandas来进行数据的读取\"></a>3：使用pandas来进行数据的读取</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import pandas as pd</span><br><span class=\"line\"># pandas 读取数据</span><br><span class=\"line\">data = pd.read_csv(&quot;Folds5x2_pp.csv&quot;)</span><br><span class=\"line\">data.head()</span><br></pre></td></tr></table></figure>\n<p>然后会看到如下结果，说明数据读取成功：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\tAT\tV\tAP\tRH\tPE</span><br><span class=\"line\">0\t8.34\t40.77\t1010.84\t90.01\t480.48</span><br><span class=\"line\">1\t23.64\t58.49\t1011.40\t74.20\t445.75</span><br><span class=\"line\">2\t29.74\t56.90\t1007.15\t41.91\t438.76</span><br><span class=\"line\">3\t19.07\t49.69\t1007.22\t76.79\t453.09</span><br><span class=\"line\">4\t11.80\t40.66\t1017.13\t97.20\t464.43</span><br></pre></td></tr></table></figure>\n<hr>\n<h1 id=\"4：准备运行算法的数据\"><a href=\"#4：准备运行算法的数据\" class=\"headerlink\" title=\"4：准备运行算法的数据\"></a>4：准备运行算法的数据</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = data[[&quot;AT&quot;,&quot;V&quot;,&quot;AP&quot;,&quot;RH&quot;]]</span><br><span class=\"line\">print X.shape</span><br><span class=\"line\">y = data[[&quot;PE&quot;]]</span><br><span class=\"line\">print y.shape</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(9568, 4)</span><br><span class=\"line\">(9568, 1)</span><br></pre></td></tr></table></figure>\n<p>说明有9658条数据，其中”AT”,”V”,”AP”,”RH” 四列作为样本特征，”PE”列作为样本输出。</p>\n<hr>\n<h1 id=\"5：划分训练集和测试集\"><a href=\"#5：划分训练集和测试集\" class=\"headerlink\" title=\"5：划分训练集和测试集\"></a>5：划分训练集和测试集</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.cross_validation import train_test_split</span><br><span class=\"line\"></span><br><span class=\"line\"># 划分训练集和测试集</span><br><span class=\"line\">X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=1)</span><br><span class=\"line\">print X_train.shape</span><br><span class=\"line\">print y_train.shape</span><br><span class=\"line\">print X_test.shape</span><br><span class=\"line\">print y_test.shape</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(7176, 4)</span><br><span class=\"line\">(7176, 1)</span><br><span class=\"line\">(2392, 4)</span><br><span class=\"line\">(2392, 1)</span><br></pre></td></tr></table></figure>\n<p>75%的数据被划分为训练集，25的数据划分为测试集。</p>\n<hr>\n<h1 id=\"6：运行sklearn-线性模型\"><a href=\"#6：运行sklearn-线性模型\" class=\"headerlink\" title=\"6：运行sklearn 线性模型\"></a>6：运行sklearn 线性模型</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.linear_model import LinearRegression</span><br><span class=\"line\"></span><br><span class=\"line\">linreg = LinearRegression()</span><br><span class=\"line\">linreg.fit(X_train,y_train)</span><br><span class=\"line\"></span><br><span class=\"line\"># 训练模型完毕，查看结果</span><br><span class=\"line\">print linreg.intercept_</span><br><span class=\"line\">print linreg.coef_</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[ 447.06297099]</span><br><span class=\"line\">[[-1.97376045 -0.23229086  0.0693515  -0.15806957]]</span><br></pre></td></tr></table></figure>\n<p>即我们得到的模型结果为：</p>\n<script type=\"math/tex; mode=display\">\nPE = 447.06297099 - 1.97376045*AT - 0.23229086*V + 0.0693515*AP -0.15806957*RH</script><hr>\n<h1 id=\"7：模型评价\"><a href=\"#7：模型评价\" class=\"headerlink\" title=\"7：模型评价\"></a>7：模型评价</h1><p>我们需要评价模型的好坏，通常对于线性回归来讲，我么一般使用均方差（MSE，Mean Squared Error）或者均方根差（RMSE，Root Mean Squared Error）来评价模型的好坏</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y_pred = linreg.predict(X_test)</span><br><span class=\"line\">from sklearn import metrics</span><br><span class=\"line\"></span><br><span class=\"line\"># 使用sklearn来计算mse和Rmse</span><br><span class=\"line\">print &quot;MSE:&quot;,metrics.mean_squared_error(y_test, y_pred)</span><br><span class=\"line\">print &quot;RMSE:&quot;,np.sqrt(metrics.mean_squared_error(y_test, y_pred))</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MSE: 20.0804012021</span><br><span class=\"line\">RMSE: 4.48111606657</span><br></pre></td></tr></table></figure>\n<p>得到了MSE或者RMSE，如果我们用其他方法得到了不同的系数，需要选择模型时，就用MSE小的时候对应的参数。</p>\n<hr>\n<h1 id=\"8：交叉验证\"><a href=\"#8：交叉验证\" class=\"headerlink\" title=\"8：交叉验证\"></a>8：交叉验证</h1><p>我们可以通过交叉验证来持续优化模型，代码如下，我们采用10折交叉验证，即cross_val_predict中的cv参数为10：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 交叉验证</span><br><span class=\"line\">from sklearn.model_selection import cross_val_predict</span><br><span class=\"line\">predicted = cross_val_predict(linreg,X,y,cv=10)</span><br><span class=\"line\">print &quot;MSE:&quot;,metrics.mean_squared_error(y, predicted)</span><br><span class=\"line\">print &quot;RMSE:&quot;,np.sqrt(metrics.mean_squared_error(y, predicted))</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MSE: 20.7955974619</span><br><span class=\"line\">RMSE: 4.56021901469</span><br></pre></td></tr></table></figure>\n<p>可以看出，采用交叉验证模型的MSE比第6节的大，主要原因是我们这里是对所有折的样本做测试集对应的预测值的MSE，而第6节仅仅对25%的测试集做了MSE。两者的先决条件并不同。</p>\n<hr>\n<h1 id=\"9：画图查看结果\"><a href=\"#9：画图查看结果\" class=\"headerlink\" title=\"9：画图查看结果\"></a>9：画图查看结果</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 画图查看结果</span><br><span class=\"line\">import matplotlib.pyplot as plt</span><br><span class=\"line\">fig, ax = plt.subplots()</span><br><span class=\"line\">ax.scatter(y, predicted)</span><br><span class=\"line\">ax.plot([y.min(), y.max()], [y.min(), y.max()], &apos;k--&apos;, lw=4)</span><br><span class=\"line\">ax.set_xlabel(&apos;Measured&apos;)</span><br><span class=\"line\">ax.set_ylabel(&apos;Predicted&apos;)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://img.blog.csdn.net/20171107133222238?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"这里写图片描述\"></p>\n","site":{"data":{}},"excerpt":"<p>参考原文：<a href=\"http://www.cnblogs.com/pinard/p/6016029.html\" target=\"_blank\" rel=\"external\">http://www.cnblogs.com/pinard/p/6016029.html</a><br>这里进行了手动实现，增强记忆。<br>","more":"</p>\n<h1 id=\"1：数据集介绍\"><a href=\"#1：数据集介绍\" class=\"headerlink\" title=\"1：数据集介绍\"></a>1：数据集介绍</h1><p>使用的数据是UCI大学公开的机器学习数据</p>\n<p>数据的介绍在这： <a href=\"http://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant\" target=\"_blank\" rel=\"external\">http://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant</a></p>\n<p>数据的下载地址在这：<a href=\"http://archive.ics.uci.edu/ml/machine-learning-databases/00294/\" target=\"_blank\" rel=\"external\">http://archive.ics.uci.edu/ml/machine-learning-databases/00294/</a></p>\n<p>里面是一个循环发电场的数据，共有9568个样本数据，每个数据有5列，分别是:AT（温度）, V（压力）, AP（湿度）, RH（压强）, PE（输出电力)。我们不用纠结于每项具体的意思。</p>\n<p>我们的问题是得到一个线性的关系，对应PE是样本输出，而AT/V/AP/RH这4个是样本特征， 机器学习的目的就是得到一个线性回归模型，即:</p>\n<script type=\"math/tex; mode=display\">\nPE = \\theta _{0} + \\theta _{0} * AT + \\theta _{0} * V +\\theta _{0} * AP +\\theta _{0}*RH</script><p>而需要学习的，就是θ0,θ1,θ2,θ3,θ4这5个参数。</p>\n<hr>\n<h1 id=\"2：准备数据\"><a href=\"#2：准备数据\" class=\"headerlink\" title=\"2：准备数据\"></a>2：准备数据</h1><p>下载源数据之后，解压会得到一个xlsx的文件，打开另存为csv文件，数据已经整理好，没有非法数据，但是数据并没有进行归一化，不过这里我们可以使用sklearn来帮我处理</p>\n<p>sklearn的归一化处理参考：<a href=\"http://blog.csdn.net/gamer_gyt/article/details/77761884\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/gamer_gyt/article/details/77761884</a></p>\n<hr>\n<h1 id=\"3：使用pandas来进行数据的读取\"><a href=\"#3：使用pandas来进行数据的读取\" class=\"headerlink\" title=\"3：使用pandas来进行数据的读取\"></a>3：使用pandas来进行数据的读取</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import pandas as pd</span><br><span class=\"line\"># pandas 读取数据</span><br><span class=\"line\">data = pd.read_csv(&quot;Folds5x2_pp.csv&quot;)</span><br><span class=\"line\">data.head()</span><br></pre></td></tr></table></figure>\n<p>然后会看到如下结果，说明数据读取成功：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\tAT\tV\tAP\tRH\tPE</span><br><span class=\"line\">0\t8.34\t40.77\t1010.84\t90.01\t480.48</span><br><span class=\"line\">1\t23.64\t58.49\t1011.40\t74.20\t445.75</span><br><span class=\"line\">2\t29.74\t56.90\t1007.15\t41.91\t438.76</span><br><span class=\"line\">3\t19.07\t49.69\t1007.22\t76.79\t453.09</span><br><span class=\"line\">4\t11.80\t40.66\t1017.13\t97.20\t464.43</span><br></pre></td></tr></table></figure>\n<hr>\n<h1 id=\"4：准备运行算法的数据\"><a href=\"#4：准备运行算法的数据\" class=\"headerlink\" title=\"4：准备运行算法的数据\"></a>4：准备运行算法的数据</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = data[[&quot;AT&quot;,&quot;V&quot;,&quot;AP&quot;,&quot;RH&quot;]]</span><br><span class=\"line\">print X.shape</span><br><span class=\"line\">y = data[[&quot;PE&quot;]]</span><br><span class=\"line\">print y.shape</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(9568, 4)</span><br><span class=\"line\">(9568, 1)</span><br></pre></td></tr></table></figure>\n<p>说明有9658条数据，其中”AT”,”V”,”AP”,”RH” 四列作为样本特征，”PE”列作为样本输出。</p>\n<hr>\n<h1 id=\"5：划分训练集和测试集\"><a href=\"#5：划分训练集和测试集\" class=\"headerlink\" title=\"5：划分训练集和测试集\"></a>5：划分训练集和测试集</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.cross_validation import train_test_split</span><br><span class=\"line\"></span><br><span class=\"line\"># 划分训练集和测试集</span><br><span class=\"line\">X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=1)</span><br><span class=\"line\">print X_train.shape</span><br><span class=\"line\">print y_train.shape</span><br><span class=\"line\">print X_test.shape</span><br><span class=\"line\">print y_test.shape</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(7176, 4)</span><br><span class=\"line\">(7176, 1)</span><br><span class=\"line\">(2392, 4)</span><br><span class=\"line\">(2392, 1)</span><br></pre></td></tr></table></figure>\n<p>75%的数据被划分为训练集，25的数据划分为测试集。</p>\n<hr>\n<h1 id=\"6：运行sklearn-线性模型\"><a href=\"#6：运行sklearn-线性模型\" class=\"headerlink\" title=\"6：运行sklearn 线性模型\"></a>6：运行sklearn 线性模型</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.linear_model import LinearRegression</span><br><span class=\"line\"></span><br><span class=\"line\">linreg = LinearRegression()</span><br><span class=\"line\">linreg.fit(X_train,y_train)</span><br><span class=\"line\"></span><br><span class=\"line\"># 训练模型完毕，查看结果</span><br><span class=\"line\">print linreg.intercept_</span><br><span class=\"line\">print linreg.coef_</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[ 447.06297099]</span><br><span class=\"line\">[[-1.97376045 -0.23229086  0.0693515  -0.15806957]]</span><br></pre></td></tr></table></figure>\n<p>即我们得到的模型结果为：</p>\n<script type=\"math/tex; mode=display\">\nPE = 447.06297099 - 1.97376045*AT - 0.23229086*V + 0.0693515*AP -0.15806957*RH</script><hr>\n<h1 id=\"7：模型评价\"><a href=\"#7：模型评价\" class=\"headerlink\" title=\"7：模型评价\"></a>7：模型评价</h1><p>我们需要评价模型的好坏，通常对于线性回归来讲，我么一般使用均方差（MSE，Mean Squared Error）或者均方根差（RMSE，Root Mean Squared Error）来评价模型的好坏</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y_pred = linreg.predict(X_test)</span><br><span class=\"line\">from sklearn import metrics</span><br><span class=\"line\"></span><br><span class=\"line\"># 使用sklearn来计算mse和Rmse</span><br><span class=\"line\">print &quot;MSE:&quot;,metrics.mean_squared_error(y_test, y_pred)</span><br><span class=\"line\">print &quot;RMSE:&quot;,np.sqrt(metrics.mean_squared_error(y_test, y_pred))</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MSE: 20.0804012021</span><br><span class=\"line\">RMSE: 4.48111606657</span><br></pre></td></tr></table></figure>\n<p>得到了MSE或者RMSE，如果我们用其他方法得到了不同的系数，需要选择模型时，就用MSE小的时候对应的参数。</p>\n<hr>\n<h1 id=\"8：交叉验证\"><a href=\"#8：交叉验证\" class=\"headerlink\" title=\"8：交叉验证\"></a>8：交叉验证</h1><p>我们可以通过交叉验证来持续优化模型，代码如下，我们采用10折交叉验证，即cross_val_predict中的cv参数为10：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 交叉验证</span><br><span class=\"line\">from sklearn.model_selection import cross_val_predict</span><br><span class=\"line\">predicted = cross_val_predict(linreg,X,y,cv=10)</span><br><span class=\"line\">print &quot;MSE:&quot;,metrics.mean_squared_error(y, predicted)</span><br><span class=\"line\">print &quot;RMSE:&quot;,np.sqrt(metrics.mean_squared_error(y, predicted))</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MSE: 20.7955974619</span><br><span class=\"line\">RMSE: 4.56021901469</span><br></pre></td></tr></table></figure>\n<p>可以看出，采用交叉验证模型的MSE比第6节的大，主要原因是我们这里是对所有折的样本做测试集对应的预测值的MSE，而第6节仅仅对25%的测试集做了MSE。两者的先决条件并不同。</p>\n<hr>\n<h1 id=\"9：画图查看结果\"><a href=\"#9：画图查看结果\" class=\"headerlink\" title=\"9：画图查看结果\"></a>9：画图查看结果</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 画图查看结果</span><br><span class=\"line\">import matplotlib.pyplot as plt</span><br><span class=\"line\">fig, ax = plt.subplots()</span><br><span class=\"line\">ax.scatter(y, predicted)</span><br><span class=\"line\">ax.plot([y.min(), y.max()], [y.min(), y.max()], &apos;k--&apos;, lw=4)</span><br><span class=\"line\">ax.set_xlabel(&apos;Measured&apos;)</span><br><span class=\"line\">ax.set_ylabel(&apos;Predicted&apos;)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://img.blog.csdn.net/20171107133222238?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"这里写图片描述\"></p>"},{"title":"回归分析之理论篇","date":"2017-09-17T00:10:27.000Z","_content":"2015年的机器学习博客其实都是看《机器学习实战》这本书时学到的，说实话当时也是知其然，不知其所以然，以至于对其理解不深刻，好多细节和理论知识都搞的是乱七八糟，自从工作之后再去看一个算法，思考的比之前多了点，查看资料也比之前多了点，生怕理解错误，影响其他人，当然在理解的程度上还是不够深刻，这也是一个学习的过程吧，记录一下，欢迎指正。\n\nCSDN链接：[点击阅读](http://blog.csdn.net/gamer_gyt/article/details/78008144)\n<!--More-->\n# 一：一些名词定义\n\n## 1）指数分布族\n指数分布族是指可以表示为指数形式的概率分布。\n$$\nf_X(x\\mid\\theta) = h(x) \\exp \\left (\\eta(\\theta) \\cdot T(x) -A(\\theta)\\right )\n$$\n其中，η为自然参数(nature parameter)，T(x)是充分统计量（sufficient statistic）。当参数A，h，T都固定以后，就定义了一个以η为参数的函数族。\n\n伯努利分布与高斯分布是两个典型的指数分布族\n\n### 伯努利分布\n又名两点分布或者0-1分布，是一个离散型概率分布。假设1的概率为p，0的概率为q，则\n其概率质量函数为：\n```\n{\\displaystyle f_{X}(x)=p^{x}(1-p)^{1-x}=\\left\\{{\\begin{matrix}p&{\\mbox{if }}x=1,\\\\q\\ &{\\mbox{if }}x=0.\\\\\\end{matrix}}\\right.}\n```\n其期望值为：\n$$\n{\\displaystyle \\operatorname {E} [X]=\\sum _{i=0}^{1}x_{i}f_{X}(x)=0+p=p}\n$$\n\n其方差为：\n$$\n{\\displaystyle \\operatorname {var} [X]=\\sum _{i=0}^{1}(x_{i}-E[X])^{2}f_{X}(x)=(0-p)^{2}(1-p)+(1-p)^{2}p=p(1-p)=pq}\n$$\n\n\n### 正态分布(高斯分布)\n若随机变量X服从一个位置参数为 ${\\displaystyle \\mu }$ 、尺度参数为 ${\\displaystyle \\sigma } $ 的概率分布，记为：\n$$\nX \\sim N(\\mu,\\sigma^2),\n$$\n\n其概率密度函数为:\n```\nf(x) = {1 \\over \\sigma\\sqrt{2\\pi} }\\,e^{- {{(x-\\mu )^2 \\over 2\\sigma^2}}}\n```\n\n正态分布的数学期望值或期望值$ {\\displaystyle \\mu } $ 等于位置参数，决定了分布的位置；其方差 $ {\\displaystyle \\sigma ^{2}} $ 的开平方或标准差$ {\\displaystyle \\sigma }$ 等于尺度参数，决定了分布的幅度。\n\n### 标准正态分布：\n\n如果$ {\\displaystyle \\mu =0} $ 并且 $ {\\displaystyle \\sigma =1} $ 则这个正态分布称为标准正态分布。简化为：\n```\nf(x) = \\frac{1}{\\sqrt{2\\pi}} \\, \\exp\\left(-\\frac{x^2}{2} \\right)\n```\n如下图所示：\n\n![image](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Normal_distribution_pdf.png/650px-Normal_distribution_pdf.png)\n\n正态分布中一些值得注意的量：\n\n- 密度函数关于平均值对称\n- 平均值与它的众数（statistical mode）以及中位数（median）同一数值。\n- 函数曲线下68.268949%的面积在平均数左右的一个标准差范围内。\n- 95.449974%的面积在平均数左右两个标准差 $ {\\displaystyle 2\\sigma } $ 的范围内。\n- 99.730020%的面积在平均数左右三个标准差$ {\\displaystyle 3\\sigma } $ 的范围内。\n- 99.993666%的面积在平均数左右四个标准差$ {\\displaystyle 4\\sigma } $ 的范围内。\n- 函数曲线的反曲点（inflection point）为离平均数一个标准差距离的位置。\n\n## 2）多重共线性和完全共线性\n\n多重共线性：指线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真或难以估计准确。一般来说，由于经济数据的限制使得模型设计不当，导致设计矩阵中解释变量间存在普遍的相关关系。通俗点理解就是自变量里边有一些是打酱油的，可以由另外一些变量推导出来，当变量中存在大量的多重共线性变量就会导致模型误差很大，这个时候就需要从自变量中将“打酱油”的变量给剔除掉。\n\n完全共线性：在多元回归中，一个自变量是一个或多个其他自变量的线性函数。\n\n两者在某种特殊情况下是有交集的。\n\n## 3）T检验\nT检验又叫student T 检验，主要用于样本含量小，总标准差 $\\sigma$ 未知的正太分布数据。T检验是用于小样本的两个平均值差异程度的检查方法，他是用T分布理论值来推断事件发生的概率，从而判断两个平均数的差异是否显著。\n参考: http://blog.csdn.net/shulixu/article/details/53354206\n\n\n\n## 4）关系\n- 函数关系\n> 确定性关系，y=3+2x\n- 相关关系\n>非确定性关系，比如说高中时数学成绩好的人，一般物理成绩也好，这是因为它们背后使用的都是数学逻辑，这种酒叫做非确定性关系。\n\n## 5）虚拟变量\n定义：\n>又称虚设变量、名义变量或哑变量，用以反映质的属性的一个人工变量，是量化了的自变量，通常取值为0或1。（通常为离散变量，因子变量）\n\n作用：\n>引入哑变量可使线形回归模型变得更复杂，但对问题描述更简明，一个方程能达到两个方程的作用，而且接近现实。\n\n设置：\n>例如：体重（w）和身高（h），性别（s）的关系，但这里性别并非连续的或者数字可以表示的变量，你并不能拿 1表示男，2表示女，这里的性别是离散变量，只能为男或者女，所以这里就需要引入哑变量来处理。\n性别（s） =》 isman（男1，非男0），iswoman （因为只有两种可能，所以这里只需要引入一个哑变量即可），同理假设这里有另外一个变量肤色（有黑，白，黄三种可能），那么这里只需引入两个哑变量即可（isblack，iswhite），因为不是这两种的话那肯定是黄色皮肤了。\n\n例子：\n针对上边所说的体重和身高，性别的关系。\n\n构建模型：\n- 1）加法模型\n```\nw = a + b * h + c * isman\n```\n针对数据样本而言，性别是确定的，所以 c * isman 的结果不是c就是0，所以在加法模型下，影响的是模型在y轴上的截距。这说明的是针对不同的性别而言，回归方程是平衡的，只不过是截距不一样。\n\n- 2）乘法模型\n```\nw = a + b * h + c * isman * h + d * iswoman * h\n```\n同样针对数据样本而言，性别也是确定的，假设一个男性，isman 为1，iswoman 为0，则上述模型变成了 w = a + b*h + c * h =a + (b+c) * h，这个时候就是在y轴上的截距一样，而斜率不一致。\n\n- 3）混合模型\n```\nw = a + b * h + c * isman + d * iswoman + e * isman * h + f * iswoman * h\n```\n假设一个针对一个性别为男的样本数据，该模型变可以变成 w = a + b*h + c + e * h = a +c + (b+e)*h，这个时候斜率和截距都是不一样的。\n\n# 二：什么是回归（分析）\n回归就是利用样本（已知数据），产生拟合方程，从而（对未知数据）进行预测。比如说我有一组随机变量X（X1，X2，X3...）和另外一组随机变量Y（Y1，Y2，Y3...）,那么研究变量X与Y之间的统计学方法就叫做回归分析。当然这里X和Y是单一对应的，所以这里是一元线性回归。\n\n回归分为线性回归和非线性回归，其中一些非线性回归可以用线性回归的方法来进行分析的叫做==广义线性回归==，接下来我们来了解下每一种回归：\n\n## 1）线性回归\n线性回归可以分为一元线性回归和多元线性回归。当然线性回归中自变量的指数都是1，这里的线性并非真的是指用一条线将数据连起来，也可以是一个二维平面，三维平面等。\n\n一元线性回归：自变量只有一个的回归，比如说北京二环的房子面积（Area）和房子总价（Money）的关系，随着面积（Area）的增大，房屋价格也是不断增长。这里的自变量只有面积，所以这里是一元线性回归。\n\n多元线性回归：自变量大于等于两个，比如说北京二环的房子面积（Area），楼层（floor）和房屋价格（Money）的关系，这里自变量是两个，所以是二元线性回归，三元，多元同理。\n\n## 2）非线性回归\n有一类模型，其回归参数不是线性的，也不能通过转换的方法将其变为线性的参数，这类模型称为非线性回归模型。非线性回归可以分为一元回归和多元回归。非线性回归中至少有一个自变量的指数不为1。回归分析中，当研究的因果关系只涉及因变量和一个自变量时，叫做一元回归分析；当研究的因果关系涉及因变量和两个或两个以上自变量时，叫做多元回归分析。\n\n## 3）广义线性回归\n一些非线性回归可以用线性回归的方法来进行分析叫做广义线性回归。\n典型的代表是Logistic回归。\n\n## 4）如何衡量相关关系既判断适不适合使用线性回归模型？\n使用相关系数（-1，1），绝对值越接近于1，相关系数越高，越适合使用线性回归模型（Rxy>0,代表正相关，Rxy<0,代表负相关）\n\n$$\nr_{XY} = \\frac{ \\sum (X_{i}-\\bar{X})(Y_{i}-\\bar{Y}) }{ \\sqrt{ \\sum (X_{i}-\\bar{X})^2) \\sum (Y_{i}-\\bar{Y})^2) } }\n$$\n\n# 三：回归中困难点\n## 1）选定变量\n> 假设自变量特别多，有一些是和因变量相关的，有一些是和因变量不相关的，这里我们就需要筛选出有用的变量，如果筛选后变量还特别多的话，可以采用降维的方式进行变量缩减（可以参考之前的PCA降维的文章：http://blog.csdn.net/gamer_gyt/article/details/51418069 ，基本是整理《机器学习实战》这本书的笔记）\n\n## 2）发现多重共线性\n(1).方差扩大因子法( VIF)\n\n>一般认为如果最大的VIF超过10，常常表示存在多重共线性。\n\n(2).容差容忍定法\n\n>如果容差（tolerance）<=0.1，常常表示存在多重共线性。\n\n(3). 条件索引\n\n>条件索引(condition index)>10，可以说明存在比较严重的共线性\n\n\n\n## 3）过拟合与欠拟合问题\n过拟合和欠拟合其实对每一个模型来讲都是存在的，过拟合就是模型过于符合训练数据的趋势，欠拟合就是模型对于训练数据和测试数据都表现出不好的情况。针对于欠拟合来讲，是很容易发现的，通常不被讨论。\n\n在进行模型训练的时候，算法要进行不断的学习，模型在训练数据和测试数据上的错误都在不断下降，但是，如果学习的时间过长的话，模型在训练数据集上的表现将会继续下降，这是因为模型已经过拟合，并且学习到了训练数据集中不恰当的细节和噪音，同时，测试集上的错误率开始上升，也是模型泛化能力在下降。\n\n这个完美的临界点就在于测试集中的错误率在上升时，此时训练集和测试集上都有良好的表现。通常有两种手段可以帮助你找到这个完美的临界点：重采样方法和验证集方法。\n\n### 如何限制过拟合？\n> 过拟合和欠拟合可以导致很差的模型表现。但是到目前为止大部分机器学习实际应用时的问题都是过拟合。\n过拟合是个问题因为训练数据上的机器学习算法的评价方法与我们最关心的实际上的评价方法，也就是算法在位置数据上的表现是不一样的。\n当评价机器学习算法时我们有两者重要的技巧来限制过拟合\n使用重采样来评价模型效能\n保留一个验证数据集\n最流行的重采样技术是k折交叉验证。指的是在训练数据的子集上训练和测试模型k次，同时建立对于机器学习模型在未知数据上表现的评估。\n验证集只是训练数据的子集，你把它保留到你进行机器学习算法的最后才使用。在训练数据上选择和调谐机器学习算法之后，我们在验证集上在对于模型进行评估，以便得到一些关于模型在未知数据上的表现的认知。\n\n## 4）检验模型是否合理\n验证目前主要采用如下三类办法：\n1、拟合优度检验\n主要有R^2，t检验，f检验等等\n这三种检验为常规验证，只要在95%的置信度内满足即可说明拟合效果良好。\n2、预测值和真实值比较\n主要是差值和比值，一般差值和比值都不超过5%。\n3、另外的办法\nGEH方法最为常用。GEH是Geoffrey E. Havers于1970年左右提出的一种模型验证方法，其巧妙的运用一个拟定的公式和标准界定模型的拟合优劣。\nGEH=(2(M-C)^2/(M+C))^(1/2)\n其中M是预测值，C是实际观测值\n如果GEH小于5，认为模型拟合效果良好，如果GEH在5-10之间，必须对数据不可靠需要进行检查，如果GEH大于10，说明数据存在问题的几率很高。\nhttp://blog.sina.com.cn/s/blog_66188c300100hl45.html\n\n## 5）线性回归的模型评判\n- 误差平方和（残差平方和）\n\n例如二维平面上的一点（x1，y1），经过线性回归模型预测其值为 y_1，那么预测模型的好与坏就是计算预测结果到直线的距离的大小，由于是一组数据，那么便是这一组数据的和。\n\n点到直线的距离公式为： \n$$\n \\frac{\\left | A_{x_{0}}+B_{y_{0}} +C \\right |}{\\sqrt{A^2 + B^2 }}\n$$\n由于涉及到开方，在计算过程中十分不方便，所以这里转换为纵轴上的差值，即利用预测值与真实值的差进行累加求和，最小时即为最佳的线性回归模型，但是这里涉及到预测值与真实值的差可能为负数，所以这里用平方，所以最终的误差平方和为：\n$$\nRSS = \\sum_{i=1}^{n}(y_{i}- \\hat{y_{i}} )^2 = \\sum_{i=1}^{n}[y_{i} - (\\alpha +\\beta x_{i})]^2\n$$\n\n- AIC准则（赤池信息准则）\n$$\nAIC=n ln (RSSp/n)+2p\n$$\nn为变量总个数，p为选出的变量个数，AIC越小越好\n\n","source":"_posts/机器学习/回归分析之理论篇.md","raw":"---\ntitle: 回归分析之理论篇\ndate: 2017-9-17 08:10:27\ntags: [回归分析,正态分布]\ncategories: 技术篇\n---\n2015年的机器学习博客其实都是看《机器学习实战》这本书时学到的，说实话当时也是知其然，不知其所以然，以至于对其理解不深刻，好多细节和理论知识都搞的是乱七八糟，自从工作之后再去看一个算法，思考的比之前多了点，查看资料也比之前多了点，生怕理解错误，影响其他人，当然在理解的程度上还是不够深刻，这也是一个学习的过程吧，记录一下，欢迎指正。\n\nCSDN链接：[点击阅读](http://blog.csdn.net/gamer_gyt/article/details/78008144)\n<!--More-->\n# 一：一些名词定义\n\n## 1）指数分布族\n指数分布族是指可以表示为指数形式的概率分布。\n$$\nf_X(x\\mid\\theta) = h(x) \\exp \\left (\\eta(\\theta) \\cdot T(x) -A(\\theta)\\right )\n$$\n其中，η为自然参数(nature parameter)，T(x)是充分统计量（sufficient statistic）。当参数A，h，T都固定以后，就定义了一个以η为参数的函数族。\n\n伯努利分布与高斯分布是两个典型的指数分布族\n\n### 伯努利分布\n又名两点分布或者0-1分布，是一个离散型概率分布。假设1的概率为p，0的概率为q，则\n其概率质量函数为：\n```\n{\\displaystyle f_{X}(x)=p^{x}(1-p)^{1-x}=\\left\\{{\\begin{matrix}p&{\\mbox{if }}x=1,\\\\q\\ &{\\mbox{if }}x=0.\\\\\\end{matrix}}\\right.}\n```\n其期望值为：\n$$\n{\\displaystyle \\operatorname {E} [X]=\\sum _{i=0}^{1}x_{i}f_{X}(x)=0+p=p}\n$$\n\n其方差为：\n$$\n{\\displaystyle \\operatorname {var} [X]=\\sum _{i=0}^{1}(x_{i}-E[X])^{2}f_{X}(x)=(0-p)^{2}(1-p)+(1-p)^{2}p=p(1-p)=pq}\n$$\n\n\n### 正态分布(高斯分布)\n若随机变量X服从一个位置参数为 ${\\displaystyle \\mu }$ 、尺度参数为 ${\\displaystyle \\sigma } $ 的概率分布，记为：\n$$\nX \\sim N(\\mu,\\sigma^2),\n$$\n\n其概率密度函数为:\n```\nf(x) = {1 \\over \\sigma\\sqrt{2\\pi} }\\,e^{- {{(x-\\mu )^2 \\over 2\\sigma^2}}}\n```\n\n正态分布的数学期望值或期望值$ {\\displaystyle \\mu } $ 等于位置参数，决定了分布的位置；其方差 $ {\\displaystyle \\sigma ^{2}} $ 的开平方或标准差$ {\\displaystyle \\sigma }$ 等于尺度参数，决定了分布的幅度。\n\n### 标准正态分布：\n\n如果$ {\\displaystyle \\mu =0} $ 并且 $ {\\displaystyle \\sigma =1} $ 则这个正态分布称为标准正态分布。简化为：\n```\nf(x) = \\frac{1}{\\sqrt{2\\pi}} \\, \\exp\\left(-\\frac{x^2}{2} \\right)\n```\n如下图所示：\n\n![image](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Normal_distribution_pdf.png/650px-Normal_distribution_pdf.png)\n\n正态分布中一些值得注意的量：\n\n- 密度函数关于平均值对称\n- 平均值与它的众数（statistical mode）以及中位数（median）同一数值。\n- 函数曲线下68.268949%的面积在平均数左右的一个标准差范围内。\n- 95.449974%的面积在平均数左右两个标准差 $ {\\displaystyle 2\\sigma } $ 的范围内。\n- 99.730020%的面积在平均数左右三个标准差$ {\\displaystyle 3\\sigma } $ 的范围内。\n- 99.993666%的面积在平均数左右四个标准差$ {\\displaystyle 4\\sigma } $ 的范围内。\n- 函数曲线的反曲点（inflection point）为离平均数一个标准差距离的位置。\n\n## 2）多重共线性和完全共线性\n\n多重共线性：指线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真或难以估计准确。一般来说，由于经济数据的限制使得模型设计不当，导致设计矩阵中解释变量间存在普遍的相关关系。通俗点理解就是自变量里边有一些是打酱油的，可以由另外一些变量推导出来，当变量中存在大量的多重共线性变量就会导致模型误差很大，这个时候就需要从自变量中将“打酱油”的变量给剔除掉。\n\n完全共线性：在多元回归中，一个自变量是一个或多个其他自变量的线性函数。\n\n两者在某种特殊情况下是有交集的。\n\n## 3）T检验\nT检验又叫student T 检验，主要用于样本含量小，总标准差 $\\sigma$ 未知的正太分布数据。T检验是用于小样本的两个平均值差异程度的检查方法，他是用T分布理论值来推断事件发生的概率，从而判断两个平均数的差异是否显著。\n参考: http://blog.csdn.net/shulixu/article/details/53354206\n\n\n\n## 4）关系\n- 函数关系\n> 确定性关系，y=3+2x\n- 相关关系\n>非确定性关系，比如说高中时数学成绩好的人，一般物理成绩也好，这是因为它们背后使用的都是数学逻辑，这种酒叫做非确定性关系。\n\n## 5）虚拟变量\n定义：\n>又称虚设变量、名义变量或哑变量，用以反映质的属性的一个人工变量，是量化了的自变量，通常取值为0或1。（通常为离散变量，因子变量）\n\n作用：\n>引入哑变量可使线形回归模型变得更复杂，但对问题描述更简明，一个方程能达到两个方程的作用，而且接近现实。\n\n设置：\n>例如：体重（w）和身高（h），性别（s）的关系，但这里性别并非连续的或者数字可以表示的变量，你并不能拿 1表示男，2表示女，这里的性别是离散变量，只能为男或者女，所以这里就需要引入哑变量来处理。\n性别（s） =》 isman（男1，非男0），iswoman （因为只有两种可能，所以这里只需要引入一个哑变量即可），同理假设这里有另外一个变量肤色（有黑，白，黄三种可能），那么这里只需引入两个哑变量即可（isblack，iswhite），因为不是这两种的话那肯定是黄色皮肤了。\n\n例子：\n针对上边所说的体重和身高，性别的关系。\n\n构建模型：\n- 1）加法模型\n```\nw = a + b * h + c * isman\n```\n针对数据样本而言，性别是确定的，所以 c * isman 的结果不是c就是0，所以在加法模型下，影响的是模型在y轴上的截距。这说明的是针对不同的性别而言，回归方程是平衡的，只不过是截距不一样。\n\n- 2）乘法模型\n```\nw = a + b * h + c * isman * h + d * iswoman * h\n```\n同样针对数据样本而言，性别也是确定的，假设一个男性，isman 为1，iswoman 为0，则上述模型变成了 w = a + b*h + c * h =a + (b+c) * h，这个时候就是在y轴上的截距一样，而斜率不一致。\n\n- 3）混合模型\n```\nw = a + b * h + c * isman + d * iswoman + e * isman * h + f * iswoman * h\n```\n假设一个针对一个性别为男的样本数据，该模型变可以变成 w = a + b*h + c + e * h = a +c + (b+e)*h，这个时候斜率和截距都是不一样的。\n\n# 二：什么是回归（分析）\n回归就是利用样本（已知数据），产生拟合方程，从而（对未知数据）进行预测。比如说我有一组随机变量X（X1，X2，X3...）和另外一组随机变量Y（Y1，Y2，Y3...）,那么研究变量X与Y之间的统计学方法就叫做回归分析。当然这里X和Y是单一对应的，所以这里是一元线性回归。\n\n回归分为线性回归和非线性回归，其中一些非线性回归可以用线性回归的方法来进行分析的叫做==广义线性回归==，接下来我们来了解下每一种回归：\n\n## 1）线性回归\n线性回归可以分为一元线性回归和多元线性回归。当然线性回归中自变量的指数都是1，这里的线性并非真的是指用一条线将数据连起来，也可以是一个二维平面，三维平面等。\n\n一元线性回归：自变量只有一个的回归，比如说北京二环的房子面积（Area）和房子总价（Money）的关系，随着面积（Area）的增大，房屋价格也是不断增长。这里的自变量只有面积，所以这里是一元线性回归。\n\n多元线性回归：自变量大于等于两个，比如说北京二环的房子面积（Area），楼层（floor）和房屋价格（Money）的关系，这里自变量是两个，所以是二元线性回归，三元，多元同理。\n\n## 2）非线性回归\n有一类模型，其回归参数不是线性的，也不能通过转换的方法将其变为线性的参数，这类模型称为非线性回归模型。非线性回归可以分为一元回归和多元回归。非线性回归中至少有一个自变量的指数不为1。回归分析中，当研究的因果关系只涉及因变量和一个自变量时，叫做一元回归分析；当研究的因果关系涉及因变量和两个或两个以上自变量时，叫做多元回归分析。\n\n## 3）广义线性回归\n一些非线性回归可以用线性回归的方法来进行分析叫做广义线性回归。\n典型的代表是Logistic回归。\n\n## 4）如何衡量相关关系既判断适不适合使用线性回归模型？\n使用相关系数（-1，1），绝对值越接近于1，相关系数越高，越适合使用线性回归模型（Rxy>0,代表正相关，Rxy<0,代表负相关）\n\n$$\nr_{XY} = \\frac{ \\sum (X_{i}-\\bar{X})(Y_{i}-\\bar{Y}) }{ \\sqrt{ \\sum (X_{i}-\\bar{X})^2) \\sum (Y_{i}-\\bar{Y})^2) } }\n$$\n\n# 三：回归中困难点\n## 1）选定变量\n> 假设自变量特别多，有一些是和因变量相关的，有一些是和因变量不相关的，这里我们就需要筛选出有用的变量，如果筛选后变量还特别多的话，可以采用降维的方式进行变量缩减（可以参考之前的PCA降维的文章：http://blog.csdn.net/gamer_gyt/article/details/51418069 ，基本是整理《机器学习实战》这本书的笔记）\n\n## 2）发现多重共线性\n(1).方差扩大因子法( VIF)\n\n>一般认为如果最大的VIF超过10，常常表示存在多重共线性。\n\n(2).容差容忍定法\n\n>如果容差（tolerance）<=0.1，常常表示存在多重共线性。\n\n(3). 条件索引\n\n>条件索引(condition index)>10，可以说明存在比较严重的共线性\n\n\n\n## 3）过拟合与欠拟合问题\n过拟合和欠拟合其实对每一个模型来讲都是存在的，过拟合就是模型过于符合训练数据的趋势，欠拟合就是模型对于训练数据和测试数据都表现出不好的情况。针对于欠拟合来讲，是很容易发现的，通常不被讨论。\n\n在进行模型训练的时候，算法要进行不断的学习，模型在训练数据和测试数据上的错误都在不断下降，但是，如果学习的时间过长的话，模型在训练数据集上的表现将会继续下降，这是因为模型已经过拟合，并且学习到了训练数据集中不恰当的细节和噪音，同时，测试集上的错误率开始上升，也是模型泛化能力在下降。\n\n这个完美的临界点就在于测试集中的错误率在上升时，此时训练集和测试集上都有良好的表现。通常有两种手段可以帮助你找到这个完美的临界点：重采样方法和验证集方法。\n\n### 如何限制过拟合？\n> 过拟合和欠拟合可以导致很差的模型表现。但是到目前为止大部分机器学习实际应用时的问题都是过拟合。\n过拟合是个问题因为训练数据上的机器学习算法的评价方法与我们最关心的实际上的评价方法，也就是算法在位置数据上的表现是不一样的。\n当评价机器学习算法时我们有两者重要的技巧来限制过拟合\n使用重采样来评价模型效能\n保留一个验证数据集\n最流行的重采样技术是k折交叉验证。指的是在训练数据的子集上训练和测试模型k次，同时建立对于机器学习模型在未知数据上表现的评估。\n验证集只是训练数据的子集，你把它保留到你进行机器学习算法的最后才使用。在训练数据上选择和调谐机器学习算法之后，我们在验证集上在对于模型进行评估，以便得到一些关于模型在未知数据上的表现的认知。\n\n## 4）检验模型是否合理\n验证目前主要采用如下三类办法：\n1、拟合优度检验\n主要有R^2，t检验，f检验等等\n这三种检验为常规验证，只要在95%的置信度内满足即可说明拟合效果良好。\n2、预测值和真实值比较\n主要是差值和比值，一般差值和比值都不超过5%。\n3、另外的办法\nGEH方法最为常用。GEH是Geoffrey E. Havers于1970年左右提出的一种模型验证方法，其巧妙的运用一个拟定的公式和标准界定模型的拟合优劣。\nGEH=(2(M-C)^2/(M+C))^(1/2)\n其中M是预测值，C是实际观测值\n如果GEH小于5，认为模型拟合效果良好，如果GEH在5-10之间，必须对数据不可靠需要进行检查，如果GEH大于10，说明数据存在问题的几率很高。\nhttp://blog.sina.com.cn/s/blog_66188c300100hl45.html\n\n## 5）线性回归的模型评判\n- 误差平方和（残差平方和）\n\n例如二维平面上的一点（x1，y1），经过线性回归模型预测其值为 y_1，那么预测模型的好与坏就是计算预测结果到直线的距离的大小，由于是一组数据，那么便是这一组数据的和。\n\n点到直线的距离公式为： \n$$\n \\frac{\\left | A_{x_{0}}+B_{y_{0}} +C \\right |}{\\sqrt{A^2 + B^2 }}\n$$\n由于涉及到开方，在计算过程中十分不方便，所以这里转换为纵轴上的差值，即利用预测值与真实值的差进行累加求和，最小时即为最佳的线性回归模型，但是这里涉及到预测值与真实值的差可能为负数，所以这里用平方，所以最终的误差平方和为：\n$$\nRSS = \\sum_{i=1}^{n}(y_{i}- \\hat{y_{i}} )^2 = \\sum_{i=1}^{n}[y_{i} - (\\alpha +\\beta x_{i})]^2\n$$\n\n- AIC准则（赤池信息准则）\n$$\nAIC=n ln (RSSp/n)+2p\n$$\nn为变量总个数，p为选出的变量个数，AIC越小越好\n\n","slug":"机器学习/回归分析之理论篇","published":1,"updated":"2017-12-19T02:54:56.787Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjbd1r26f000wkxuwz489l5lq","content":"<p>2015年的机器学习博客其实都是看《机器学习实战》这本书时学到的，说实话当时也是知其然，不知其所以然，以至于对其理解不深刻，好多细节和理论知识都搞的是乱七八糟，自从工作之后再去看一个算法，思考的比之前多了点，查看资料也比之前多了点，生怕理解错误，影响其他人，当然在理解的程度上还是不够深刻，这也是一个学习的过程吧，记录一下，欢迎指正。</p>\n<p>CSDN链接：<a href=\"http://blog.csdn.net/gamer_gyt/article/details/78008144\" target=\"_blank\" rel=\"external\">点击阅读</a><br><a id=\"more\"></a></p>\n<h1 id=\"一：一些名词定义\"><a href=\"#一：一些名词定义\" class=\"headerlink\" title=\"一：一些名词定义\"></a>一：一些名词定义</h1><h2 id=\"1）指数分布族\"><a href=\"#1）指数分布族\" class=\"headerlink\" title=\"1）指数分布族\"></a>1）指数分布族</h2><p>指数分布族是指可以表示为指数形式的概率分布。</p>\n<script type=\"math/tex; mode=display\">\nf_X(x\\mid\\theta) = h(x) \\exp \\left (\\eta(\\theta) \\cdot T(x) -A(\\theta)\\right )</script><p>其中，η为自然参数(nature parameter)，T(x)是充分统计量（sufficient statistic）。当参数A，h，T都固定以后，就定义了一个以η为参数的函数族。</p>\n<p>伯努利分布与高斯分布是两个典型的指数分布族</p>\n<h3 id=\"伯努利分布\"><a href=\"#伯努利分布\" class=\"headerlink\" title=\"伯努利分布\"></a>伯努利分布</h3><p>又名两点分布或者0-1分布，是一个离散型概率分布。假设1的概率为p，0的概率为q，则<br>其概率质量函数为：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;\\displaystyle f_&#123;X&#125;(x)=p^&#123;x&#125;(1-p)^&#123;1-x&#125;=\\left\\&#123;&#123;\\begin&#123;matrix&#125;p&amp;&#123;\\mbox&#123;if &#125;&#125;x=1,\\\\q\\ &amp;&#123;\\mbox&#123;if &#125;&#125;x=0.\\\\\\end&#123;matrix&#125;&#125;\\right.&#125;</span><br></pre></td></tr></table></figure></p>\n<p>其期望值为：</p>\n<script type=\"math/tex; mode=display\">\n{\\displaystyle \\operatorname {E} [X]=\\sum _{i=0}^{1}x_{i}f_{X}(x)=0+p=p}</script><p>其方差为：</p>\n<script type=\"math/tex; mode=display\">\n{\\displaystyle \\operatorname {var} [X]=\\sum _{i=0}^{1}(x_{i}-E[X])^{2}f_{X}(x)=(0-p)^{2}(1-p)+(1-p)^{2}p=p(1-p)=pq}</script><h3 id=\"正态分布-高斯分布\"><a href=\"#正态分布-高斯分布\" class=\"headerlink\" title=\"正态分布(高斯分布)\"></a>正态分布(高斯分布)</h3><p>若随机变量X服从一个位置参数为 ${\\displaystyle \\mu }$ 、尺度参数为 ${\\displaystyle \\sigma } $ 的概率分布，记为：</p>\n<script type=\"math/tex; mode=display\">\nX \\sim N(\\mu,\\sigma^2),</script><p>其概率密度函数为:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">f(x) = &#123;1 \\over \\sigma\\sqrt&#123;2\\pi&#125; &#125;\\,e^&#123;- &#123;&#123;(x-\\mu )^2 \\over 2\\sigma^2&#125;&#125;&#125;</span><br></pre></td></tr></table></figure></p>\n<p>正态分布的数学期望值或期望值$ {\\displaystyle \\mu } $ 等于位置参数，决定了分布的位置；其方差 $ {\\displaystyle \\sigma ^{2}} $ 的开平方或标准差$ {\\displaystyle \\sigma }$ 等于尺度参数，决定了分布的幅度。</p>\n<h3 id=\"标准正态分布：\"><a href=\"#标准正态分布：\" class=\"headerlink\" title=\"标准正态分布：\"></a>标准正态分布：</h3><p>如果$ {\\displaystyle \\mu =0} $ 并且 $ {\\displaystyle \\sigma =1} $ 则这个正态分布称为标准正态分布。简化为：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">f(x) = \\frac&#123;1&#125;&#123;\\sqrt&#123;2\\pi&#125;&#125; \\, \\exp\\left(-\\frac&#123;x^2&#125;&#123;2&#125; \\right)</span><br></pre></td></tr></table></figure></p>\n<p>如下图所示：</p>\n<p><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Normal_distribution_pdf.png/650px-Normal_distribution_pdf.png\" alt=\"image\"></p>\n<p>正态分布中一些值得注意的量：</p>\n<ul>\n<li>密度函数关于平均值对称</li>\n<li>平均值与它的众数（statistical mode）以及中位数（median）同一数值。</li>\n<li>函数曲线下68.268949%的面积在平均数左右的一个标准差范围内。</li>\n<li>95.449974%的面积在平均数左右两个标准差 $ {\\displaystyle 2\\sigma } $ 的范围内。</li>\n<li>99.730020%的面积在平均数左右三个标准差$ {\\displaystyle 3\\sigma } $ 的范围内。</li>\n<li>99.993666%的面积在平均数左右四个标准差$ {\\displaystyle 4\\sigma } $ 的范围内。</li>\n<li>函数曲线的反曲点（inflection point）为离平均数一个标准差距离的位置。</li>\n</ul>\n<h2 id=\"2）多重共线性和完全共线性\"><a href=\"#2）多重共线性和完全共线性\" class=\"headerlink\" title=\"2）多重共线性和完全共线性\"></a>2）多重共线性和完全共线性</h2><p>多重共线性：指线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真或难以估计准确。一般来说，由于经济数据的限制使得模型设计不当，导致设计矩阵中解释变量间存在普遍的相关关系。通俗点理解就是自变量里边有一些是打酱油的，可以由另外一些变量推导出来，当变量中存在大量的多重共线性变量就会导致模型误差很大，这个时候就需要从自变量中将“打酱油”的变量给剔除掉。</p>\n<p>完全共线性：在多元回归中，一个自变量是一个或多个其他自变量的线性函数。</p>\n<p>两者在某种特殊情况下是有交集的。</p>\n<h2 id=\"3）T检验\"><a href=\"#3）T检验\" class=\"headerlink\" title=\"3）T检验\"></a>3）T检验</h2><p>T检验又叫student T 检验，主要用于样本含量小，总标准差 $\\sigma$ 未知的正太分布数据。T检验是用于小样本的两个平均值差异程度的检查方法，他是用T分布理论值来推断事件发生的概率，从而判断两个平均数的差异是否显著。<br>参考: <a href=\"http://blog.csdn.net/shulixu/article/details/53354206\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/shulixu/article/details/53354206</a></p>\n<h2 id=\"4）关系\"><a href=\"#4）关系\" class=\"headerlink\" title=\"4）关系\"></a>4）关系</h2><ul>\n<li>函数关系<blockquote>\n<p>确定性关系，y=3+2x</p>\n</blockquote>\n</li>\n<li>相关关系<blockquote>\n<p>非确定性关系，比如说高中时数学成绩好的人，一般物理成绩也好，这是因为它们背后使用的都是数学逻辑，这种酒叫做非确定性关系。</p>\n</blockquote>\n</li>\n</ul>\n<h2 id=\"5）虚拟变量\"><a href=\"#5）虚拟变量\" class=\"headerlink\" title=\"5）虚拟变量\"></a>5）虚拟变量</h2><p>定义：</p>\n<blockquote>\n<p>又称虚设变量、名义变量或哑变量，用以反映质的属性的一个人工变量，是量化了的自变量，通常取值为0或1。（通常为离散变量，因子变量）</p>\n</blockquote>\n<p>作用：</p>\n<blockquote>\n<p>引入哑变量可使线形回归模型变得更复杂，但对问题描述更简明，一个方程能达到两个方程的作用，而且接近现实。</p>\n</blockquote>\n<p>设置：</p>\n<blockquote>\n<p>例如：体重（w）和身高（h），性别（s）的关系，但这里性别并非连续的或者数字可以表示的变量，你并不能拿 1表示男，2表示女，这里的性别是离散变量，只能为男或者女，所以这里就需要引入哑变量来处理。<br>性别（s） =》 isman（男1，非男0），iswoman （因为只有两种可能，所以这里只需要引入一个哑变量即可），同理假设这里有另外一个变量肤色（有黑，白，黄三种可能），那么这里只需引入两个哑变量即可（isblack，iswhite），因为不是这两种的话那肯定是黄色皮肤了。</p>\n</blockquote>\n<p>例子：<br>针对上边所说的体重和身高，性别的关系。</p>\n<p>构建模型：</p>\n<ul>\n<li>1）加法模型<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">w = a + b * h + c * isman</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>针对数据样本而言，性别是确定的，所以 c * isman 的结果不是c就是0，所以在加法模型下，影响的是模型在y轴上的截距。这说明的是针对不同的性别而言，回归方程是平衡的，只不过是截距不一样。</p>\n<ul>\n<li>2）乘法模型<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">w = a + b * h + c * isman * h + d * iswoman * h</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>同样针对数据样本而言，性别也是确定的，假设一个男性，isman 为1，iswoman 为0，则上述模型变成了 w = a + b<em>h + c </em> h =a + (b+c) * h，这个时候就是在y轴上的截距一样，而斜率不一致。</p>\n<ul>\n<li>3）混合模型<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">w = a + b * h + c * isman + d * iswoman + e * isman * h + f * iswoman * h</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>假设一个针对一个性别为男的样本数据，该模型变可以变成 w = a + b<em>h + c + e </em> h = a +c + (b+e)*h，这个时候斜率和截距都是不一样的。</p>\n<h1 id=\"二：什么是回归（分析）\"><a href=\"#二：什么是回归（分析）\" class=\"headerlink\" title=\"二：什么是回归（分析）\"></a>二：什么是回归（分析）</h1><p>回归就是利用样本（已知数据），产生拟合方程，从而（对未知数据）进行预测。比如说我有一组随机变量X（X1，X2，X3…）和另外一组随机变量Y（Y1，Y2，Y3…）,那么研究变量X与Y之间的统计学方法就叫做回归分析。当然这里X和Y是单一对应的，所以这里是一元线性回归。</p>\n<p>回归分为线性回归和非线性回归，其中一些非线性回归可以用线性回归的方法来进行分析的叫做==广义线性回归==，接下来我们来了解下每一种回归：</p>\n<h2 id=\"1）线性回归\"><a href=\"#1）线性回归\" class=\"headerlink\" title=\"1）线性回归\"></a>1）线性回归</h2><p>线性回归可以分为一元线性回归和多元线性回归。当然线性回归中自变量的指数都是1，这里的线性并非真的是指用一条线将数据连起来，也可以是一个二维平面，三维平面等。</p>\n<p>一元线性回归：自变量只有一个的回归，比如说北京二环的房子面积（Area）和房子总价（Money）的关系，随着面积（Area）的增大，房屋价格也是不断增长。这里的自变量只有面积，所以这里是一元线性回归。</p>\n<p>多元线性回归：自变量大于等于两个，比如说北京二环的房子面积（Area），楼层（floor）和房屋价格（Money）的关系，这里自变量是两个，所以是二元线性回归，三元，多元同理。</p>\n<h2 id=\"2）非线性回归\"><a href=\"#2）非线性回归\" class=\"headerlink\" title=\"2）非线性回归\"></a>2）非线性回归</h2><p>有一类模型，其回归参数不是线性的，也不能通过转换的方法将其变为线性的参数，这类模型称为非线性回归模型。非线性回归可以分为一元回归和多元回归。非线性回归中至少有一个自变量的指数不为1。回归分析中，当研究的因果关系只涉及因变量和一个自变量时，叫做一元回归分析；当研究的因果关系涉及因变量和两个或两个以上自变量时，叫做多元回归分析。</p>\n<h2 id=\"3）广义线性回归\"><a href=\"#3）广义线性回归\" class=\"headerlink\" title=\"3）广义线性回归\"></a>3）广义线性回归</h2><p>一些非线性回归可以用线性回归的方法来进行分析叫做广义线性回归。<br>典型的代表是Logistic回归。</p>\n<h2 id=\"4）如何衡量相关关系既判断适不适合使用线性回归模型？\"><a href=\"#4）如何衡量相关关系既判断适不适合使用线性回归模型？\" class=\"headerlink\" title=\"4）如何衡量相关关系既判断适不适合使用线性回归模型？\"></a>4）如何衡量相关关系既判断适不适合使用线性回归模型？</h2><p>使用相关系数（-1，1），绝对值越接近于1，相关系数越高，越适合使用线性回归模型（Rxy&gt;0,代表正相关，Rxy&lt;0,代表负相关）</p>\n<script type=\"math/tex; mode=display\">\nr_{XY} = \\frac{ \\sum (X_{i}-\\bar{X})(Y_{i}-\\bar{Y}) }{ \\sqrt{ \\sum (X_{i}-\\bar{X})^2) \\sum (Y_{i}-\\bar{Y})^2) } }</script><h1 id=\"三：回归中困难点\"><a href=\"#三：回归中困难点\" class=\"headerlink\" title=\"三：回归中困难点\"></a>三：回归中困难点</h1><h2 id=\"1）选定变量\"><a href=\"#1）选定变量\" class=\"headerlink\" title=\"1）选定变量\"></a>1）选定变量</h2><blockquote>\n<p>假设自变量特别多，有一些是和因变量相关的，有一些是和因变量不相关的，这里我们就需要筛选出有用的变量，如果筛选后变量还特别多的话，可以采用降维的方式进行变量缩减（可以参考之前的PCA降维的文章：<a href=\"http://blog.csdn.net/gamer_gyt/article/details/51418069\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/gamer_gyt/article/details/51418069</a> ，基本是整理《机器学习实战》这本书的笔记）</p>\n</blockquote>\n<h2 id=\"2）发现多重共线性\"><a href=\"#2）发现多重共线性\" class=\"headerlink\" title=\"2）发现多重共线性\"></a>2）发现多重共线性</h2><p>(1).方差扩大因子法( VIF)</p>\n<blockquote>\n<p>一般认为如果最大的VIF超过10，常常表示存在多重共线性。</p>\n</blockquote>\n<p>(2).容差容忍定法</p>\n<blockquote>\n<p>如果容差（tolerance）&lt;=0.1，常常表示存在多重共线性。</p>\n</blockquote>\n<p>(3). 条件索引</p>\n<blockquote>\n<p>条件索引(condition index)&gt;10，可以说明存在比较严重的共线性</p>\n</blockquote>\n<h2 id=\"3）过拟合与欠拟合问题\"><a href=\"#3）过拟合与欠拟合问题\" class=\"headerlink\" title=\"3）过拟合与欠拟合问题\"></a>3）过拟合与欠拟合问题</h2><p>过拟合和欠拟合其实对每一个模型来讲都是存在的，过拟合就是模型过于符合训练数据的趋势，欠拟合就是模型对于训练数据和测试数据都表现出不好的情况。针对于欠拟合来讲，是很容易发现的，通常不被讨论。</p>\n<p>在进行模型训练的时候，算法要进行不断的学习，模型在训练数据和测试数据上的错误都在不断下降，但是，如果学习的时间过长的话，模型在训练数据集上的表现将会继续下降，这是因为模型已经过拟合，并且学习到了训练数据集中不恰当的细节和噪音，同时，测试集上的错误率开始上升，也是模型泛化能力在下降。</p>\n<p>这个完美的临界点就在于测试集中的错误率在上升时，此时训练集和测试集上都有良好的表现。通常有两种手段可以帮助你找到这个完美的临界点：重采样方法和验证集方法。</p>\n<h3 id=\"如何限制过拟合？\"><a href=\"#如何限制过拟合？\" class=\"headerlink\" title=\"如何限制过拟合？\"></a>如何限制过拟合？</h3><blockquote>\n<p>过拟合和欠拟合可以导致很差的模型表现。但是到目前为止大部分机器学习实际应用时的问题都是过拟合。<br>过拟合是个问题因为训练数据上的机器学习算法的评价方法与我们最关心的实际上的评价方法，也就是算法在位置数据上的表现是不一样的。<br>当评价机器学习算法时我们有两者重要的技巧来限制过拟合<br>使用重采样来评价模型效能<br>保留一个验证数据集<br>最流行的重采样技术是k折交叉验证。指的是在训练数据的子集上训练和测试模型k次，同时建立对于机器学习模型在未知数据上表现的评估。<br>验证集只是训练数据的子集，你把它保留到你进行机器学习算法的最后才使用。在训练数据上选择和调谐机器学习算法之后，我们在验证集上在对于模型进行评估，以便得到一些关于模型在未知数据上的表现的认知。</p>\n</blockquote>\n<h2 id=\"4）检验模型是否合理\"><a href=\"#4）检验模型是否合理\" class=\"headerlink\" title=\"4）检验模型是否合理\"></a>4）检验模型是否合理</h2><p>验证目前主要采用如下三类办法：<br>1、拟合优度检验<br>主要有R^2，t检验，f检验等等<br>这三种检验为常规验证，只要在95%的置信度内满足即可说明拟合效果良好。<br>2、预测值和真实值比较<br>主要是差值和比值，一般差值和比值都不超过5%。<br>3、另外的办法<br>GEH方法最为常用。GEH是Geoffrey E. Havers于1970年左右提出的一种模型验证方法，其巧妙的运用一个拟定的公式和标准界定模型的拟合优劣。<br>GEH=(2(M-C)^2/(M+C))^(1/2)<br>其中M是预测值，C是实际观测值<br>如果GEH小于5，认为模型拟合效果良好，如果GEH在5-10之间，必须对数据不可靠需要进行检查，如果GEH大于10，说明数据存在问题的几率很高。<br><a href=\"http://blog.sina.com.cn/s/blog_66188c300100hl45.html\" target=\"_blank\" rel=\"external\">http://blog.sina.com.cn/s/blog_66188c300100hl45.html</a></p>\n<h2 id=\"5）线性回归的模型评判\"><a href=\"#5）线性回归的模型评判\" class=\"headerlink\" title=\"5）线性回归的模型评判\"></a>5）线性回归的模型评判</h2><ul>\n<li>误差平方和（残差平方和）</li>\n</ul>\n<p>例如二维平面上的一点（x1，y1），经过线性回归模型预测其值为 y_1，那么预测模型的好与坏就是计算预测结果到直线的距离的大小，由于是一组数据，那么便是这一组数据的和。</p>\n<p>点到直线的距离公式为： </p>\n<script type=\"math/tex; mode=display\">\n \\frac{\\left | A_{x_{0}}+B_{y_{0}} +C \\right |}{\\sqrt{A^2 + B^2 }}</script><p>由于涉及到开方，在计算过程中十分不方便，所以这里转换为纵轴上的差值，即利用预测值与真实值的差进行累加求和，最小时即为最佳的线性回归模型，但是这里涉及到预测值与真实值的差可能为负数，所以这里用平方，所以最终的误差平方和为：</p>\n<script type=\"math/tex; mode=display\">\nRSS = \\sum_{i=1}^{n}(y_{i}- \\hat{y_{i}} )^2 = \\sum_{i=1}^{n}[y_{i} - (\\alpha +\\beta x_{i})]^2</script><ul>\n<li>AIC准则（赤池信息准则）<script type=\"math/tex; mode=display\">\nAIC=n ln (RSSp/n)+2p</script>n为变量总个数，p为选出的变量个数，AIC越小越好</li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>2015年的机器学习博客其实都是看《机器学习实战》这本书时学到的，说实话当时也是知其然，不知其所以然，以至于对其理解不深刻，好多细节和理论知识都搞的是乱七八糟，自从工作之后再去看一个算法，思考的比之前多了点，查看资料也比之前多了点，生怕理解错误，影响其他人，当然在理解的程度上还是不够深刻，这也是一个学习的过程吧，记录一下，欢迎指正。</p>\n<p>CSDN链接：<a href=\"http://blog.csdn.net/gamer_gyt/article/details/78008144\" target=\"_blank\" rel=\"external\">点击阅读</a><br>","more":"</p>\n<h1 id=\"一：一些名词定义\"><a href=\"#一：一些名词定义\" class=\"headerlink\" title=\"一：一些名词定义\"></a>一：一些名词定义</h1><h2 id=\"1）指数分布族\"><a href=\"#1）指数分布族\" class=\"headerlink\" title=\"1）指数分布族\"></a>1）指数分布族</h2><p>指数分布族是指可以表示为指数形式的概率分布。</p>\n<script type=\"math/tex; mode=display\">\nf_X(x\\mid\\theta) = h(x) \\exp \\left (\\eta(\\theta) \\cdot T(x) -A(\\theta)\\right )</script><p>其中，η为自然参数(nature parameter)，T(x)是充分统计量（sufficient statistic）。当参数A，h，T都固定以后，就定义了一个以η为参数的函数族。</p>\n<p>伯努利分布与高斯分布是两个典型的指数分布族</p>\n<h3 id=\"伯努利分布\"><a href=\"#伯努利分布\" class=\"headerlink\" title=\"伯努利分布\"></a>伯努利分布</h3><p>又名两点分布或者0-1分布，是一个离散型概率分布。假设1的概率为p，0的概率为q，则<br>其概率质量函数为：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;\\displaystyle f_&#123;X&#125;(x)=p^&#123;x&#125;(1-p)^&#123;1-x&#125;=\\left\\&#123;&#123;\\begin&#123;matrix&#125;p&amp;&#123;\\mbox&#123;if &#125;&#125;x=1,\\\\q\\ &amp;&#123;\\mbox&#123;if &#125;&#125;x=0.\\\\\\end&#123;matrix&#125;&#125;\\right.&#125;</span><br></pre></td></tr></table></figure></p>\n<p>其期望值为：</p>\n<script type=\"math/tex; mode=display\">\n{\\displaystyle \\operatorname {E} [X]=\\sum _{i=0}^{1}x_{i}f_{X}(x)=0+p=p}</script><p>其方差为：</p>\n<script type=\"math/tex; mode=display\">\n{\\displaystyle \\operatorname {var} [X]=\\sum _{i=0}^{1}(x_{i}-E[X])^{2}f_{X}(x)=(0-p)^{2}(1-p)+(1-p)^{2}p=p(1-p)=pq}</script><h3 id=\"正态分布-高斯分布\"><a href=\"#正态分布-高斯分布\" class=\"headerlink\" title=\"正态分布(高斯分布)\"></a>正态分布(高斯分布)</h3><p>若随机变量X服从一个位置参数为 ${\\displaystyle \\mu }$ 、尺度参数为 ${\\displaystyle \\sigma } $ 的概率分布，记为：</p>\n<script type=\"math/tex; mode=display\">\nX \\sim N(\\mu,\\sigma^2),</script><p>其概率密度函数为:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">f(x) = &#123;1 \\over \\sigma\\sqrt&#123;2\\pi&#125; &#125;\\,e^&#123;- &#123;&#123;(x-\\mu )^2 \\over 2\\sigma^2&#125;&#125;&#125;</span><br></pre></td></tr></table></figure></p>\n<p>正态分布的数学期望值或期望值$ {\\displaystyle \\mu } $ 等于位置参数，决定了分布的位置；其方差 $ {\\displaystyle \\sigma ^{2}} $ 的开平方或标准差$ {\\displaystyle \\sigma }$ 等于尺度参数，决定了分布的幅度。</p>\n<h3 id=\"标准正态分布：\"><a href=\"#标准正态分布：\" class=\"headerlink\" title=\"标准正态分布：\"></a>标准正态分布：</h3><p>如果$ {\\displaystyle \\mu =0} $ 并且 $ {\\displaystyle \\sigma =1} $ 则这个正态分布称为标准正态分布。简化为：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">f(x) = \\frac&#123;1&#125;&#123;\\sqrt&#123;2\\pi&#125;&#125; \\, \\exp\\left(-\\frac&#123;x^2&#125;&#123;2&#125; \\right)</span><br></pre></td></tr></table></figure></p>\n<p>如下图所示：</p>\n<p><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Normal_distribution_pdf.png/650px-Normal_distribution_pdf.png\" alt=\"image\"></p>\n<p>正态分布中一些值得注意的量：</p>\n<ul>\n<li>密度函数关于平均值对称</li>\n<li>平均值与它的众数（statistical mode）以及中位数（median）同一数值。</li>\n<li>函数曲线下68.268949%的面积在平均数左右的一个标准差范围内。</li>\n<li>95.449974%的面积在平均数左右两个标准差 $ {\\displaystyle 2\\sigma } $ 的范围内。</li>\n<li>99.730020%的面积在平均数左右三个标准差$ {\\displaystyle 3\\sigma } $ 的范围内。</li>\n<li>99.993666%的面积在平均数左右四个标准差$ {\\displaystyle 4\\sigma } $ 的范围内。</li>\n<li>函数曲线的反曲点（inflection point）为离平均数一个标准差距离的位置。</li>\n</ul>\n<h2 id=\"2）多重共线性和完全共线性\"><a href=\"#2）多重共线性和完全共线性\" class=\"headerlink\" title=\"2）多重共线性和完全共线性\"></a>2）多重共线性和完全共线性</h2><p>多重共线性：指线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真或难以估计准确。一般来说，由于经济数据的限制使得模型设计不当，导致设计矩阵中解释变量间存在普遍的相关关系。通俗点理解就是自变量里边有一些是打酱油的，可以由另外一些变量推导出来，当变量中存在大量的多重共线性变量就会导致模型误差很大，这个时候就需要从自变量中将“打酱油”的变量给剔除掉。</p>\n<p>完全共线性：在多元回归中，一个自变量是一个或多个其他自变量的线性函数。</p>\n<p>两者在某种特殊情况下是有交集的。</p>\n<h2 id=\"3）T检验\"><a href=\"#3）T检验\" class=\"headerlink\" title=\"3）T检验\"></a>3）T检验</h2><p>T检验又叫student T 检验，主要用于样本含量小，总标准差 $\\sigma$ 未知的正太分布数据。T检验是用于小样本的两个平均值差异程度的检查方法，他是用T分布理论值来推断事件发生的概率，从而判断两个平均数的差异是否显著。<br>参考: <a href=\"http://blog.csdn.net/shulixu/article/details/53354206\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/shulixu/article/details/53354206</a></p>\n<h2 id=\"4）关系\"><a href=\"#4）关系\" class=\"headerlink\" title=\"4）关系\"></a>4）关系</h2><ul>\n<li>函数关系<blockquote>\n<p>确定性关系，y=3+2x</p>\n</blockquote>\n</li>\n<li>相关关系<blockquote>\n<p>非确定性关系，比如说高中时数学成绩好的人，一般物理成绩也好，这是因为它们背后使用的都是数学逻辑，这种酒叫做非确定性关系。</p>\n</blockquote>\n</li>\n</ul>\n<h2 id=\"5）虚拟变量\"><a href=\"#5）虚拟变量\" class=\"headerlink\" title=\"5）虚拟变量\"></a>5）虚拟变量</h2><p>定义：</p>\n<blockquote>\n<p>又称虚设变量、名义变量或哑变量，用以反映质的属性的一个人工变量，是量化了的自变量，通常取值为0或1。（通常为离散变量，因子变量）</p>\n</blockquote>\n<p>作用：</p>\n<blockquote>\n<p>引入哑变量可使线形回归模型变得更复杂，但对问题描述更简明，一个方程能达到两个方程的作用，而且接近现实。</p>\n</blockquote>\n<p>设置：</p>\n<blockquote>\n<p>例如：体重（w）和身高（h），性别（s）的关系，但这里性别并非连续的或者数字可以表示的变量，你并不能拿 1表示男，2表示女，这里的性别是离散变量，只能为男或者女，所以这里就需要引入哑变量来处理。<br>性别（s） =》 isman（男1，非男0），iswoman （因为只有两种可能，所以这里只需要引入一个哑变量即可），同理假设这里有另外一个变量肤色（有黑，白，黄三种可能），那么这里只需引入两个哑变量即可（isblack，iswhite），因为不是这两种的话那肯定是黄色皮肤了。</p>\n</blockquote>\n<p>例子：<br>针对上边所说的体重和身高，性别的关系。</p>\n<p>构建模型：</p>\n<ul>\n<li>1）加法模型<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">w = a + b * h + c * isman</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>针对数据样本而言，性别是确定的，所以 c * isman 的结果不是c就是0，所以在加法模型下，影响的是模型在y轴上的截距。这说明的是针对不同的性别而言，回归方程是平衡的，只不过是截距不一样。</p>\n<ul>\n<li>2）乘法模型<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">w = a + b * h + c * isman * h + d * iswoman * h</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>同样针对数据样本而言，性别也是确定的，假设一个男性，isman 为1，iswoman 为0，则上述模型变成了 w = a + b<em>h + c </em> h =a + (b+c) * h，这个时候就是在y轴上的截距一样，而斜率不一致。</p>\n<ul>\n<li>3）混合模型<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">w = a + b * h + c * isman + d * iswoman + e * isman * h + f * iswoman * h</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>假设一个针对一个性别为男的样本数据，该模型变可以变成 w = a + b<em>h + c + e </em> h = a +c + (b+e)*h，这个时候斜率和截距都是不一样的。</p>\n<h1 id=\"二：什么是回归（分析）\"><a href=\"#二：什么是回归（分析）\" class=\"headerlink\" title=\"二：什么是回归（分析）\"></a>二：什么是回归（分析）</h1><p>回归就是利用样本（已知数据），产生拟合方程，从而（对未知数据）进行预测。比如说我有一组随机变量X（X1，X2，X3…）和另外一组随机变量Y（Y1，Y2，Y3…）,那么研究变量X与Y之间的统计学方法就叫做回归分析。当然这里X和Y是单一对应的，所以这里是一元线性回归。</p>\n<p>回归分为线性回归和非线性回归，其中一些非线性回归可以用线性回归的方法来进行分析的叫做==广义线性回归==，接下来我们来了解下每一种回归：</p>\n<h2 id=\"1）线性回归\"><a href=\"#1）线性回归\" class=\"headerlink\" title=\"1）线性回归\"></a>1）线性回归</h2><p>线性回归可以分为一元线性回归和多元线性回归。当然线性回归中自变量的指数都是1，这里的线性并非真的是指用一条线将数据连起来，也可以是一个二维平面，三维平面等。</p>\n<p>一元线性回归：自变量只有一个的回归，比如说北京二环的房子面积（Area）和房子总价（Money）的关系，随着面积（Area）的增大，房屋价格也是不断增长。这里的自变量只有面积，所以这里是一元线性回归。</p>\n<p>多元线性回归：自变量大于等于两个，比如说北京二环的房子面积（Area），楼层（floor）和房屋价格（Money）的关系，这里自变量是两个，所以是二元线性回归，三元，多元同理。</p>\n<h2 id=\"2）非线性回归\"><a href=\"#2）非线性回归\" class=\"headerlink\" title=\"2）非线性回归\"></a>2）非线性回归</h2><p>有一类模型，其回归参数不是线性的，也不能通过转换的方法将其变为线性的参数，这类模型称为非线性回归模型。非线性回归可以分为一元回归和多元回归。非线性回归中至少有一个自变量的指数不为1。回归分析中，当研究的因果关系只涉及因变量和一个自变量时，叫做一元回归分析；当研究的因果关系涉及因变量和两个或两个以上自变量时，叫做多元回归分析。</p>\n<h2 id=\"3）广义线性回归\"><a href=\"#3）广义线性回归\" class=\"headerlink\" title=\"3）广义线性回归\"></a>3）广义线性回归</h2><p>一些非线性回归可以用线性回归的方法来进行分析叫做广义线性回归。<br>典型的代表是Logistic回归。</p>\n<h2 id=\"4）如何衡量相关关系既判断适不适合使用线性回归模型？\"><a href=\"#4）如何衡量相关关系既判断适不适合使用线性回归模型？\" class=\"headerlink\" title=\"4）如何衡量相关关系既判断适不适合使用线性回归模型？\"></a>4）如何衡量相关关系既判断适不适合使用线性回归模型？</h2><p>使用相关系数（-1，1），绝对值越接近于1，相关系数越高，越适合使用线性回归模型（Rxy&gt;0,代表正相关，Rxy&lt;0,代表负相关）</p>\n<script type=\"math/tex; mode=display\">\nr_{XY} = \\frac{ \\sum (X_{i}-\\bar{X})(Y_{i}-\\bar{Y}) }{ \\sqrt{ \\sum (X_{i}-\\bar{X})^2) \\sum (Y_{i}-\\bar{Y})^2) } }</script><h1 id=\"三：回归中困难点\"><a href=\"#三：回归中困难点\" class=\"headerlink\" title=\"三：回归中困难点\"></a>三：回归中困难点</h1><h2 id=\"1）选定变量\"><a href=\"#1）选定变量\" class=\"headerlink\" title=\"1）选定变量\"></a>1）选定变量</h2><blockquote>\n<p>假设自变量特别多，有一些是和因变量相关的，有一些是和因变量不相关的，这里我们就需要筛选出有用的变量，如果筛选后变量还特别多的话，可以采用降维的方式进行变量缩减（可以参考之前的PCA降维的文章：<a href=\"http://blog.csdn.net/gamer_gyt/article/details/51418069\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/gamer_gyt/article/details/51418069</a> ，基本是整理《机器学习实战》这本书的笔记）</p>\n</blockquote>\n<h2 id=\"2）发现多重共线性\"><a href=\"#2）发现多重共线性\" class=\"headerlink\" title=\"2）发现多重共线性\"></a>2）发现多重共线性</h2><p>(1).方差扩大因子法( VIF)</p>\n<blockquote>\n<p>一般认为如果最大的VIF超过10，常常表示存在多重共线性。</p>\n</blockquote>\n<p>(2).容差容忍定法</p>\n<blockquote>\n<p>如果容差（tolerance）&lt;=0.1，常常表示存在多重共线性。</p>\n</blockquote>\n<p>(3). 条件索引</p>\n<blockquote>\n<p>条件索引(condition index)&gt;10，可以说明存在比较严重的共线性</p>\n</blockquote>\n<h2 id=\"3）过拟合与欠拟合问题\"><a href=\"#3）过拟合与欠拟合问题\" class=\"headerlink\" title=\"3）过拟合与欠拟合问题\"></a>3）过拟合与欠拟合问题</h2><p>过拟合和欠拟合其实对每一个模型来讲都是存在的，过拟合就是模型过于符合训练数据的趋势，欠拟合就是模型对于训练数据和测试数据都表现出不好的情况。针对于欠拟合来讲，是很容易发现的，通常不被讨论。</p>\n<p>在进行模型训练的时候，算法要进行不断的学习，模型在训练数据和测试数据上的错误都在不断下降，但是，如果学习的时间过长的话，模型在训练数据集上的表现将会继续下降，这是因为模型已经过拟合，并且学习到了训练数据集中不恰当的细节和噪音，同时，测试集上的错误率开始上升，也是模型泛化能力在下降。</p>\n<p>这个完美的临界点就在于测试集中的错误率在上升时，此时训练集和测试集上都有良好的表现。通常有两种手段可以帮助你找到这个完美的临界点：重采样方法和验证集方法。</p>\n<h3 id=\"如何限制过拟合？\"><a href=\"#如何限制过拟合？\" class=\"headerlink\" title=\"如何限制过拟合？\"></a>如何限制过拟合？</h3><blockquote>\n<p>过拟合和欠拟合可以导致很差的模型表现。但是到目前为止大部分机器学习实际应用时的问题都是过拟合。<br>过拟合是个问题因为训练数据上的机器学习算法的评价方法与我们最关心的实际上的评价方法，也就是算法在位置数据上的表现是不一样的。<br>当评价机器学习算法时我们有两者重要的技巧来限制过拟合<br>使用重采样来评价模型效能<br>保留一个验证数据集<br>最流行的重采样技术是k折交叉验证。指的是在训练数据的子集上训练和测试模型k次，同时建立对于机器学习模型在未知数据上表现的评估。<br>验证集只是训练数据的子集，你把它保留到你进行机器学习算法的最后才使用。在训练数据上选择和调谐机器学习算法之后，我们在验证集上在对于模型进行评估，以便得到一些关于模型在未知数据上的表现的认知。</p>\n</blockquote>\n<h2 id=\"4）检验模型是否合理\"><a href=\"#4）检验模型是否合理\" class=\"headerlink\" title=\"4）检验模型是否合理\"></a>4）检验模型是否合理</h2><p>验证目前主要采用如下三类办法：<br>1、拟合优度检验<br>主要有R^2，t检验，f检验等等<br>这三种检验为常规验证，只要在95%的置信度内满足即可说明拟合效果良好。<br>2、预测值和真实值比较<br>主要是差值和比值，一般差值和比值都不超过5%。<br>3、另外的办法<br>GEH方法最为常用。GEH是Geoffrey E. Havers于1970年左右提出的一种模型验证方法，其巧妙的运用一个拟定的公式和标准界定模型的拟合优劣。<br>GEH=(2(M-C)^2/(M+C))^(1/2)<br>其中M是预测值，C是实际观测值<br>如果GEH小于5，认为模型拟合效果良好，如果GEH在5-10之间，必须对数据不可靠需要进行检查，如果GEH大于10，说明数据存在问题的几率很高。<br><a href=\"http://blog.sina.com.cn/s/blog_66188c300100hl45.html\" target=\"_blank\" rel=\"external\">http://blog.sina.com.cn/s/blog_66188c300100hl45.html</a></p>\n<h2 id=\"5）线性回归的模型评判\"><a href=\"#5）线性回归的模型评判\" class=\"headerlink\" title=\"5）线性回归的模型评判\"></a>5）线性回归的模型评判</h2><ul>\n<li>误差平方和（残差平方和）</li>\n</ul>\n<p>例如二维平面上的一点（x1，y1），经过线性回归模型预测其值为 y_1，那么预测模型的好与坏就是计算预测结果到直线的距离的大小，由于是一组数据，那么便是这一组数据的和。</p>\n<p>点到直线的距离公式为： </p>\n<script type=\"math/tex; mode=display\">\n \\frac{\\left | A_{x_{0}}+B_{y_{0}} +C \\right |}{\\sqrt{A^2 + B^2 }}</script><p>由于涉及到开方，在计算过程中十分不方便，所以这里转换为纵轴上的差值，即利用预测值与真实值的差进行累加求和，最小时即为最佳的线性回归模型，但是这里涉及到预测值与真实值的差可能为负数，所以这里用平方，所以最终的误差平方和为：</p>\n<script type=\"math/tex; mode=display\">\nRSS = \\sum_{i=1}^{n}(y_{i}- \\hat{y_{i}} )^2 = \\sum_{i=1}^{n}[y_{i} - (\\alpha +\\beta x_{i})]^2</script><ul>\n<li>AIC准则（赤池信息准则）<script type=\"math/tex; mode=display\">\nAIC=n ln (RSSp/n)+2p</script>n为变量总个数，p为选出的变量个数，AIC越小越好</li>\n</ul>"},{"title":"回归分析之线性回归（N元线性回归）","date":"2017-09-29T08:45:14.000Z","_content":"\n在上一篇文章中我们介绍了 [ 回归分析之理论篇][1]，在其中我们有聊到线性回归和非线性回归，包括广义线性回归，这一篇文章我们来聊下回归分析中的线性回归。\n\n<!--More-->\n# 一元线性回归\n预测房价：\n输入编号\t| 平方米\t| 价格\n-|-|-\n1 |\t150 |\t6450\n2\t| 200\t| 7450\n3|\t250\t|8450\n4|\t300\t|9450\n5|\t350\t|11450\n6|\t400\t|15450\n7|\t600|\t18450\n\n针对上边这种一元数据来讲，我们可以构建的一元线性回归函数为\n$$\nH(x) = k*x + b\n$$\n其中H(x)为平方米价格表，k是一元回归系数，b为常数。最小二乘法的公式：\n$$\nk =\\frac{ \\sum_{1}^{n} (x_{i} - \\bar{x} )(y_{i} - \\bar{y}) } { \\sum_{1}^{n}(x_{i}-\\bar{x})^{2} }\n$$\n自己使用python代码实现为：\n```\ndef leastsq(x,y):\n    \"\"\"\n    x,y分别是要拟合的数据的自变量列表和因变量列表\n    \"\"\"\n    meanX = sum(x) * 1.0 / len(x)      # 求x的平均值\n    meanY = sum(y) * 1.0 / len(y)     # 求y的平均值\n\n    xSum = 0.0\n    ySum = 0.0\n\n    for i in range(len(x)):\n        xSum += (x[i] - meanX) * (y[i] - meanY)\n        ySum += (x[i] - meanX) ** 2\n\n    k = ySum/xSum\n    b = ySum - k * meanX\n\n    return k,b\n```\n\n使用python的scipy包进行计算:\n```\nleastsq(func, x0, args=(), Dfun=None, full_output=0, col_deriv=0, ftol=1.49012e-08, xtol=1.49012e-08, gtol=0.0, maxfev=0, epsfcn=None, factor=100, diag=None)\n\nfrom scipy.optimize import leastsq\nimport numpy as np\n\ndef fun(p, x):\n    \"\"\"\n    定义想要拟合的函数\n    \"\"\"\n    k,b = p    # 从参数p获得拟合的参数\n    return k*x + b\n\ndef err(p, x, y):\n    return fun(p,x) - y\n\n#定义起始的参数 即从 y = 1*x+1 开始，其实这个值可以随便设，只不过会影响到找到最优解的时间\np0 = [1,1]\n\n#将list类型转换为 numpy.ndarray 类型，最初我直接使用\n#list 类型,结果 leastsq函数报错，后来在别的blog上看到了，原来要将类型转\n#换为numpy的类型\n\nx1 = np.array([150,200,250,300,350,400,600])\ny1 = np.array([6450,7450,8450,9450,11450,15450,18450])\n\nxishu = leastsq(err, p0, args=(x1,y1))\n\nprint xishu[0]\n\n```\n\n当然python的leastsq函数不仅仅局限于一元一次的应用，也可以应用到一元二次，二元二次，多元多次等，具体可以看下这篇博客：http://www.cnblogs.com/NanShan2016/p/5493429.html\n\n# 多元线性回归\n总之：我们可以用python leastsq函数解决几乎所有的线性回归的问题了，比如说\n$$y = a * x^2 + b * x + c$$\n$$y = a * x_1^2 + b * x_1 + c * x_2 + d$$\n$$y = a * x_1^3 + b * x_1^2 + c * x_1 + d$$\n在使用时只需把参数列表和 fun 函数中的return 换一下，拿以下函数举例\n$$y = a * x_1^2 + b * x_1 + c * x_2 + d$$\n\n对应的python 代码是：\n```\nfrom scipy.optimize import leastsq\nimport numpy as np\n\n\ndef fun(p, x1, x2):\n    \"\"\"\n    定义想要拟合的函数\n    \"\"\"\n    a,b,c,d = p    # 从参数p获得拟合的参数\n    return a * (x1**2) + b * x1 + c * x2 + d\n\ndef err(p, x1, x2, y):\n    return fun(p,x1,x2) - y\n\n#定义起始的参数 即从 y = 1*x+1 开始，其实这个值可以随便设，只不过会影响到找到最优解的时间\np0 = [1,1,1,1]\n\n#将list类型转换为 numpy.ndarray 类型，最初我直接使用\n#list 类型,结果 leastsq函数报错，后来在别的blog上看到了，原来要将类型转\n#换为numpy的类型\n\nx1 = np.array([150,200,250,300,350,400,600])    # 面积\nx2 = np.array([4,2,7,9,12,14,15])               # 楼层\ny1 = np.array([6450,7450,8450,9450,11450,15450,18450])   # 价格/平方米\n\nxishu = leastsq(err, p0, args=(x1,x2,y1))\n\nprint xishu[0]\n```\n\n# sklearn中的线性回归应用\n## 普通最小二乘回归\n这里我们使用的是sklearn中的linear_model来模拟$$y=a * x_1 + b * x_2 + c$$\n\n```\nIn [1]: from sklearn.linear_model import LinearRegression\n\nIn [2]: linreg = LinearRegression()\n\nIn [3]: linreg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])\n\nIn [4]: linreg.coef_\nOut[4]: array([ 0.5,  0.5])\n\nIn [5]: linreg.intercept_\nOut[5]: 1.1102230246251565e-16\n\nIn [6]: linreg.predict([4,4])\nOut[6]: array([ 4.])\n\nIn [7]: zip([\"x1\",\"x2\"], linreg.coef_)\nOut[7]: [('x1', 0.5), ('x2', 0.49999999999999989)]\n```\n所以可得$$ y = 0.5 * x_1 + 0.5 * x_2 + 1.11e-16$$\n\nlinreg.coef_  为系数 a,b\n\nlinreg.intercept_ 为截距 c\n\n缺点：因为系数矩阵x与它的转置矩阵相乘得到的矩阵不能求逆，导致最小二乘法得到的回归系数不稳定，方差很大。\n\n\n## 多项式回归：基函数扩展线性模型\n机器学习中一种常见的模式是使用线性模型训练数据的非线性函数。这种方法保持了一般快速的线性方法的性能，同时允许它们适应更广泛的数据范围。\n\n例如，可以通过构造系数的多项式特征来扩展一个简单的线性回归。在标准线性回归的情况下，你可能有一个类似于二维数据的模型：\n$$\ny(w,x) = w_{0} + w_{1} x_{1} + w_{2} x_{2}\n$$\n\n如果我们想把抛物面拟合成数据而不是平面，我们可以结合二阶多项式的特征，使模型看起来像这样:\n$$\ny(w,x) = w_{0} + w_{1} x_{1} + w_{2} x_{2} + w_{3} x_{1}x_{2} + w_{4} x_{1}^2 + w_{5} x_{2}^2\n$$\n\n我们发现，这仍然是一个线性模型，想象着创建一个新变量：\n$$\nz = [x_{1},x_{2},x_{1} x_{2},x_{1}^2,x_{2}^2]\n$$\n\n可以把线性回归模型写成下边这种形式：\n$$\ny(w,x) = w_{0} + w_{1} z_{1} + w_{2} z_{2} + w_{3} z_{3} + w_{4} z_{4} + w_{5} z_{5}\n$$\n我们看到，所得的多项式回归与我们上面所考虑的线性模型相同（即模型在W中是线性的），可以用同样的方法来求解。通过考虑在用这些基函数建立的高维空间中的线性拟合，该模型具有灵活性，可以适应更广泛的数据范围。\n\n使用如下代码，将二维数据进行二元转换,转换规则为：\n$$\n[x_1, x_2] => [1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]\n$$\n\n```\nIn [15]: from sklearn.preprocessing import PolynomialFeatures\n\nIn [16]: import numpy as np\n\nIn [17]: X = np.arange(6).reshape(3,2)\n\nIn [18]: X\nOut[18]: \narray([[0, 1],\n       [2, 3],\n       [4, 5]])\n\nIn [19]: poly = PolynomialFeatures(degree=2)\n\nIn [20]: poly.fit_transform(X)\nOut[20]: \narray([[  1.,   0.,   1.,   0.,   0.,   1.],\n       [  1.,   2.,   3.,   4.,   6.,   9.],\n       [  1.,   4.,   5.,  16.,  20.,  25.]])\n```\n\n验证：\n```\nIn [38]: from sklearn.preprocessing import PolynomialFeatures\n\nIn [39]: from sklearn.linear_model import LinearRegression\n\nIn [40]: from sklearn.pipeline import Pipeline\n\nIn [41]: import numpy as np\n\nIn [42]: \n\nIn [42]: model = Pipeline( [ (\"poly\",PolynomialFeatures(degree=3)),(\"linear\",LinearRegression(fit_intercept=False)) ] )\n\nIn [43]: model\nOut[43]: Pipeline(steps=[('poly', PolynomialFeatures(degree=3, include_bias=True, interaction_only=False)), ('linear', LinearRegression(copy_X=True, fit_intercept=False, n_jobs=1, normalize=False))])\n\nIn [44]: x = np.arange(5)\n\nIn [45]: y = 3 - 2 * x + x ** 2 - x ** 3\n\nIn [46]: y\nOut[46]: array([  3,   1,  -5, -21, -53])\n\nIn [47]: model = model.fit(x[:,np.newaxis],y)\n\nIn [48]: model.named_steps['linear'].coef_\nOut[48]: array([ 3., -2.,  1., -1.])\n```\n我们可以看出最后求出的参数和一元三次方程是一致的。\n\n这里如果把degree改为2，y的方程也换一下，结果也是一致的\n```\nIn [51]: from sklearn.linear_model import LinearRegression\n\nIn [52]: from sklearn.preprocessing import PolynomialFeatures\n\nIn [53]: from sklearn.pipeline import Pipeline\n\nIn [54]: import numpy as np\n\nIn [55]: model = Pipeline( [ (\"poly\",PolynomialFeatures(degree=2)),(\"linear\",LinearRegression(fit_intercept=False)) ] )\n\nIn [56]: x = np.arange(5)\n\nIn [57]: y = 3 + 2 * x + x ** 2\n\nIn [58]: model = model.fit(x[:, np.newaxis], y)\n\nIn [59]: model.named_steps['linear'].coef_\nOut[59]: array([ 3., 2.,  1.])\n```\n\n\n## 线性回归的评测\n在[上一篇文章](http://note.youdao.com/)中我们聊到了回归模型的评测方法，解下来我们详细聊聊如何来评价一个回归模型的好坏。\n\n这里我们定义预测值和真实值分别为：\n```\ntrue = [10, 5, 3, 2]\npred = [9, 5, 5, 3]\n```\n\n1: 平均绝对误差（Mean Absolute Error, MAE）\n$$\n\\frac{1}{N}(\\sum_{1}^{n} |y_i - \\bar{y}|)\n$$\n\n2: 均方误差（Mean Squared Error, MSE）\n$$\n\\frac{1}{N}\\sum_{1}^{n}(y_i - \\bar{y})^2\n$$\n\n3: 均方根误差（Root Mean Squared Error, RMSE）\n$$\n\\frac{1}{N} \\sqrt{ \\sum_{1}^{n}(y_i - \\bar{y})^2 }\n$$\n\n```\nIn [80]: from sklearn import metrics\n\nIn [81]: import numpy as np\n\nIn [82]: true = [10, 5, 3, 2]\n\nIn [83]: pred = [9, 5, 5, 3]\n\nIn [84]: print(\"MAE: \", metrics.mean_absolute_error(true,pred))\n('MAE: ', 1.0)\n\nIn [85]: print(\"MAE By Hand: \", (1+0+2+1)/4.)\n('MAE By Hand: ', 1.0)\n\nIn [86]: print(\"MSE: \", metrics.mean_squared_error(true,pred))\n('MSE: ', 1.5)\n\nIn [87]: print(\"MSE By Hand: \", (1 ** 2 + 0 ** 2 + 2 ** 2 + 1 ** 2 ) / 4.)\n('MSE By Hand: ', 1.5)\n\nIn [88]: print(\"RMSE: \", np.sqrt(metrics.mean_squared_error(true,pred)))\n('RMSE: ', 1.2247448713915889)\n\nIn [89]: print(\"RMSE By Hand: \", np.sqrt((1 ** 2 + 0 ** 2 + 2 ** 2 + 1 ** 2 ) / 4.))\n('RMSE By Hand: ', 1.2247448713915889)\n```\n\n---\n# 总结\n线性回归在现实中还是可以解决很多问题的，但是并不是万能的，后续我会继续整理逻辑回归，岭回归等相关回归的知识，如果你感觉有用，欢迎分享！\n\n  [1]: http://blog.csdn.net/gamer_gyt/article/details/78008144","source":"_posts/机器学习/回归分析之线性回归（N元线性回归）.md","raw":"---\ntitle: 回归分析之线性回归（N元线性回归）\ndate: 2017-09-29 16:45:14\ntags: [回归分析]\ncategories: 技术篇\n---\n\n在上一篇文章中我们介绍了 [ 回归分析之理论篇][1]，在其中我们有聊到线性回归和非线性回归，包括广义线性回归，这一篇文章我们来聊下回归分析中的线性回归。\n\n<!--More-->\n# 一元线性回归\n预测房价：\n输入编号\t| 平方米\t| 价格\n-|-|-\n1 |\t150 |\t6450\n2\t| 200\t| 7450\n3|\t250\t|8450\n4|\t300\t|9450\n5|\t350\t|11450\n6|\t400\t|15450\n7|\t600|\t18450\n\n针对上边这种一元数据来讲，我们可以构建的一元线性回归函数为\n$$\nH(x) = k*x + b\n$$\n其中H(x)为平方米价格表，k是一元回归系数，b为常数。最小二乘法的公式：\n$$\nk =\\frac{ \\sum_{1}^{n} (x_{i} - \\bar{x} )(y_{i} - \\bar{y}) } { \\sum_{1}^{n}(x_{i}-\\bar{x})^{2} }\n$$\n自己使用python代码实现为：\n```\ndef leastsq(x,y):\n    \"\"\"\n    x,y分别是要拟合的数据的自变量列表和因变量列表\n    \"\"\"\n    meanX = sum(x) * 1.0 / len(x)      # 求x的平均值\n    meanY = sum(y) * 1.0 / len(y)     # 求y的平均值\n\n    xSum = 0.0\n    ySum = 0.0\n\n    for i in range(len(x)):\n        xSum += (x[i] - meanX) * (y[i] - meanY)\n        ySum += (x[i] - meanX) ** 2\n\n    k = ySum/xSum\n    b = ySum - k * meanX\n\n    return k,b\n```\n\n使用python的scipy包进行计算:\n```\nleastsq(func, x0, args=(), Dfun=None, full_output=0, col_deriv=0, ftol=1.49012e-08, xtol=1.49012e-08, gtol=0.0, maxfev=0, epsfcn=None, factor=100, diag=None)\n\nfrom scipy.optimize import leastsq\nimport numpy as np\n\ndef fun(p, x):\n    \"\"\"\n    定义想要拟合的函数\n    \"\"\"\n    k,b = p    # 从参数p获得拟合的参数\n    return k*x + b\n\ndef err(p, x, y):\n    return fun(p,x) - y\n\n#定义起始的参数 即从 y = 1*x+1 开始，其实这个值可以随便设，只不过会影响到找到最优解的时间\np0 = [1,1]\n\n#将list类型转换为 numpy.ndarray 类型，最初我直接使用\n#list 类型,结果 leastsq函数报错，后来在别的blog上看到了，原来要将类型转\n#换为numpy的类型\n\nx1 = np.array([150,200,250,300,350,400,600])\ny1 = np.array([6450,7450,8450,9450,11450,15450,18450])\n\nxishu = leastsq(err, p0, args=(x1,y1))\n\nprint xishu[0]\n\n```\n\n当然python的leastsq函数不仅仅局限于一元一次的应用，也可以应用到一元二次，二元二次，多元多次等，具体可以看下这篇博客：http://www.cnblogs.com/NanShan2016/p/5493429.html\n\n# 多元线性回归\n总之：我们可以用python leastsq函数解决几乎所有的线性回归的问题了，比如说\n$$y = a * x^2 + b * x + c$$\n$$y = a * x_1^2 + b * x_1 + c * x_2 + d$$\n$$y = a * x_1^3 + b * x_1^2 + c * x_1 + d$$\n在使用时只需把参数列表和 fun 函数中的return 换一下，拿以下函数举例\n$$y = a * x_1^2 + b * x_1 + c * x_2 + d$$\n\n对应的python 代码是：\n```\nfrom scipy.optimize import leastsq\nimport numpy as np\n\n\ndef fun(p, x1, x2):\n    \"\"\"\n    定义想要拟合的函数\n    \"\"\"\n    a,b,c,d = p    # 从参数p获得拟合的参数\n    return a * (x1**2) + b * x1 + c * x2 + d\n\ndef err(p, x1, x2, y):\n    return fun(p,x1,x2) - y\n\n#定义起始的参数 即从 y = 1*x+1 开始，其实这个值可以随便设，只不过会影响到找到最优解的时间\np0 = [1,1,1,1]\n\n#将list类型转换为 numpy.ndarray 类型，最初我直接使用\n#list 类型,结果 leastsq函数报错，后来在别的blog上看到了，原来要将类型转\n#换为numpy的类型\n\nx1 = np.array([150,200,250,300,350,400,600])    # 面积\nx2 = np.array([4,2,7,9,12,14,15])               # 楼层\ny1 = np.array([6450,7450,8450,9450,11450,15450,18450])   # 价格/平方米\n\nxishu = leastsq(err, p0, args=(x1,x2,y1))\n\nprint xishu[0]\n```\n\n# sklearn中的线性回归应用\n## 普通最小二乘回归\n这里我们使用的是sklearn中的linear_model来模拟$$y=a * x_1 + b * x_2 + c$$\n\n```\nIn [1]: from sklearn.linear_model import LinearRegression\n\nIn [2]: linreg = LinearRegression()\n\nIn [3]: linreg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])\n\nIn [4]: linreg.coef_\nOut[4]: array([ 0.5,  0.5])\n\nIn [5]: linreg.intercept_\nOut[5]: 1.1102230246251565e-16\n\nIn [6]: linreg.predict([4,4])\nOut[6]: array([ 4.])\n\nIn [7]: zip([\"x1\",\"x2\"], linreg.coef_)\nOut[7]: [('x1', 0.5), ('x2', 0.49999999999999989)]\n```\n所以可得$$ y = 0.5 * x_1 + 0.5 * x_2 + 1.11e-16$$\n\nlinreg.coef_  为系数 a,b\n\nlinreg.intercept_ 为截距 c\n\n缺点：因为系数矩阵x与它的转置矩阵相乘得到的矩阵不能求逆，导致最小二乘法得到的回归系数不稳定，方差很大。\n\n\n## 多项式回归：基函数扩展线性模型\n机器学习中一种常见的模式是使用线性模型训练数据的非线性函数。这种方法保持了一般快速的线性方法的性能，同时允许它们适应更广泛的数据范围。\n\n例如，可以通过构造系数的多项式特征来扩展一个简单的线性回归。在标准线性回归的情况下，你可能有一个类似于二维数据的模型：\n$$\ny(w,x) = w_{0} + w_{1} x_{1} + w_{2} x_{2}\n$$\n\n如果我们想把抛物面拟合成数据而不是平面，我们可以结合二阶多项式的特征，使模型看起来像这样:\n$$\ny(w,x) = w_{0} + w_{1} x_{1} + w_{2} x_{2} + w_{3} x_{1}x_{2} + w_{4} x_{1}^2 + w_{5} x_{2}^2\n$$\n\n我们发现，这仍然是一个线性模型，想象着创建一个新变量：\n$$\nz = [x_{1},x_{2},x_{1} x_{2},x_{1}^2,x_{2}^2]\n$$\n\n可以把线性回归模型写成下边这种形式：\n$$\ny(w,x) = w_{0} + w_{1} z_{1} + w_{2} z_{2} + w_{3} z_{3} + w_{4} z_{4} + w_{5} z_{5}\n$$\n我们看到，所得的多项式回归与我们上面所考虑的线性模型相同（即模型在W中是线性的），可以用同样的方法来求解。通过考虑在用这些基函数建立的高维空间中的线性拟合，该模型具有灵活性，可以适应更广泛的数据范围。\n\n使用如下代码，将二维数据进行二元转换,转换规则为：\n$$\n[x_1, x_2] => [1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]\n$$\n\n```\nIn [15]: from sklearn.preprocessing import PolynomialFeatures\n\nIn [16]: import numpy as np\n\nIn [17]: X = np.arange(6).reshape(3,2)\n\nIn [18]: X\nOut[18]: \narray([[0, 1],\n       [2, 3],\n       [4, 5]])\n\nIn [19]: poly = PolynomialFeatures(degree=2)\n\nIn [20]: poly.fit_transform(X)\nOut[20]: \narray([[  1.,   0.,   1.,   0.,   0.,   1.],\n       [  1.,   2.,   3.,   4.,   6.,   9.],\n       [  1.,   4.,   5.,  16.,  20.,  25.]])\n```\n\n验证：\n```\nIn [38]: from sklearn.preprocessing import PolynomialFeatures\n\nIn [39]: from sklearn.linear_model import LinearRegression\n\nIn [40]: from sklearn.pipeline import Pipeline\n\nIn [41]: import numpy as np\n\nIn [42]: \n\nIn [42]: model = Pipeline( [ (\"poly\",PolynomialFeatures(degree=3)),(\"linear\",LinearRegression(fit_intercept=False)) ] )\n\nIn [43]: model\nOut[43]: Pipeline(steps=[('poly', PolynomialFeatures(degree=3, include_bias=True, interaction_only=False)), ('linear', LinearRegression(copy_X=True, fit_intercept=False, n_jobs=1, normalize=False))])\n\nIn [44]: x = np.arange(5)\n\nIn [45]: y = 3 - 2 * x + x ** 2 - x ** 3\n\nIn [46]: y\nOut[46]: array([  3,   1,  -5, -21, -53])\n\nIn [47]: model = model.fit(x[:,np.newaxis],y)\n\nIn [48]: model.named_steps['linear'].coef_\nOut[48]: array([ 3., -2.,  1., -1.])\n```\n我们可以看出最后求出的参数和一元三次方程是一致的。\n\n这里如果把degree改为2，y的方程也换一下，结果也是一致的\n```\nIn [51]: from sklearn.linear_model import LinearRegression\n\nIn [52]: from sklearn.preprocessing import PolynomialFeatures\n\nIn [53]: from sklearn.pipeline import Pipeline\n\nIn [54]: import numpy as np\n\nIn [55]: model = Pipeline( [ (\"poly\",PolynomialFeatures(degree=2)),(\"linear\",LinearRegression(fit_intercept=False)) ] )\n\nIn [56]: x = np.arange(5)\n\nIn [57]: y = 3 + 2 * x + x ** 2\n\nIn [58]: model = model.fit(x[:, np.newaxis], y)\n\nIn [59]: model.named_steps['linear'].coef_\nOut[59]: array([ 3., 2.,  1.])\n```\n\n\n## 线性回归的评测\n在[上一篇文章](http://note.youdao.com/)中我们聊到了回归模型的评测方法，解下来我们详细聊聊如何来评价一个回归模型的好坏。\n\n这里我们定义预测值和真实值分别为：\n```\ntrue = [10, 5, 3, 2]\npred = [9, 5, 5, 3]\n```\n\n1: 平均绝对误差（Mean Absolute Error, MAE）\n$$\n\\frac{1}{N}(\\sum_{1}^{n} |y_i - \\bar{y}|)\n$$\n\n2: 均方误差（Mean Squared Error, MSE）\n$$\n\\frac{1}{N}\\sum_{1}^{n}(y_i - \\bar{y})^2\n$$\n\n3: 均方根误差（Root Mean Squared Error, RMSE）\n$$\n\\frac{1}{N} \\sqrt{ \\sum_{1}^{n}(y_i - \\bar{y})^2 }\n$$\n\n```\nIn [80]: from sklearn import metrics\n\nIn [81]: import numpy as np\n\nIn [82]: true = [10, 5, 3, 2]\n\nIn [83]: pred = [9, 5, 5, 3]\n\nIn [84]: print(\"MAE: \", metrics.mean_absolute_error(true,pred))\n('MAE: ', 1.0)\n\nIn [85]: print(\"MAE By Hand: \", (1+0+2+1)/4.)\n('MAE By Hand: ', 1.0)\n\nIn [86]: print(\"MSE: \", metrics.mean_squared_error(true,pred))\n('MSE: ', 1.5)\n\nIn [87]: print(\"MSE By Hand: \", (1 ** 2 + 0 ** 2 + 2 ** 2 + 1 ** 2 ) / 4.)\n('MSE By Hand: ', 1.5)\n\nIn [88]: print(\"RMSE: \", np.sqrt(metrics.mean_squared_error(true,pred)))\n('RMSE: ', 1.2247448713915889)\n\nIn [89]: print(\"RMSE By Hand: \", np.sqrt((1 ** 2 + 0 ** 2 + 2 ** 2 + 1 ** 2 ) / 4.))\n('RMSE By Hand: ', 1.2247448713915889)\n```\n\n---\n# 总结\n线性回归在现实中还是可以解决很多问题的，但是并不是万能的，后续我会继续整理逻辑回归，岭回归等相关回归的知识，如果你感觉有用，欢迎分享！\n\n  [1]: http://blog.csdn.net/gamer_gyt/article/details/78008144","slug":"机器学习/回归分析之线性回归（N元线性回归）","published":1,"updated":"2017-12-19T02:54:56.787Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjbd1r26h000ykxuwqkz87wmw","content":"<p>在上一篇文章中我们介绍了 <a href=\"http://blog.csdn.net/gamer_gyt/article/details/78008144\" target=\"_blank\" rel=\"external\"> 回归分析之理论篇</a>，在其中我们有聊到线性回归和非线性回归，包括广义线性回归，这一篇文章我们来聊下回归分析中的线性回归。</p>\n<a id=\"more\"></a>\n<h1 id=\"一元线性回归\"><a href=\"#一元线性回归\" class=\"headerlink\" title=\"一元线性回归\"></a>一元线性回归</h1><p>预测房价：<br>输入编号    | 平方米    | 价格<br>-|-|-<br>1 |    150 |    6450<br>2    | 200    | 7450<br>3|    250    |8450<br>4|    300    |9450<br>5|    350    |11450<br>6|    400    |15450<br>7|    600|    18450</p>\n<p>针对上边这种一元数据来讲，我们可以构建的一元线性回归函数为</p>\n<script type=\"math/tex; mode=display\">\nH(x) = k*x + b</script><p>其中H(x)为平方米价格表，k是一元回归系数，b为常数。最小二乘法的公式：</p>\n<script type=\"math/tex; mode=display\">\nk =\\frac{ \\sum_{1}^{n} (x_{i} - \\bar{x} )(y_{i} - \\bar{y}) } { \\sum_{1}^{n}(x_{i}-\\bar{x})^{2} }</script><p>自己使用python代码实现为：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def leastsq(x,y):</span><br><span class=\"line\">    &quot;&quot;&quot;</span><br><span class=\"line\">    x,y分别是要拟合的数据的自变量列表和因变量列表</span><br><span class=\"line\">    &quot;&quot;&quot;</span><br><span class=\"line\">    meanX = sum(x) * 1.0 / len(x)      # 求x的平均值</span><br><span class=\"line\">    meanY = sum(y) * 1.0 / len(y)     # 求y的平均值</span><br><span class=\"line\"></span><br><span class=\"line\">    xSum = 0.0</span><br><span class=\"line\">    ySum = 0.0</span><br><span class=\"line\"></span><br><span class=\"line\">    for i in range(len(x)):</span><br><span class=\"line\">        xSum += (x[i] - meanX) * (y[i] - meanY)</span><br><span class=\"line\">        ySum += (x[i] - meanX) ** 2</span><br><span class=\"line\"></span><br><span class=\"line\">    k = ySum/xSum</span><br><span class=\"line\">    b = ySum - k * meanX</span><br><span class=\"line\"></span><br><span class=\"line\">    return k,b</span><br></pre></td></tr></table></figure></p>\n<p>使用python的scipy包进行计算:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">leastsq(func, x0, args=(), Dfun=None, full_output=0, col_deriv=0, ftol=1.49012e-08, xtol=1.49012e-08, gtol=0.0, maxfev=0, epsfcn=None, factor=100, diag=None)</span><br><span class=\"line\"></span><br><span class=\"line\">from scipy.optimize import leastsq</span><br><span class=\"line\">import numpy as np</span><br><span class=\"line\"></span><br><span class=\"line\">def fun(p, x):</span><br><span class=\"line\">    &quot;&quot;&quot;</span><br><span class=\"line\">    定义想要拟合的函数</span><br><span class=\"line\">    &quot;&quot;&quot;</span><br><span class=\"line\">    k,b = p    # 从参数p获得拟合的参数</span><br><span class=\"line\">    return k*x + b</span><br><span class=\"line\"></span><br><span class=\"line\">def err(p, x, y):</span><br><span class=\"line\">    return fun(p,x) - y</span><br><span class=\"line\"></span><br><span class=\"line\">#定义起始的参数 即从 y = 1*x+1 开始，其实这个值可以随便设，只不过会影响到找到最优解的时间</span><br><span class=\"line\">p0 = [1,1]</span><br><span class=\"line\"></span><br><span class=\"line\">#将list类型转换为 numpy.ndarray 类型，最初我直接使用</span><br><span class=\"line\">#list 类型,结果 leastsq函数报错，后来在别的blog上看到了，原来要将类型转</span><br><span class=\"line\">#换为numpy的类型</span><br><span class=\"line\"></span><br><span class=\"line\">x1 = np.array([150,200,250,300,350,400,600])</span><br><span class=\"line\">y1 = np.array([6450,7450,8450,9450,11450,15450,18450])</span><br><span class=\"line\"></span><br><span class=\"line\">xishu = leastsq(err, p0, args=(x1,y1))</span><br><span class=\"line\"></span><br><span class=\"line\">print xishu[0]</span><br></pre></td></tr></table></figure></p>\n<p>当然python的leastsq函数不仅仅局限于一元一次的应用，也可以应用到一元二次，二元二次，多元多次等，具体可以看下这篇博客：<a href=\"http://www.cnblogs.com/NanShan2016/p/5493429.html\" target=\"_blank\" rel=\"external\">http://www.cnblogs.com/NanShan2016/p/5493429.html</a></p>\n<h1 id=\"多元线性回归\"><a href=\"#多元线性回归\" class=\"headerlink\" title=\"多元线性回归\"></a>多元线性回归</h1><p>总之：我们可以用python leastsq函数解决几乎所有的线性回归的问题了，比如说</p>\n<script type=\"math/tex; mode=display\">y = a * x^2 + b * x + c</script><script type=\"math/tex; mode=display\">y = a * x_1^2 + b * x_1 + c * x_2 + d</script><script type=\"math/tex; mode=display\">y = a * x_1^3 + b * x_1^2 + c * x_1 + d</script><p>在使用时只需把参数列表和 fun 函数中的return 换一下，拿以下函数举例</p>\n<script type=\"math/tex; mode=display\">y = a * x_1^2 + b * x_1 + c * x_2 + d</script><p>对应的python 代码是：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from scipy.optimize import leastsq</span><br><span class=\"line\">import numpy as np</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">def fun(p, x1, x2):</span><br><span class=\"line\">    &quot;&quot;&quot;</span><br><span class=\"line\">    定义想要拟合的函数</span><br><span class=\"line\">    &quot;&quot;&quot;</span><br><span class=\"line\">    a,b,c,d = p    # 从参数p获得拟合的参数</span><br><span class=\"line\">    return a * (x1**2) + b * x1 + c * x2 + d</span><br><span class=\"line\"></span><br><span class=\"line\">def err(p, x1, x2, y):</span><br><span class=\"line\">    return fun(p,x1,x2) - y</span><br><span class=\"line\"></span><br><span class=\"line\">#定义起始的参数 即从 y = 1*x+1 开始，其实这个值可以随便设，只不过会影响到找到最优解的时间</span><br><span class=\"line\">p0 = [1,1,1,1]</span><br><span class=\"line\"></span><br><span class=\"line\">#将list类型转换为 numpy.ndarray 类型，最初我直接使用</span><br><span class=\"line\">#list 类型,结果 leastsq函数报错，后来在别的blog上看到了，原来要将类型转</span><br><span class=\"line\">#换为numpy的类型</span><br><span class=\"line\"></span><br><span class=\"line\">x1 = np.array([150,200,250,300,350,400,600])    # 面积</span><br><span class=\"line\">x2 = np.array([4,2,7,9,12,14,15])               # 楼层</span><br><span class=\"line\">y1 = np.array([6450,7450,8450,9450,11450,15450,18450])   # 价格/平方米</span><br><span class=\"line\"></span><br><span class=\"line\">xishu = leastsq(err, p0, args=(x1,x2,y1))</span><br><span class=\"line\"></span><br><span class=\"line\">print xishu[0]</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"sklearn中的线性回归应用\"><a href=\"#sklearn中的线性回归应用\" class=\"headerlink\" title=\"sklearn中的线性回归应用\"></a>sklearn中的线性回归应用</h1><h2 id=\"普通最小二乘回归\"><a href=\"#普通最小二乘回归\" class=\"headerlink\" title=\"普通最小二乘回归\"></a>普通最小二乘回归</h2><p>这里我们使用的是sklearn中的linear_model来模拟<script type=\"math/tex\">y=a * x_1 + b * x_2 + c</script></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [1]: from sklearn.linear_model import LinearRegression</span><br><span class=\"line\"></span><br><span class=\"line\">In [2]: linreg = LinearRegression()</span><br><span class=\"line\"></span><br><span class=\"line\">In [3]: linreg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])</span><br><span class=\"line\"></span><br><span class=\"line\">In [4]: linreg.coef_</span><br><span class=\"line\">Out[4]: array([ 0.5,  0.5])</span><br><span class=\"line\"></span><br><span class=\"line\">In [5]: linreg.intercept_</span><br><span class=\"line\">Out[5]: 1.1102230246251565e-16</span><br><span class=\"line\"></span><br><span class=\"line\">In [6]: linreg.predict([4,4])</span><br><span class=\"line\">Out[6]: array([ 4.])</span><br><span class=\"line\"></span><br><span class=\"line\">In [7]: zip([&quot;x1&quot;,&quot;x2&quot;], linreg.coef_)</span><br><span class=\"line\">Out[7]: [(&apos;x1&apos;, 0.5), (&apos;x2&apos;, 0.49999999999999989)]</span><br></pre></td></tr></table></figure>\n<p>所以可得<script type=\"math/tex\">y = 0.5 * x_1 + 0.5 * x_2 + 1.11e-16</script></p>\n<p>linreg.coef_  为系数 a,b</p>\n<p>linreg.intercept_ 为截距 c</p>\n<p>缺点：因为系数矩阵x与它的转置矩阵相乘得到的矩阵不能求逆，导致最小二乘法得到的回归系数不稳定，方差很大。</p>\n<h2 id=\"多项式回归：基函数扩展线性模型\"><a href=\"#多项式回归：基函数扩展线性模型\" class=\"headerlink\" title=\"多项式回归：基函数扩展线性模型\"></a>多项式回归：基函数扩展线性模型</h2><p>机器学习中一种常见的模式是使用线性模型训练数据的非线性函数。这种方法保持了一般快速的线性方法的性能，同时允许它们适应更广泛的数据范围。</p>\n<p>例如，可以通过构造系数的多项式特征来扩展一个简单的线性回归。在标准线性回归的情况下，你可能有一个类似于二维数据的模型：</p>\n<script type=\"math/tex; mode=display\">\ny(w,x) = w_{0} + w_{1} x_{1} + w_{2} x_{2}</script><p>如果我们想把抛物面拟合成数据而不是平面，我们可以结合二阶多项式的特征，使模型看起来像这样:</p>\n<script type=\"math/tex; mode=display\">\ny(w,x) = w_{0} + w_{1} x_{1} + w_{2} x_{2} + w_{3} x_{1}x_{2} + w_{4} x_{1}^2 + w_{5} x_{2}^2</script><p>我们发现，这仍然是一个线性模型，想象着创建一个新变量：</p>\n<script type=\"math/tex; mode=display\">\nz = [x_{1},x_{2},x_{1} x_{2},x_{1}^2,x_{2}^2]</script><p>可以把线性回归模型写成下边这种形式：</p>\n<script type=\"math/tex; mode=display\">\ny(w,x) = w_{0} + w_{1} z_{1} + w_{2} z_{2} + w_{3} z_{3} + w_{4} z_{4} + w_{5} z_{5}</script><p>我们看到，所得的多项式回归与我们上面所考虑的线性模型相同（即模型在W中是线性的），可以用同样的方法来求解。通过考虑在用这些基函数建立的高维空间中的线性拟合，该模型具有灵活性，可以适应更广泛的数据范围。</p>\n<p>使用如下代码，将二维数据进行二元转换,转换规则为：</p>\n<script type=\"math/tex; mode=display\">\n[x_1, x_2] => [1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]</script><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [15]: from sklearn.preprocessing import PolynomialFeatures</span><br><span class=\"line\"></span><br><span class=\"line\">In [16]: import numpy as np</span><br><span class=\"line\"></span><br><span class=\"line\">In [17]: X = np.arange(6).reshape(3,2)</span><br><span class=\"line\"></span><br><span class=\"line\">In [18]: X</span><br><span class=\"line\">Out[18]: </span><br><span class=\"line\">array([[0, 1],</span><br><span class=\"line\">       [2, 3],</span><br><span class=\"line\">       [4, 5]])</span><br><span class=\"line\"></span><br><span class=\"line\">In [19]: poly = PolynomialFeatures(degree=2)</span><br><span class=\"line\"></span><br><span class=\"line\">In [20]: poly.fit_transform(X)</span><br><span class=\"line\">Out[20]: </span><br><span class=\"line\">array([[  1.,   0.,   1.,   0.,   0.,   1.],</span><br><span class=\"line\">       [  1.,   2.,   3.,   4.,   6.,   9.],</span><br><span class=\"line\">       [  1.,   4.,   5.,  16.,  20.,  25.]])</span><br></pre></td></tr></table></figure>\n<p>验证：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [38]: from sklearn.preprocessing import PolynomialFeatures</span><br><span class=\"line\"></span><br><span class=\"line\">In [39]: from sklearn.linear_model import LinearRegression</span><br><span class=\"line\"></span><br><span class=\"line\">In [40]: from sklearn.pipeline import Pipeline</span><br><span class=\"line\"></span><br><span class=\"line\">In [41]: import numpy as np</span><br><span class=\"line\"></span><br><span class=\"line\">In [42]: </span><br><span class=\"line\"></span><br><span class=\"line\">In [42]: model = Pipeline( [ (&quot;poly&quot;,PolynomialFeatures(degree=3)),(&quot;linear&quot;,LinearRegression(fit_intercept=False)) ] )</span><br><span class=\"line\"></span><br><span class=\"line\">In [43]: model</span><br><span class=\"line\">Out[43]: Pipeline(steps=[(&apos;poly&apos;, PolynomialFeatures(degree=3, include_bias=True, interaction_only=False)), (&apos;linear&apos;, LinearRegression(copy_X=True, fit_intercept=False, n_jobs=1, normalize=False))])</span><br><span class=\"line\"></span><br><span class=\"line\">In [44]: x = np.arange(5)</span><br><span class=\"line\"></span><br><span class=\"line\">In [45]: y = 3 - 2 * x + x ** 2 - x ** 3</span><br><span class=\"line\"></span><br><span class=\"line\">In [46]: y</span><br><span class=\"line\">Out[46]: array([  3,   1,  -5, -21, -53])</span><br><span class=\"line\"></span><br><span class=\"line\">In [47]: model = model.fit(x[:,np.newaxis],y)</span><br><span class=\"line\"></span><br><span class=\"line\">In [48]: model.named_steps[&apos;linear&apos;].coef_</span><br><span class=\"line\">Out[48]: array([ 3., -2.,  1., -1.])</span><br></pre></td></tr></table></figure></p>\n<p>我们可以看出最后求出的参数和一元三次方程是一致的。</p>\n<p>这里如果把degree改为2，y的方程也换一下，结果也是一致的<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [51]: from sklearn.linear_model import LinearRegression</span><br><span class=\"line\"></span><br><span class=\"line\">In [52]: from sklearn.preprocessing import PolynomialFeatures</span><br><span class=\"line\"></span><br><span class=\"line\">In [53]: from sklearn.pipeline import Pipeline</span><br><span class=\"line\"></span><br><span class=\"line\">In [54]: import numpy as np</span><br><span class=\"line\"></span><br><span class=\"line\">In [55]: model = Pipeline( [ (&quot;poly&quot;,PolynomialFeatures(degree=2)),(&quot;linear&quot;,LinearRegression(fit_intercept=False)) ] )</span><br><span class=\"line\"></span><br><span class=\"line\">In [56]: x = np.arange(5)</span><br><span class=\"line\"></span><br><span class=\"line\">In [57]: y = 3 + 2 * x + x ** 2</span><br><span class=\"line\"></span><br><span class=\"line\">In [58]: model = model.fit(x[:, np.newaxis], y)</span><br><span class=\"line\"></span><br><span class=\"line\">In [59]: model.named_steps[&apos;linear&apos;].coef_</span><br><span class=\"line\">Out[59]: array([ 3., 2.,  1.])</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"线性回归的评测\"><a href=\"#线性回归的评测\" class=\"headerlink\" title=\"线性回归的评测\"></a>线性回归的评测</h2><p>在<a href=\"http://note.youdao.com/\" target=\"_blank\" rel=\"external\">上一篇文章</a>中我们聊到了回归模型的评测方法，解下来我们详细聊聊如何来评价一个回归模型的好坏。</p>\n<p>这里我们定义预测值和真实值分别为：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">true = [10, 5, 3, 2]</span><br><span class=\"line\">pred = [9, 5, 5, 3]</span><br></pre></td></tr></table></figure></p>\n<p>1: 平均绝对误差（Mean Absolute Error, MAE）</p>\n<script type=\"math/tex; mode=display\">\n\\frac{1}{N}(\\sum_{1}^{n} |y_i - \\bar{y}|)</script><p>2: 均方误差（Mean Squared Error, MSE）</p>\n<script type=\"math/tex; mode=display\">\n\\frac{1}{N}\\sum_{1}^{n}(y_i - \\bar{y})^2</script><p>3: 均方根误差（Root Mean Squared Error, RMSE）</p>\n<script type=\"math/tex; mode=display\">\n\\frac{1}{N} \\sqrt{ \\sum_{1}^{n}(y_i - \\bar{y})^2 }</script><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [80]: from sklearn import metrics</span><br><span class=\"line\"></span><br><span class=\"line\">In [81]: import numpy as np</span><br><span class=\"line\"></span><br><span class=\"line\">In [82]: true = [10, 5, 3, 2]</span><br><span class=\"line\"></span><br><span class=\"line\">In [83]: pred = [9, 5, 5, 3]</span><br><span class=\"line\"></span><br><span class=\"line\">In [84]: print(&quot;MAE: &quot;, metrics.mean_absolute_error(true,pred))</span><br><span class=\"line\">(&apos;MAE: &apos;, 1.0)</span><br><span class=\"line\"></span><br><span class=\"line\">In [85]: print(&quot;MAE By Hand: &quot;, (1+0+2+1)/4.)</span><br><span class=\"line\">(&apos;MAE By Hand: &apos;, 1.0)</span><br><span class=\"line\"></span><br><span class=\"line\">In [86]: print(&quot;MSE: &quot;, metrics.mean_squared_error(true,pred))</span><br><span class=\"line\">(&apos;MSE: &apos;, 1.5)</span><br><span class=\"line\"></span><br><span class=\"line\">In [87]: print(&quot;MSE By Hand: &quot;, (1 ** 2 + 0 ** 2 + 2 ** 2 + 1 ** 2 ) / 4.)</span><br><span class=\"line\">(&apos;MSE By Hand: &apos;, 1.5)</span><br><span class=\"line\"></span><br><span class=\"line\">In [88]: print(&quot;RMSE: &quot;, np.sqrt(metrics.mean_squared_error(true,pred)))</span><br><span class=\"line\">(&apos;RMSE: &apos;, 1.2247448713915889)</span><br><span class=\"line\"></span><br><span class=\"line\">In [89]: print(&quot;RMSE By Hand: &quot;, np.sqrt((1 ** 2 + 0 ** 2 + 2 ** 2 + 1 ** 2 ) / 4.))</span><br><span class=\"line\">(&apos;RMSE By Hand: &apos;, 1.2247448713915889)</span><br></pre></td></tr></table></figure>\n<hr>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>线性回归在现实中还是可以解决很多问题的，但是并不是万能的，后续我会继续整理逻辑回归，岭回归等相关回归的知识，如果你感觉有用，欢迎分享！</p>\n","site":{"data":{}},"excerpt":"<p>在上一篇文章中我们介绍了 <a href=\"http://blog.csdn.net/gamer_gyt/article/details/78008144\" target=\"_blank\" rel=\"external\"> 回归分析之理论篇</a>，在其中我们有聊到线性回归和非线性回归，包括广义线性回归，这一篇文章我们来聊下回归分析中的线性回归。</p>","more":"<h1 id=\"一元线性回归\"><a href=\"#一元线性回归\" class=\"headerlink\" title=\"一元线性回归\"></a>一元线性回归</h1><p>预测房价：<br>输入编号    | 平方米    | 价格<br>-|-|-<br>1 |    150 |    6450<br>2    | 200    | 7450<br>3|    250    |8450<br>4|    300    |9450<br>5|    350    |11450<br>6|    400    |15450<br>7|    600|    18450</p>\n<p>针对上边这种一元数据来讲，我们可以构建的一元线性回归函数为</p>\n<script type=\"math/tex; mode=display\">\nH(x) = k*x + b</script><p>其中H(x)为平方米价格表，k是一元回归系数，b为常数。最小二乘法的公式：</p>\n<script type=\"math/tex; mode=display\">\nk =\\frac{ \\sum_{1}^{n} (x_{i} - \\bar{x} )(y_{i} - \\bar{y}) } { \\sum_{1}^{n}(x_{i}-\\bar{x})^{2} }</script><p>自己使用python代码实现为：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def leastsq(x,y):</span><br><span class=\"line\">    &quot;&quot;&quot;</span><br><span class=\"line\">    x,y分别是要拟合的数据的自变量列表和因变量列表</span><br><span class=\"line\">    &quot;&quot;&quot;</span><br><span class=\"line\">    meanX = sum(x) * 1.0 / len(x)      # 求x的平均值</span><br><span class=\"line\">    meanY = sum(y) * 1.0 / len(y)     # 求y的平均值</span><br><span class=\"line\"></span><br><span class=\"line\">    xSum = 0.0</span><br><span class=\"line\">    ySum = 0.0</span><br><span class=\"line\"></span><br><span class=\"line\">    for i in range(len(x)):</span><br><span class=\"line\">        xSum += (x[i] - meanX) * (y[i] - meanY)</span><br><span class=\"line\">        ySum += (x[i] - meanX) ** 2</span><br><span class=\"line\"></span><br><span class=\"line\">    k = ySum/xSum</span><br><span class=\"line\">    b = ySum - k * meanX</span><br><span class=\"line\"></span><br><span class=\"line\">    return k,b</span><br></pre></td></tr></table></figure></p>\n<p>使用python的scipy包进行计算:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">leastsq(func, x0, args=(), Dfun=None, full_output=0, col_deriv=0, ftol=1.49012e-08, xtol=1.49012e-08, gtol=0.0, maxfev=0, epsfcn=None, factor=100, diag=None)</span><br><span class=\"line\"></span><br><span class=\"line\">from scipy.optimize import leastsq</span><br><span class=\"line\">import numpy as np</span><br><span class=\"line\"></span><br><span class=\"line\">def fun(p, x):</span><br><span class=\"line\">    &quot;&quot;&quot;</span><br><span class=\"line\">    定义想要拟合的函数</span><br><span class=\"line\">    &quot;&quot;&quot;</span><br><span class=\"line\">    k,b = p    # 从参数p获得拟合的参数</span><br><span class=\"line\">    return k*x + b</span><br><span class=\"line\"></span><br><span class=\"line\">def err(p, x, y):</span><br><span class=\"line\">    return fun(p,x) - y</span><br><span class=\"line\"></span><br><span class=\"line\">#定义起始的参数 即从 y = 1*x+1 开始，其实这个值可以随便设，只不过会影响到找到最优解的时间</span><br><span class=\"line\">p0 = [1,1]</span><br><span class=\"line\"></span><br><span class=\"line\">#将list类型转换为 numpy.ndarray 类型，最初我直接使用</span><br><span class=\"line\">#list 类型,结果 leastsq函数报错，后来在别的blog上看到了，原来要将类型转</span><br><span class=\"line\">#换为numpy的类型</span><br><span class=\"line\"></span><br><span class=\"line\">x1 = np.array([150,200,250,300,350,400,600])</span><br><span class=\"line\">y1 = np.array([6450,7450,8450,9450,11450,15450,18450])</span><br><span class=\"line\"></span><br><span class=\"line\">xishu = leastsq(err, p0, args=(x1,y1))</span><br><span class=\"line\"></span><br><span class=\"line\">print xishu[0]</span><br></pre></td></tr></table></figure></p>\n<p>当然python的leastsq函数不仅仅局限于一元一次的应用，也可以应用到一元二次，二元二次，多元多次等，具体可以看下这篇博客：<a href=\"http://www.cnblogs.com/NanShan2016/p/5493429.html\" target=\"_blank\" rel=\"external\">http://www.cnblogs.com/NanShan2016/p/5493429.html</a></p>\n<h1 id=\"多元线性回归\"><a href=\"#多元线性回归\" class=\"headerlink\" title=\"多元线性回归\"></a>多元线性回归</h1><p>总之：我们可以用python leastsq函数解决几乎所有的线性回归的问题了，比如说</p>\n<script type=\"math/tex; mode=display\">y = a * x^2 + b * x + c</script><script type=\"math/tex; mode=display\">y = a * x_1^2 + b * x_1 + c * x_2 + d</script><script type=\"math/tex; mode=display\">y = a * x_1^3 + b * x_1^2 + c * x_1 + d</script><p>在使用时只需把参数列表和 fun 函数中的return 换一下，拿以下函数举例</p>\n<script type=\"math/tex; mode=display\">y = a * x_1^2 + b * x_1 + c * x_2 + d</script><p>对应的python 代码是：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from scipy.optimize import leastsq</span><br><span class=\"line\">import numpy as np</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">def fun(p, x1, x2):</span><br><span class=\"line\">    &quot;&quot;&quot;</span><br><span class=\"line\">    定义想要拟合的函数</span><br><span class=\"line\">    &quot;&quot;&quot;</span><br><span class=\"line\">    a,b,c,d = p    # 从参数p获得拟合的参数</span><br><span class=\"line\">    return a * (x1**2) + b * x1 + c * x2 + d</span><br><span class=\"line\"></span><br><span class=\"line\">def err(p, x1, x2, y):</span><br><span class=\"line\">    return fun(p,x1,x2) - y</span><br><span class=\"line\"></span><br><span class=\"line\">#定义起始的参数 即从 y = 1*x+1 开始，其实这个值可以随便设，只不过会影响到找到最优解的时间</span><br><span class=\"line\">p0 = [1,1,1,1]</span><br><span class=\"line\"></span><br><span class=\"line\">#将list类型转换为 numpy.ndarray 类型，最初我直接使用</span><br><span class=\"line\">#list 类型,结果 leastsq函数报错，后来在别的blog上看到了，原来要将类型转</span><br><span class=\"line\">#换为numpy的类型</span><br><span class=\"line\"></span><br><span class=\"line\">x1 = np.array([150,200,250,300,350,400,600])    # 面积</span><br><span class=\"line\">x2 = np.array([4,2,7,9,12,14,15])               # 楼层</span><br><span class=\"line\">y1 = np.array([6450,7450,8450,9450,11450,15450,18450])   # 价格/平方米</span><br><span class=\"line\"></span><br><span class=\"line\">xishu = leastsq(err, p0, args=(x1,x2,y1))</span><br><span class=\"line\"></span><br><span class=\"line\">print xishu[0]</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"sklearn中的线性回归应用\"><a href=\"#sklearn中的线性回归应用\" class=\"headerlink\" title=\"sklearn中的线性回归应用\"></a>sklearn中的线性回归应用</h1><h2 id=\"普通最小二乘回归\"><a href=\"#普通最小二乘回归\" class=\"headerlink\" title=\"普通最小二乘回归\"></a>普通最小二乘回归</h2><p>这里我们使用的是sklearn中的linear_model来模拟<script type=\"math/tex\">y=a * x_1 + b * x_2 + c</script></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [1]: from sklearn.linear_model import LinearRegression</span><br><span class=\"line\"></span><br><span class=\"line\">In [2]: linreg = LinearRegression()</span><br><span class=\"line\"></span><br><span class=\"line\">In [3]: linreg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])</span><br><span class=\"line\"></span><br><span class=\"line\">In [4]: linreg.coef_</span><br><span class=\"line\">Out[4]: array([ 0.5,  0.5])</span><br><span class=\"line\"></span><br><span class=\"line\">In [5]: linreg.intercept_</span><br><span class=\"line\">Out[5]: 1.1102230246251565e-16</span><br><span class=\"line\"></span><br><span class=\"line\">In [6]: linreg.predict([4,4])</span><br><span class=\"line\">Out[6]: array([ 4.])</span><br><span class=\"line\"></span><br><span class=\"line\">In [7]: zip([&quot;x1&quot;,&quot;x2&quot;], linreg.coef_)</span><br><span class=\"line\">Out[7]: [(&apos;x1&apos;, 0.5), (&apos;x2&apos;, 0.49999999999999989)]</span><br></pre></td></tr></table></figure>\n<p>所以可得<script type=\"math/tex\">y = 0.5 * x_1 + 0.5 * x_2 + 1.11e-16</script></p>\n<p>linreg.coef_  为系数 a,b</p>\n<p>linreg.intercept_ 为截距 c</p>\n<p>缺点：因为系数矩阵x与它的转置矩阵相乘得到的矩阵不能求逆，导致最小二乘法得到的回归系数不稳定，方差很大。</p>\n<h2 id=\"多项式回归：基函数扩展线性模型\"><a href=\"#多项式回归：基函数扩展线性模型\" class=\"headerlink\" title=\"多项式回归：基函数扩展线性模型\"></a>多项式回归：基函数扩展线性模型</h2><p>机器学习中一种常见的模式是使用线性模型训练数据的非线性函数。这种方法保持了一般快速的线性方法的性能，同时允许它们适应更广泛的数据范围。</p>\n<p>例如，可以通过构造系数的多项式特征来扩展一个简单的线性回归。在标准线性回归的情况下，你可能有一个类似于二维数据的模型：</p>\n<script type=\"math/tex; mode=display\">\ny(w,x) = w_{0} + w_{1} x_{1} + w_{2} x_{2}</script><p>如果我们想把抛物面拟合成数据而不是平面，我们可以结合二阶多项式的特征，使模型看起来像这样:</p>\n<script type=\"math/tex; mode=display\">\ny(w,x) = w_{0} + w_{1} x_{1} + w_{2} x_{2} + w_{3} x_{1}x_{2} + w_{4} x_{1}^2 + w_{5} x_{2}^2</script><p>我们发现，这仍然是一个线性模型，想象着创建一个新变量：</p>\n<script type=\"math/tex; mode=display\">\nz = [x_{1},x_{2},x_{1} x_{2},x_{1}^2,x_{2}^2]</script><p>可以把线性回归模型写成下边这种形式：</p>\n<script type=\"math/tex; mode=display\">\ny(w,x) = w_{0} + w_{1} z_{1} + w_{2} z_{2} + w_{3} z_{3} + w_{4} z_{4} + w_{5} z_{5}</script><p>我们看到，所得的多项式回归与我们上面所考虑的线性模型相同（即模型在W中是线性的），可以用同样的方法来求解。通过考虑在用这些基函数建立的高维空间中的线性拟合，该模型具有灵活性，可以适应更广泛的数据范围。</p>\n<p>使用如下代码，将二维数据进行二元转换,转换规则为：</p>\n<script type=\"math/tex; mode=display\">\n[x_1, x_2] => [1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]</script><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [15]: from sklearn.preprocessing import PolynomialFeatures</span><br><span class=\"line\"></span><br><span class=\"line\">In [16]: import numpy as np</span><br><span class=\"line\"></span><br><span class=\"line\">In [17]: X = np.arange(6).reshape(3,2)</span><br><span class=\"line\"></span><br><span class=\"line\">In [18]: X</span><br><span class=\"line\">Out[18]: </span><br><span class=\"line\">array([[0, 1],</span><br><span class=\"line\">       [2, 3],</span><br><span class=\"line\">       [4, 5]])</span><br><span class=\"line\"></span><br><span class=\"line\">In [19]: poly = PolynomialFeatures(degree=2)</span><br><span class=\"line\"></span><br><span class=\"line\">In [20]: poly.fit_transform(X)</span><br><span class=\"line\">Out[20]: </span><br><span class=\"line\">array([[  1.,   0.,   1.,   0.,   0.,   1.],</span><br><span class=\"line\">       [  1.,   2.,   3.,   4.,   6.,   9.],</span><br><span class=\"line\">       [  1.,   4.,   5.,  16.,  20.,  25.]])</span><br></pre></td></tr></table></figure>\n<p>验证：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [38]: from sklearn.preprocessing import PolynomialFeatures</span><br><span class=\"line\"></span><br><span class=\"line\">In [39]: from sklearn.linear_model import LinearRegression</span><br><span class=\"line\"></span><br><span class=\"line\">In [40]: from sklearn.pipeline import Pipeline</span><br><span class=\"line\"></span><br><span class=\"line\">In [41]: import numpy as np</span><br><span class=\"line\"></span><br><span class=\"line\">In [42]: </span><br><span class=\"line\"></span><br><span class=\"line\">In [42]: model = Pipeline( [ (&quot;poly&quot;,PolynomialFeatures(degree=3)),(&quot;linear&quot;,LinearRegression(fit_intercept=False)) ] )</span><br><span class=\"line\"></span><br><span class=\"line\">In [43]: model</span><br><span class=\"line\">Out[43]: Pipeline(steps=[(&apos;poly&apos;, PolynomialFeatures(degree=3, include_bias=True, interaction_only=False)), (&apos;linear&apos;, LinearRegression(copy_X=True, fit_intercept=False, n_jobs=1, normalize=False))])</span><br><span class=\"line\"></span><br><span class=\"line\">In [44]: x = np.arange(5)</span><br><span class=\"line\"></span><br><span class=\"line\">In [45]: y = 3 - 2 * x + x ** 2 - x ** 3</span><br><span class=\"line\"></span><br><span class=\"line\">In [46]: y</span><br><span class=\"line\">Out[46]: array([  3,   1,  -5, -21, -53])</span><br><span class=\"line\"></span><br><span class=\"line\">In [47]: model = model.fit(x[:,np.newaxis],y)</span><br><span class=\"line\"></span><br><span class=\"line\">In [48]: model.named_steps[&apos;linear&apos;].coef_</span><br><span class=\"line\">Out[48]: array([ 3., -2.,  1., -1.])</span><br></pre></td></tr></table></figure></p>\n<p>我们可以看出最后求出的参数和一元三次方程是一致的。</p>\n<p>这里如果把degree改为2，y的方程也换一下，结果也是一致的<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [51]: from sklearn.linear_model import LinearRegression</span><br><span class=\"line\"></span><br><span class=\"line\">In [52]: from sklearn.preprocessing import PolynomialFeatures</span><br><span class=\"line\"></span><br><span class=\"line\">In [53]: from sklearn.pipeline import Pipeline</span><br><span class=\"line\"></span><br><span class=\"line\">In [54]: import numpy as np</span><br><span class=\"line\"></span><br><span class=\"line\">In [55]: model = Pipeline( [ (&quot;poly&quot;,PolynomialFeatures(degree=2)),(&quot;linear&quot;,LinearRegression(fit_intercept=False)) ] )</span><br><span class=\"line\"></span><br><span class=\"line\">In [56]: x = np.arange(5)</span><br><span class=\"line\"></span><br><span class=\"line\">In [57]: y = 3 + 2 * x + x ** 2</span><br><span class=\"line\"></span><br><span class=\"line\">In [58]: model = model.fit(x[:, np.newaxis], y)</span><br><span class=\"line\"></span><br><span class=\"line\">In [59]: model.named_steps[&apos;linear&apos;].coef_</span><br><span class=\"line\">Out[59]: array([ 3., 2.,  1.])</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"线性回归的评测\"><a href=\"#线性回归的评测\" class=\"headerlink\" title=\"线性回归的评测\"></a>线性回归的评测</h2><p>在<a href=\"http://note.youdao.com/\" target=\"_blank\" rel=\"external\">上一篇文章</a>中我们聊到了回归模型的评测方法，解下来我们详细聊聊如何来评价一个回归模型的好坏。</p>\n<p>这里我们定义预测值和真实值分别为：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">true = [10, 5, 3, 2]</span><br><span class=\"line\">pred = [9, 5, 5, 3]</span><br></pre></td></tr></table></figure></p>\n<p>1: 平均绝对误差（Mean Absolute Error, MAE）</p>\n<script type=\"math/tex; mode=display\">\n\\frac{1}{N}(\\sum_{1}^{n} |y_i - \\bar{y}|)</script><p>2: 均方误差（Mean Squared Error, MSE）</p>\n<script type=\"math/tex; mode=display\">\n\\frac{1}{N}\\sum_{1}^{n}(y_i - \\bar{y})^2</script><p>3: 均方根误差（Root Mean Squared Error, RMSE）</p>\n<script type=\"math/tex; mode=display\">\n\\frac{1}{N} \\sqrt{ \\sum_{1}^{n}(y_i - \\bar{y})^2 }</script><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [80]: from sklearn import metrics</span><br><span class=\"line\"></span><br><span class=\"line\">In [81]: import numpy as np</span><br><span class=\"line\"></span><br><span class=\"line\">In [82]: true = [10, 5, 3, 2]</span><br><span class=\"line\"></span><br><span class=\"line\">In [83]: pred = [9, 5, 5, 3]</span><br><span class=\"line\"></span><br><span class=\"line\">In [84]: print(&quot;MAE: &quot;, metrics.mean_absolute_error(true,pred))</span><br><span class=\"line\">(&apos;MAE: &apos;, 1.0)</span><br><span class=\"line\"></span><br><span class=\"line\">In [85]: print(&quot;MAE By Hand: &quot;, (1+0+2+1)/4.)</span><br><span class=\"line\">(&apos;MAE By Hand: &apos;, 1.0)</span><br><span class=\"line\"></span><br><span class=\"line\">In [86]: print(&quot;MSE: &quot;, metrics.mean_squared_error(true,pred))</span><br><span class=\"line\">(&apos;MSE: &apos;, 1.5)</span><br><span class=\"line\"></span><br><span class=\"line\">In [87]: print(&quot;MSE By Hand: &quot;, (1 ** 2 + 0 ** 2 + 2 ** 2 + 1 ** 2 ) / 4.)</span><br><span class=\"line\">(&apos;MSE By Hand: &apos;, 1.5)</span><br><span class=\"line\"></span><br><span class=\"line\">In [88]: print(&quot;RMSE: &quot;, np.sqrt(metrics.mean_squared_error(true,pred)))</span><br><span class=\"line\">(&apos;RMSE: &apos;, 1.2247448713915889)</span><br><span class=\"line\"></span><br><span class=\"line\">In [89]: print(&quot;RMSE By Hand: &quot;, np.sqrt((1 ** 2 + 0 ** 2 + 2 ** 2 + 1 ** 2 ) / 4.))</span><br><span class=\"line\">(&apos;RMSE By Hand: &apos;, 1.2247448713915889)</span><br></pre></td></tr></table></figure>\n<hr>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>线性回归在现实中还是可以解决很多问题的，但是并不是万能的，后续我会继续整理逻辑回归，岭回归等相关回归的知识，如果你感觉有用，欢迎分享！</p>"},{"title":"数据归一化和其在sklearn中的处理","date":"2017-09-01T03:33:50.000Z","_content":"\n# 一：数据归一化\n\n数据归一化（标准化）处理是数据挖掘的一项基础工作，不同评价指标往往具有不同的量纲和量纲单位，这样的情况会影响到数据分析的结果，为了消除指标之间的量纲影响，需要进行数据标准化处理，以解决数据指标之间的可比性。原始数据经过数据标准化处理后，各指标处于同一数量级，适合进行综合对比评价。\n<!--More-->\n归一化方法有两种形式，一种是把数变为（0，1）之间的小数，一种是把有量纲表达式变为无量纲表达式。在机器学习中我们更关注的把数据变到0～1之间，接下来我们讨论的也是第一种形式。\n\n## 1）min-max标准化\nmin-max标准化也叫做离差标准化，是对原始数据的线性变换，使结果落到[0,1]区间，其对应的数学公式如下：\n\n$$\nX_{scale} = \\frac{x-min}{max-min}\n$$\n\n对应的python实现为\n```\n# x为数据 比如说 [1,2,1,3,2,4,1]\ndef Normalization(x):\n    return [(float(i)-min(x))/float(max(x)-min(x)) for i in x]\n```\n\n如果要将数据转换到[-1,1]之间，可以修改其数学公式为：\n\n$$\nX_{scale} = \\frac{x-x_{mean}}{max-min}\n$$\nx_mean 表示平均值。\n\n对应的python实现为\n```\nimport numpy as np\n\n# x为数据 比如说 [1,2,1,3,2,4,1]\ndef Normalization(x):\n    return [(float(i)-np.mean(x))/float(max(x)-min(x)) for i in x]\n```\n\n其中max为样本数据的最大值，min为样本数据的最小值。这种方法有个缺陷就是当有新数据加入时，可能导致max和min的变化，需要重新定义。\n\n该标准化方法有一个缺点就是，如果数据中有一些偏离正常数据的异常点，就会导致标准化结果的不准确性。比如说一个公司员工（A，B，C，D）的薪水为6k,8k,7k,10w,这种情况下进行归一化对每个员工来讲都是不合理的。\n\n当然还有一些其他的办法也能实现数据的标准化。\n\n## 2）z-score标准化\nz-score标准化也叫标准差标准化，代表的是分值偏离均值的程度，经过处理的数据符合标准正态分布，即均值为0，标准差为1。其转化函数为\n\n$$\nX_{scale} = \\frac{x-\\mu }{\\sigma }\n$$\n\n其中μ为所有样本数据的均值，σ为所有样本数据的标准差。\n\n其对应的python实现为：\n```\nimport numpy as np\n\n#x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\ndef z_score(x):\n    return (x - np.mean(x) )/np.std(x, ddof = 1)\n```\nz-score标准化方法同样对于离群异常值的影响。接下来看一种改进的z-score标准化方法。\n\n## 3）改进的z-score标准化\n\n将标准分公式中的均值改为中位数，将标准差改为绝对偏差。\n\n$$\nX_{scale} = \\frac{x-x_{center} }{\\sigma_{1} }$$\n中位数是指将所有数据进行排序，取中间的那个值，如数据量是偶数，则取中间两个数据的平均值。\n\nσ1为所有样本数据的绝对偏差,其计算公式为：\n$$\n\\frac{1}{N} \\sum_{1}^{n}|x_{i} - x_{center}|\n$$\n\n----\n# 二：sklearn中的归一化\n\nsklearn.preprocessing 提供了一些实用的函数 用来处理数据的维度，以供算法使用。\n\n## 1）均值-标准差缩放\n\n即我们上边对应的z-score标准化。\n在sklearn的学习中，数据集的标准化是很多机器学习模型算法的常见要求。如果个别特征看起来不是很符合正态分布，那么他们可能为表现不好。\n\n实际上，我们经常忽略分布的形状，只是通过减去整组数据的平均值，使之更靠近数据中心分布，然后通过将非连续数特征除以其标准偏差进行分类。\n\n\n例如，用于学习算法（例如支持向量机的RBF内核或线性模型的l1和l2正则化器）的目标函数中使用的许多元素假设所有特征都以零为中心并且具有相同顺序的方差。如果特征的方差大于其他数量级，则可能主导目标函数，使估计器无法按预期正确地学习其他特征。\n\n例子：\n```\n>>> from sklearn import preprocessing\n>>> import numpy as np\n>>> X_train = np.array([[ 1., -1.,  2.],\n...                     [ 2.,  0.,  0.],\n...                     [ 0.,  1., -1.]])\n>>> X_scaled = preprocessing.scale(X_train)\n>>> X_scaled\narray([[ 0.        , -1.22474487,  1.33630621],\n       [ 1.22474487,  0.        , -0.26726124],\n       [-1.22474487,  1.22474487, -1.06904497]])\n```\n标准化后的数据符合标准正太分布\n```\n>>> X_scaled.mean(axis=0)\narray([ 0.,  0.,  0.])\n>>> X_scaled.std(axis=0)\narray([ 1.,  1.,  1.])\n```\n\n预处理模块还提供了一个实用程序级StandardScaler，它实现了Transformer API来计算训练集上的平均值和标准偏差，以便能够稍后在测试集上重新应用相同的变换。\n```\n>>> scaler = preprocessing.StandardScaler().fit(X_train)\n>>> scaler\nStandardScaler(copy=True, with_mean=True, with_std=True)\n>>> scaler.mean_\narray([ 1.        ,  0.        ,  0.33333333])\n>>> scaler.scale_\narray([ 0.81649658,  0.81649658,  1.24721913])\n>>> scaler.transform(X_train)\narray([[ 0.        , -1.22474487,  1.33630621],\n       [ 1.22474487,  0.        , -0.26726124],\n       [-1.22474487,  1.22474487, -1.06904497]])\n```\n\n使用转换器可以对新数据进行转换\n```\n>>> X_test = [[-1., 1., 0.]]\n>>> scaler.transform(X_test)\narray([[-2.44948974,  1.22474487, -0.26726124]])\n```\n\n## 2）min-max标准化\n\nX_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n\n```\n\n>>> X_train = np.array([[ 1., -1.,  2.],\n...                      [ 2.,  0.,  0.],\n...                      [ 0.,  1., -1.]])\n>>> min_max_scaler = preprocessing.MinMaxScaler()\n>>> X_train_minmax = min_max_scaler.fit_transform(X_train)\n>>> X_train_minmax\narray([[ 0.5       ,  0.        ,  1.        ],\n       [ 1.        ,  0.5       ,  0.33333333],\n       [ 0.        ,  1.        ,  0.        ]])\n```\n上边我们创建的min_max_scaler 同样适用于新的测试数据\n```\n>>> X_test = np.array([[ -3., -1.,  4.]])\n>>> X_test_minmax = min_max_scaler.transform(X_test)\n>>> X_test_minmax\narray([[-1.5       ,  0.        ,  1.66666667]])\n```\n可以通过scale_和min方法查看标准差和最小值\n```\n>>> min_max_scaler.scale_ \narray([ 0.5       ,  0.5       ,  0.33333333])\n>>> min_max_scaler.min_\narray([ 0.        ,  0.5       ,  0.33333333])\n```\n\n## 3）最大值标准化\n\n对于每个数值／每个维度的最大值\n\n```\n>>> X_train\narray([[ 1., -1.,  2.],\n       [ 2.,  0.,  0.],\n       [ 0.,  1., -1.]])\n>>> max_abs_scaler = preprocessing.MaxAbsScaler()\n>>> X_train_maxabs = max_abs_scaler.fit_transform(X_train)\n>>> X_train_maxabs\narray([[ 0.5, -1. ,  1. ],\n       [ 1. ,  0. ,  0. ],\n       [ 0. ,  1. , -0.5]])\n>>> X_test = np.array([[ -3., -1.,  4.]])\n>>> X_test_maxabs = max_abs_scaler.transform(X_test)\n>>> X_test_maxabs                 \narray([[-1.5, -1. ,  2. ]])\n>>> max_abs_scaler.scale_         \narray([ 2.,  1.,  2.])\n```\n\n## 4）规范化\n规范化是文本分类和聚类中向量空间模型的基础\n\n```\n>>> X = [[ 1., -1.,  2.],\n...      [ 2.,  0.,  0.],\n...      [ 0.,  1., -1.]]\n>>> X_normalized = preprocessing.normalize(X, norm='l2')\n>>> X_normalized\narray([[ 0.40824829, -0.40824829,  0.81649658],\n       [ 1.        ,  0.        ,  0.        ],\n       [ 0.        ,  0.70710678, -0.70710678]])\n```\n\n解释：norm 该参数是可选的，默认值是l2（向量各元素的平方和然后求平方根），用来规范化每个非零向量，如果axis参数设置为0，则表示的是规范化每个非零的特征维度。\n\n机器学习中的范数规则：[点击阅读](http://blog.csdn.net/zouxy09/article/details/24971995/)<br>\n其他对应参数：[点击查看](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html#sklearn.preprocessing.normalize)\n\n\npreprocessing模块提供了训练种子的功能，我们可通过以下方式得到一个新的种子，并对新数据进行规范化处理。\n```\n>>> normalizer = preprocessing.Normalizer().fit(X)\n>>> normalizer\nNormalizer(copy=True, norm='l2')\n>>> normalizer.transform(X)\narray([[ 0.40824829, -0.40824829,  0.81649658],\n       [ 1.        ,  0.        ,  0.        ],\n       [ 0.        ,  0.70710678, -0.70710678]])\n>>> normalizer.transform([[-1,1,0]])\narray([[-0.70710678,  0.70710678,  0.        ]])\n```\n\n## 5）二值化\n将数据转换到0-1 之间\n```\n>>> X\n[[1.0, -1.0, 2.0], [2.0, 0.0, 0.0], [0.0, 1.0, -1.0]]\n>>> binarizer = preprocessing.Binarizer().fit(X)\n>>> binarizer\nBinarizer(copy=True, threshold=0.0)\n>>> binarizer.transform(X)\narray([[ 1.,  0.,  1.],\n       [ 1.,  0.,  0.],\n       [ 0.,  1.,  0.]])\n```\n可以调整二值化的门阀\n```\n>>> binarizer = preprocessing.Binarizer(threshold=1.1)\n>>> binarizer.transform(X)\narray([[ 0.,  0.,  1.],\n       [ 1.,  0.,  0.],\n       [ 0.,  0.,  0.]])\n```\n\n## 6）编码的分类特征\n\n通常情况下，特征不是作为连续值给定的。例如一个人可以有\n```\n[\"male\", \"female\"], [\"from Europe\", \"from US\", \"from Asia\"], [\"uses Firefox\", \"uses Chrome\", \"uses Safari\", \"uses Internet Explorer\"]\n```\n这些特征可以被有效的编码为整数，例如\n```\n[\"male\", \"from US\", \"uses Internet Explorer\"] => [0, 1, 3]\n[\"female\", \"from Asia\", \"uses Chrome\"] would be [1, 2, 1].\n```\n这样的整数不应该直接应用到scikit的算法中，可以通过one-of-k或者独热编码（OneHotEncorder），该种处理方式会把每个分类特征的m中可能值转换成m个二进制值。\n\n```\n>>> enc = preprocessing.OneHotEncoder()\n>>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])\nOneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n       handle_unknown='error', n_values='auto', sparse=True)\n>>> enc.transform([[0,1,3]]).toarray()\narray([[ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.]])\n```\n默认情况下，从数据集中自动推断出每个特征可以带多少个值。可以明确指定使用的参数n_values。在我们的数据集中有两种性别，三种可能的大陆和四种Web浏览器。然后，我们拟合估计量，并转换一个数据点。在结果中，前两个数字编码性别，下一组三个数字的大陆和最后四个Web浏览器。\n```\n>>> enc = preprocessing.OneHotEncoder(n_values=[2,3,4])\n>>> enc.fit([[1,2,3],[0,2,0]])\nOneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n       handle_unknown='error', n_values=[2, 3, 4], sparse=True)\n>>> enc.transform([[1,0,0]]).toarray()\narray([[ 0.,  1.,  1.,  0.,  0.,  1.,  0.,  0.,  0.]])\n```\n\n## 7）填补缺失值\n由于各种原因，真实数据中存在大量的空白值，这样的数据集，显然是不符合scikit的要求的，那么preprocessing模块提供这样一个功能，利用已知的数据来填补这些空白。\n```\n>>> import numpy as np\n>>> from sklearn.preprocessing import Imputer\n>>> imp = Imputer(missing_values='NaN',strategy='mean',verbose=0)\n>>> imp.fit([[1, 2], [np.nan, 3], [7, 6]])\nImputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)\n>>> X = [[np.nan, 2], [6, np.nan], [7, 6]]\n>>> print(imp.transform(X))                           \n[[ 4.          2.        ]\n [ 6.          3.66666667]\n [ 7.          6.        ]]\n```\n\nImputer同样支持稀疏矩阵\n```\n>>> import scipy.sparse as sp\n>>> X = sp.csc_matrix([[1,2],[0,3],[7,6]])\n>>> imp = Imputer(missing_values=0,strategy='mean',axis=0)\n>>> imp.fit(X)\nImputer(axis=0, copy=True, missing_values=0, strategy='mean', verbose=0)\n>>> X_test = sp.csc\nsp.csc          sp.csc_matrix(  \n>>> X_test = sp.csc_matrix([[0,2],[6,0],[7,6]])\n>>> print(imp.transform(X_test))\n[[ 4.          2.        ]\n [ 6.          3.66666667]\n [ 7.          6.        ]]\n```\n\n## 8）生成多项式特征\n通常，通过考虑输入数据的非线性特征来增加模型的复杂度是很有用的。一个简单而常用的方法是多项式特征，它可以得到特征的高阶和相互作用项。\n\n其遵循的原则是 \n$$ \n(X_1, X_2) -> (1, X_1, X_2, X_1^2, X_1X_2, X_2^2)\n$$\n```\n>>> import numpy as np\n>>> from sklearn.preprocessing import PolynomialFeatures\n>>> X = np.arange(6).reshape(3, 2)\n>>> X                                                 \narray([[0, 1],\n       [2, 3],\n       [4, 5]])\n>>> poly = PolynomialFeatures(2)\n>>> poly.fit_transform(X)                             \narray([[  1.,   0.,   1.,   0.,   0.,   1.],\n       [  1.,   2.,   3.,   4.,   6.,   9.],\n       [  1.,   4.,   5.,  16.,  20.,  25.]])\n```\n\n有些情况下，有相互关系的标签才是必须的，这个时候可以通过设置 interaction_only=True 来进行多项式特征的生成\n```\n>>> X = np.arange(9).reshape(3, 3)\n>>> X                                                 \narray([[0, 1, 2],\n       [3, 4, 5],\n       [6, 7, 8]])\n>>> poly = PolynomialFeatures(degree=3, interaction_only=True)\n>>> poly.fit_transform(X)                             \narray([[   1.,    0.,    1.,    2.,    0.,    0.,    2.,    0.],\n       [   1.,    3.,    4.,    5.,   12.,   15.,   20.,   60.],\n       [   1.,    6.,    7.,    8.,   42.,   48.,   56.,  336.]])\n```\n其遵循的规则是：\n$$\n(X_1, X_2, X_3) -> (1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3)\n$$\n\n---\n\n对应的scikit-learn资料为： http://scikit-learn.org/stable/modules/preprocessing.html","source":"_posts/机器学习/数据归一化和其在sklearn中的处理.md","raw":"---\ntitle: 数据归一化和其在sklearn中的处理\ndate: 2017-09-01 11:33:50\ntags: [数据归一化,sklearn]\ncategories: 技术篇\n---\n\n# 一：数据归一化\n\n数据归一化（标准化）处理是数据挖掘的一项基础工作，不同评价指标往往具有不同的量纲和量纲单位，这样的情况会影响到数据分析的结果，为了消除指标之间的量纲影响，需要进行数据标准化处理，以解决数据指标之间的可比性。原始数据经过数据标准化处理后，各指标处于同一数量级，适合进行综合对比评价。\n<!--More-->\n归一化方法有两种形式，一种是把数变为（0，1）之间的小数，一种是把有量纲表达式变为无量纲表达式。在机器学习中我们更关注的把数据变到0～1之间，接下来我们讨论的也是第一种形式。\n\n## 1）min-max标准化\nmin-max标准化也叫做离差标准化，是对原始数据的线性变换，使结果落到[0,1]区间，其对应的数学公式如下：\n\n$$\nX_{scale} = \\frac{x-min}{max-min}\n$$\n\n对应的python实现为\n```\n# x为数据 比如说 [1,2,1,3,2,4,1]\ndef Normalization(x):\n    return [(float(i)-min(x))/float(max(x)-min(x)) for i in x]\n```\n\n如果要将数据转换到[-1,1]之间，可以修改其数学公式为：\n\n$$\nX_{scale} = \\frac{x-x_{mean}}{max-min}\n$$\nx_mean 表示平均值。\n\n对应的python实现为\n```\nimport numpy as np\n\n# x为数据 比如说 [1,2,1,3,2,4,1]\ndef Normalization(x):\n    return [(float(i)-np.mean(x))/float(max(x)-min(x)) for i in x]\n```\n\n其中max为样本数据的最大值，min为样本数据的最小值。这种方法有个缺陷就是当有新数据加入时，可能导致max和min的变化，需要重新定义。\n\n该标准化方法有一个缺点就是，如果数据中有一些偏离正常数据的异常点，就会导致标准化结果的不准确性。比如说一个公司员工（A，B，C，D）的薪水为6k,8k,7k,10w,这种情况下进行归一化对每个员工来讲都是不合理的。\n\n当然还有一些其他的办法也能实现数据的标准化。\n\n## 2）z-score标准化\nz-score标准化也叫标准差标准化，代表的是分值偏离均值的程度，经过处理的数据符合标准正态分布，即均值为0，标准差为1。其转化函数为\n\n$$\nX_{scale} = \\frac{x-\\mu }{\\sigma }\n$$\n\n其中μ为所有样本数据的均值，σ为所有样本数据的标准差。\n\n其对应的python实现为：\n```\nimport numpy as np\n\n#x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\ndef z_score(x):\n    return (x - np.mean(x) )/np.std(x, ddof = 1)\n```\nz-score标准化方法同样对于离群异常值的影响。接下来看一种改进的z-score标准化方法。\n\n## 3）改进的z-score标准化\n\n将标准分公式中的均值改为中位数，将标准差改为绝对偏差。\n\n$$\nX_{scale} = \\frac{x-x_{center} }{\\sigma_{1} }$$\n中位数是指将所有数据进行排序，取中间的那个值，如数据量是偶数，则取中间两个数据的平均值。\n\nσ1为所有样本数据的绝对偏差,其计算公式为：\n$$\n\\frac{1}{N} \\sum_{1}^{n}|x_{i} - x_{center}|\n$$\n\n----\n# 二：sklearn中的归一化\n\nsklearn.preprocessing 提供了一些实用的函数 用来处理数据的维度，以供算法使用。\n\n## 1）均值-标准差缩放\n\n即我们上边对应的z-score标准化。\n在sklearn的学习中，数据集的标准化是很多机器学习模型算法的常见要求。如果个别特征看起来不是很符合正态分布，那么他们可能为表现不好。\n\n实际上，我们经常忽略分布的形状，只是通过减去整组数据的平均值，使之更靠近数据中心分布，然后通过将非连续数特征除以其标准偏差进行分类。\n\n\n例如，用于学习算法（例如支持向量机的RBF内核或线性模型的l1和l2正则化器）的目标函数中使用的许多元素假设所有特征都以零为中心并且具有相同顺序的方差。如果特征的方差大于其他数量级，则可能主导目标函数，使估计器无法按预期正确地学习其他特征。\n\n例子：\n```\n>>> from sklearn import preprocessing\n>>> import numpy as np\n>>> X_train = np.array([[ 1., -1.,  2.],\n...                     [ 2.,  0.,  0.],\n...                     [ 0.,  1., -1.]])\n>>> X_scaled = preprocessing.scale(X_train)\n>>> X_scaled\narray([[ 0.        , -1.22474487,  1.33630621],\n       [ 1.22474487,  0.        , -0.26726124],\n       [-1.22474487,  1.22474487, -1.06904497]])\n```\n标准化后的数据符合标准正太分布\n```\n>>> X_scaled.mean(axis=0)\narray([ 0.,  0.,  0.])\n>>> X_scaled.std(axis=0)\narray([ 1.,  1.,  1.])\n```\n\n预处理模块还提供了一个实用程序级StandardScaler，它实现了Transformer API来计算训练集上的平均值和标准偏差，以便能够稍后在测试集上重新应用相同的变换。\n```\n>>> scaler = preprocessing.StandardScaler().fit(X_train)\n>>> scaler\nStandardScaler(copy=True, with_mean=True, with_std=True)\n>>> scaler.mean_\narray([ 1.        ,  0.        ,  0.33333333])\n>>> scaler.scale_\narray([ 0.81649658,  0.81649658,  1.24721913])\n>>> scaler.transform(X_train)\narray([[ 0.        , -1.22474487,  1.33630621],\n       [ 1.22474487,  0.        , -0.26726124],\n       [-1.22474487,  1.22474487, -1.06904497]])\n```\n\n使用转换器可以对新数据进行转换\n```\n>>> X_test = [[-1., 1., 0.]]\n>>> scaler.transform(X_test)\narray([[-2.44948974,  1.22474487, -0.26726124]])\n```\n\n## 2）min-max标准化\n\nX_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n\n```\n\n>>> X_train = np.array([[ 1., -1.,  2.],\n...                      [ 2.,  0.,  0.],\n...                      [ 0.,  1., -1.]])\n>>> min_max_scaler = preprocessing.MinMaxScaler()\n>>> X_train_minmax = min_max_scaler.fit_transform(X_train)\n>>> X_train_minmax\narray([[ 0.5       ,  0.        ,  1.        ],\n       [ 1.        ,  0.5       ,  0.33333333],\n       [ 0.        ,  1.        ,  0.        ]])\n```\n上边我们创建的min_max_scaler 同样适用于新的测试数据\n```\n>>> X_test = np.array([[ -3., -1.,  4.]])\n>>> X_test_minmax = min_max_scaler.transform(X_test)\n>>> X_test_minmax\narray([[-1.5       ,  0.        ,  1.66666667]])\n```\n可以通过scale_和min方法查看标准差和最小值\n```\n>>> min_max_scaler.scale_ \narray([ 0.5       ,  0.5       ,  0.33333333])\n>>> min_max_scaler.min_\narray([ 0.        ,  0.5       ,  0.33333333])\n```\n\n## 3）最大值标准化\n\n对于每个数值／每个维度的最大值\n\n```\n>>> X_train\narray([[ 1., -1.,  2.],\n       [ 2.,  0.,  0.],\n       [ 0.,  1., -1.]])\n>>> max_abs_scaler = preprocessing.MaxAbsScaler()\n>>> X_train_maxabs = max_abs_scaler.fit_transform(X_train)\n>>> X_train_maxabs\narray([[ 0.5, -1. ,  1. ],\n       [ 1. ,  0. ,  0. ],\n       [ 0. ,  1. , -0.5]])\n>>> X_test = np.array([[ -3., -1.,  4.]])\n>>> X_test_maxabs = max_abs_scaler.transform(X_test)\n>>> X_test_maxabs                 \narray([[-1.5, -1. ,  2. ]])\n>>> max_abs_scaler.scale_         \narray([ 2.,  1.,  2.])\n```\n\n## 4）规范化\n规范化是文本分类和聚类中向量空间模型的基础\n\n```\n>>> X = [[ 1., -1.,  2.],\n...      [ 2.,  0.,  0.],\n...      [ 0.,  1., -1.]]\n>>> X_normalized = preprocessing.normalize(X, norm='l2')\n>>> X_normalized\narray([[ 0.40824829, -0.40824829,  0.81649658],\n       [ 1.        ,  0.        ,  0.        ],\n       [ 0.        ,  0.70710678, -0.70710678]])\n```\n\n解释：norm 该参数是可选的，默认值是l2（向量各元素的平方和然后求平方根），用来规范化每个非零向量，如果axis参数设置为0，则表示的是规范化每个非零的特征维度。\n\n机器学习中的范数规则：[点击阅读](http://blog.csdn.net/zouxy09/article/details/24971995/)<br>\n其他对应参数：[点击查看](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html#sklearn.preprocessing.normalize)\n\n\npreprocessing模块提供了训练种子的功能，我们可通过以下方式得到一个新的种子，并对新数据进行规范化处理。\n```\n>>> normalizer = preprocessing.Normalizer().fit(X)\n>>> normalizer\nNormalizer(copy=True, norm='l2')\n>>> normalizer.transform(X)\narray([[ 0.40824829, -0.40824829,  0.81649658],\n       [ 1.        ,  0.        ,  0.        ],\n       [ 0.        ,  0.70710678, -0.70710678]])\n>>> normalizer.transform([[-1,1,0]])\narray([[-0.70710678,  0.70710678,  0.        ]])\n```\n\n## 5）二值化\n将数据转换到0-1 之间\n```\n>>> X\n[[1.0, -1.0, 2.0], [2.0, 0.0, 0.0], [0.0, 1.0, -1.0]]\n>>> binarizer = preprocessing.Binarizer().fit(X)\n>>> binarizer\nBinarizer(copy=True, threshold=0.0)\n>>> binarizer.transform(X)\narray([[ 1.,  0.,  1.],\n       [ 1.,  0.,  0.],\n       [ 0.,  1.,  0.]])\n```\n可以调整二值化的门阀\n```\n>>> binarizer = preprocessing.Binarizer(threshold=1.1)\n>>> binarizer.transform(X)\narray([[ 0.,  0.,  1.],\n       [ 1.,  0.,  0.],\n       [ 0.,  0.,  0.]])\n```\n\n## 6）编码的分类特征\n\n通常情况下，特征不是作为连续值给定的。例如一个人可以有\n```\n[\"male\", \"female\"], [\"from Europe\", \"from US\", \"from Asia\"], [\"uses Firefox\", \"uses Chrome\", \"uses Safari\", \"uses Internet Explorer\"]\n```\n这些特征可以被有效的编码为整数，例如\n```\n[\"male\", \"from US\", \"uses Internet Explorer\"] => [0, 1, 3]\n[\"female\", \"from Asia\", \"uses Chrome\"] would be [1, 2, 1].\n```\n这样的整数不应该直接应用到scikit的算法中，可以通过one-of-k或者独热编码（OneHotEncorder），该种处理方式会把每个分类特征的m中可能值转换成m个二进制值。\n\n```\n>>> enc = preprocessing.OneHotEncoder()\n>>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])\nOneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n       handle_unknown='error', n_values='auto', sparse=True)\n>>> enc.transform([[0,1,3]]).toarray()\narray([[ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.]])\n```\n默认情况下，从数据集中自动推断出每个特征可以带多少个值。可以明确指定使用的参数n_values。在我们的数据集中有两种性别，三种可能的大陆和四种Web浏览器。然后，我们拟合估计量，并转换一个数据点。在结果中，前两个数字编码性别，下一组三个数字的大陆和最后四个Web浏览器。\n```\n>>> enc = preprocessing.OneHotEncoder(n_values=[2,3,4])\n>>> enc.fit([[1,2,3],[0,2,0]])\nOneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n       handle_unknown='error', n_values=[2, 3, 4], sparse=True)\n>>> enc.transform([[1,0,0]]).toarray()\narray([[ 0.,  1.,  1.,  0.,  0.,  1.,  0.,  0.,  0.]])\n```\n\n## 7）填补缺失值\n由于各种原因，真实数据中存在大量的空白值，这样的数据集，显然是不符合scikit的要求的，那么preprocessing模块提供这样一个功能，利用已知的数据来填补这些空白。\n```\n>>> import numpy as np\n>>> from sklearn.preprocessing import Imputer\n>>> imp = Imputer(missing_values='NaN',strategy='mean',verbose=0)\n>>> imp.fit([[1, 2], [np.nan, 3], [7, 6]])\nImputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)\n>>> X = [[np.nan, 2], [6, np.nan], [7, 6]]\n>>> print(imp.transform(X))                           \n[[ 4.          2.        ]\n [ 6.          3.66666667]\n [ 7.          6.        ]]\n```\n\nImputer同样支持稀疏矩阵\n```\n>>> import scipy.sparse as sp\n>>> X = sp.csc_matrix([[1,2],[0,3],[7,6]])\n>>> imp = Imputer(missing_values=0,strategy='mean',axis=0)\n>>> imp.fit(X)\nImputer(axis=0, copy=True, missing_values=0, strategy='mean', verbose=0)\n>>> X_test = sp.csc\nsp.csc          sp.csc_matrix(  \n>>> X_test = sp.csc_matrix([[0,2],[6,0],[7,6]])\n>>> print(imp.transform(X_test))\n[[ 4.          2.        ]\n [ 6.          3.66666667]\n [ 7.          6.        ]]\n```\n\n## 8）生成多项式特征\n通常，通过考虑输入数据的非线性特征来增加模型的复杂度是很有用的。一个简单而常用的方法是多项式特征，它可以得到特征的高阶和相互作用项。\n\n其遵循的原则是 \n$$ \n(X_1, X_2) -> (1, X_1, X_2, X_1^2, X_1X_2, X_2^2)\n$$\n```\n>>> import numpy as np\n>>> from sklearn.preprocessing import PolynomialFeatures\n>>> X = np.arange(6).reshape(3, 2)\n>>> X                                                 \narray([[0, 1],\n       [2, 3],\n       [4, 5]])\n>>> poly = PolynomialFeatures(2)\n>>> poly.fit_transform(X)                             \narray([[  1.,   0.,   1.,   0.,   0.,   1.],\n       [  1.,   2.,   3.,   4.,   6.,   9.],\n       [  1.,   4.,   5.,  16.,  20.,  25.]])\n```\n\n有些情况下，有相互关系的标签才是必须的，这个时候可以通过设置 interaction_only=True 来进行多项式特征的生成\n```\n>>> X = np.arange(9).reshape(3, 3)\n>>> X                                                 \narray([[0, 1, 2],\n       [3, 4, 5],\n       [6, 7, 8]])\n>>> poly = PolynomialFeatures(degree=3, interaction_only=True)\n>>> poly.fit_transform(X)                             \narray([[   1.,    0.,    1.,    2.,    0.,    0.,    2.,    0.],\n       [   1.,    3.,    4.,    5.,   12.,   15.,   20.,   60.],\n       [   1.,    6.,    7.,    8.,   42.,   48.,   56.,  336.]])\n```\n其遵循的规则是：\n$$\n(X_1, X_2, X_3) -> (1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3)\n$$\n\n---\n\n对应的scikit-learn资料为： http://scikit-learn.org/stable/modules/preprocessing.html","slug":"机器学习/数据归一化和其在sklearn中的处理","published":1,"updated":"2017-12-19T02:54:56.787Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjbd1r26k0011kxuw6z50bx3a","content":"<h1 id=\"一：数据归一化\"><a href=\"#一：数据归一化\" class=\"headerlink\" title=\"一：数据归一化\"></a>一：数据归一化</h1><p>数据归一化（标准化）处理是数据挖掘的一项基础工作，不同评价指标往往具有不同的量纲和量纲单位，这样的情况会影响到数据分析的结果，为了消除指标之间的量纲影响，需要进行数据标准化处理，以解决数据指标之间的可比性。原始数据经过数据标准化处理后，各指标处于同一数量级，适合进行综合对比评价。<br><a id=\"more\"></a><br>归一化方法有两种形式，一种是把数变为（0，1）之间的小数，一种是把有量纲表达式变为无量纲表达式。在机器学习中我们更关注的把数据变到0～1之间，接下来我们讨论的也是第一种形式。</p>\n<h2 id=\"1）min-max标准化\"><a href=\"#1）min-max标准化\" class=\"headerlink\" title=\"1）min-max标准化\"></a>1）min-max标准化</h2><p>min-max标准化也叫做离差标准化，是对原始数据的线性变换，使结果落到[0,1]区间，其对应的数学公式如下：</p>\n<script type=\"math/tex; mode=display\">\nX_{scale} = \\frac{x-min}{max-min}</script><p>对应的python实现为<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># x为数据 比如说 [1,2,1,3,2,4,1]</span><br><span class=\"line\">def Normalization(x):</span><br><span class=\"line\">    return [(float(i)-min(x))/float(max(x)-min(x)) for i in x]</span><br></pre></td></tr></table></figure></p>\n<p>如果要将数据转换到[-1,1]之间，可以修改其数学公式为：</p>\n<script type=\"math/tex; mode=display\">\nX_{scale} = \\frac{x-x_{mean}}{max-min}</script><p>x_mean 表示平均值。</p>\n<p>对应的python实现为<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import numpy as np</span><br><span class=\"line\"></span><br><span class=\"line\"># x为数据 比如说 [1,2,1,3,2,4,1]</span><br><span class=\"line\">def Normalization(x):</span><br><span class=\"line\">    return [(float(i)-np.mean(x))/float(max(x)-min(x)) for i in x]</span><br></pre></td></tr></table></figure></p>\n<p>其中max为样本数据的最大值，min为样本数据的最小值。这种方法有个缺陷就是当有新数据加入时，可能导致max和min的变化，需要重新定义。</p>\n<p>该标准化方法有一个缺点就是，如果数据中有一些偏离正常数据的异常点，就会导致标准化结果的不准确性。比如说一个公司员工（A，B，C，D）的薪水为6k,8k,7k,10w,这种情况下进行归一化对每个员工来讲都是不合理的。</p>\n<p>当然还有一些其他的办法也能实现数据的标准化。</p>\n<h2 id=\"2）z-score标准化\"><a href=\"#2）z-score标准化\" class=\"headerlink\" title=\"2）z-score标准化\"></a>2）z-score标准化</h2><p>z-score标准化也叫标准差标准化，代表的是分值偏离均值的程度，经过处理的数据符合标准正态分布，即均值为0，标准差为1。其转化函数为</p>\n<script type=\"math/tex; mode=display\">\nX_{scale} = \\frac{x-\\mu }{\\sigma }</script><p>其中μ为所有样本数据的均值，σ为所有样本数据的标准差。</p>\n<p>其对应的python实现为：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import numpy as np</span><br><span class=\"line\"></span><br><span class=\"line\">#x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</span><br><span class=\"line\">def z_score(x):</span><br><span class=\"line\">    return (x - np.mean(x) )/np.std(x, ddof = 1)</span><br></pre></td></tr></table></figure></p>\n<p>z-score标准化方法同样对于离群异常值的影响。接下来看一种改进的z-score标准化方法。</p>\n<h2 id=\"3）改进的z-score标准化\"><a href=\"#3）改进的z-score标准化\" class=\"headerlink\" title=\"3）改进的z-score标准化\"></a>3）改进的z-score标准化</h2><p>将标准分公式中的均值改为中位数，将标准差改为绝对偏差。</p>\n<script type=\"math/tex; mode=display\">\nX_{scale} = \\frac{x-x_{center} }{\\sigma_{1} }</script><p>中位数是指将所有数据进行排序，取中间的那个值，如数据量是偶数，则取中间两个数据的平均值。</p>\n<p>σ1为所有样本数据的绝对偏差,其计算公式为：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{1}{N} \\sum_{1}^{n}|x_{i} - x_{center}|</script><hr>\n<h1 id=\"二：sklearn中的归一化\"><a href=\"#二：sklearn中的归一化\" class=\"headerlink\" title=\"二：sklearn中的归一化\"></a>二：sklearn中的归一化</h1><p>sklearn.preprocessing 提供了一些实用的函数 用来处理数据的维度，以供算法使用。</p>\n<h2 id=\"1）均值-标准差缩放\"><a href=\"#1）均值-标准差缩放\" class=\"headerlink\" title=\"1）均值-标准差缩放\"></a>1）均值-标准差缩放</h2><p>即我们上边对应的z-score标准化。<br>在sklearn的学习中，数据集的标准化是很多机器学习模型算法的常见要求。如果个别特征看起来不是很符合正态分布，那么他们可能为表现不好。</p>\n<p>实际上，我们经常忽略分布的形状，只是通过减去整组数据的平均值，使之更靠近数据中心分布，然后通过将非连续数特征除以其标准偏差进行分类。</p>\n<p>例如，用于学习算法（例如支持向量机的RBF内核或线性模型的l1和l2正则化器）的目标函数中使用的许多元素假设所有特征都以零为中心并且具有相同顺序的方差。如果特征的方差大于其他数量级，则可能主导目标函数，使估计器无法按预期正确地学习其他特征。</p>\n<p>例子：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; from sklearn import preprocessing</span><br><span class=\"line\">&gt;&gt;&gt; import numpy as np</span><br><span class=\"line\">&gt;&gt;&gt; X_train = np.array([[ 1., -1.,  2.],</span><br><span class=\"line\">...                     [ 2.,  0.,  0.],</span><br><span class=\"line\">...                     [ 0.,  1., -1.]])</span><br><span class=\"line\">&gt;&gt;&gt; X_scaled = preprocessing.scale(X_train)</span><br><span class=\"line\">&gt;&gt;&gt; X_scaled</span><br><span class=\"line\">array([[ 0.        , -1.22474487,  1.33630621],</span><br><span class=\"line\">       [ 1.22474487,  0.        , -0.26726124],</span><br><span class=\"line\">       [-1.22474487,  1.22474487, -1.06904497]])</span><br></pre></td></tr></table></figure></p>\n<p>标准化后的数据符合标准正太分布<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; X_scaled.mean(axis=0)</span><br><span class=\"line\">array([ 0.,  0.,  0.])</span><br><span class=\"line\">&gt;&gt;&gt; X_scaled.std(axis=0)</span><br><span class=\"line\">array([ 1.,  1.,  1.])</span><br></pre></td></tr></table></figure></p>\n<p>预处理模块还提供了一个实用程序级StandardScaler，它实现了Transformer API来计算训练集上的平均值和标准偏差，以便能够稍后在测试集上重新应用相同的变换。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; scaler = preprocessing.StandardScaler().fit(X_train)</span><br><span class=\"line\">&gt;&gt;&gt; scaler</span><br><span class=\"line\">StandardScaler(copy=True, with_mean=True, with_std=True)</span><br><span class=\"line\">&gt;&gt;&gt; scaler.mean_</span><br><span class=\"line\">array([ 1.        ,  0.        ,  0.33333333])</span><br><span class=\"line\">&gt;&gt;&gt; scaler.scale_</span><br><span class=\"line\">array([ 0.81649658,  0.81649658,  1.24721913])</span><br><span class=\"line\">&gt;&gt;&gt; scaler.transform(X_train)</span><br><span class=\"line\">array([[ 0.        , -1.22474487,  1.33630621],</span><br><span class=\"line\">       [ 1.22474487,  0.        , -0.26726124],</span><br><span class=\"line\">       [-1.22474487,  1.22474487, -1.06904497]])</span><br></pre></td></tr></table></figure></p>\n<p>使用转换器可以对新数据进行转换<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; X_test = [[-1., 1., 0.]]</span><br><span class=\"line\">&gt;&gt;&gt; scaler.transform(X_test)</span><br><span class=\"line\">array([[-2.44948974,  1.22474487, -0.26726124]])</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"2）min-max标准化\"><a href=\"#2）min-max标准化\" class=\"headerlink\" title=\"2）min-max标准化\"></a>2）min-max标准化</h2><p>X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;&gt; X_train = np.array([[ 1., -1.,  2.],</span><br><span class=\"line\">...                      [ 2.,  0.,  0.],</span><br><span class=\"line\">...                      [ 0.,  1., -1.]])</span><br><span class=\"line\">&gt;&gt;&gt; min_max_scaler = preprocessing.MinMaxScaler()</span><br><span class=\"line\">&gt;&gt;&gt; X_train_minmax = min_max_scaler.fit_transform(X_train)</span><br><span class=\"line\">&gt;&gt;&gt; X_train_minmax</span><br><span class=\"line\">array([[ 0.5       ,  0.        ,  1.        ],</span><br><span class=\"line\">       [ 1.        ,  0.5       ,  0.33333333],</span><br><span class=\"line\">       [ 0.        ,  1.        ,  0.        ]])</span><br></pre></td></tr></table></figure>\n<p>上边我们创建的min_max_scaler 同样适用于新的测试数据<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; X_test = np.array([[ -3., -1.,  4.]])</span><br><span class=\"line\">&gt;&gt;&gt; X_test_minmax = min_max_scaler.transform(X_test)</span><br><span class=\"line\">&gt;&gt;&gt; X_test_minmax</span><br><span class=\"line\">array([[-1.5       ,  0.        ,  1.66666667]])</span><br></pre></td></tr></table></figure></p>\n<p>可以通过scale_和min方法查看标准差和最小值<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; min_max_scaler.scale_ </span><br><span class=\"line\">array([ 0.5       ,  0.5       ,  0.33333333])</span><br><span class=\"line\">&gt;&gt;&gt; min_max_scaler.min_</span><br><span class=\"line\">array([ 0.        ,  0.5       ,  0.33333333])</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"3）最大值标准化\"><a href=\"#3）最大值标准化\" class=\"headerlink\" title=\"3）最大值标准化\"></a>3）最大值标准化</h2><p>对于每个数值／每个维度的最大值</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; X_train</span><br><span class=\"line\">array([[ 1., -1.,  2.],</span><br><span class=\"line\">       [ 2.,  0.,  0.],</span><br><span class=\"line\">       [ 0.,  1., -1.]])</span><br><span class=\"line\">&gt;&gt;&gt; max_abs_scaler = preprocessing.MaxAbsScaler()</span><br><span class=\"line\">&gt;&gt;&gt; X_train_maxabs = max_abs_scaler.fit_transform(X_train)</span><br><span class=\"line\">&gt;&gt;&gt; X_train_maxabs</span><br><span class=\"line\">array([[ 0.5, -1. ,  1. ],</span><br><span class=\"line\">       [ 1. ,  0. ,  0. ],</span><br><span class=\"line\">       [ 0. ,  1. , -0.5]])</span><br><span class=\"line\">&gt;&gt;&gt; X_test = np.array([[ -3., -1.,  4.]])</span><br><span class=\"line\">&gt;&gt;&gt; X_test_maxabs = max_abs_scaler.transform(X_test)</span><br><span class=\"line\">&gt;&gt;&gt; X_test_maxabs                 </span><br><span class=\"line\">array([[-1.5, -1. ,  2. ]])</span><br><span class=\"line\">&gt;&gt;&gt; max_abs_scaler.scale_         </span><br><span class=\"line\">array([ 2.,  1.,  2.])</span><br></pre></td></tr></table></figure>\n<h2 id=\"4）规范化\"><a href=\"#4）规范化\" class=\"headerlink\" title=\"4）规范化\"></a>4）规范化</h2><p>规范化是文本分类和聚类中向量空间模型的基础</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; X = [[ 1., -1.,  2.],</span><br><span class=\"line\">...      [ 2.,  0.,  0.],</span><br><span class=\"line\">...      [ 0.,  1., -1.]]</span><br><span class=\"line\">&gt;&gt;&gt; X_normalized = preprocessing.normalize(X, norm=&apos;l2&apos;)</span><br><span class=\"line\">&gt;&gt;&gt; X_normalized</span><br><span class=\"line\">array([[ 0.40824829, -0.40824829,  0.81649658],</span><br><span class=\"line\">       [ 1.        ,  0.        ,  0.        ],</span><br><span class=\"line\">       [ 0.        ,  0.70710678, -0.70710678]])</span><br></pre></td></tr></table></figure>\n<p>解释：norm 该参数是可选的，默认值是l2（向量各元素的平方和然后求平方根），用来规范化每个非零向量，如果axis参数设置为0，则表示的是规范化每个非零的特征维度。</p>\n<p>机器学习中的范数规则：<a href=\"http://blog.csdn.net/zouxy09/article/details/24971995/\" target=\"_blank\" rel=\"external\">点击阅读</a><br><br>其他对应参数：<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html#sklearn.preprocessing.normalize\" target=\"_blank\" rel=\"external\">点击查看</a></p>\n<p>preprocessing模块提供了训练种子的功能，我们可通过以下方式得到一个新的种子，并对新数据进行规范化处理。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; normalizer = preprocessing.Normalizer().fit(X)</span><br><span class=\"line\">&gt;&gt;&gt; normalizer</span><br><span class=\"line\">Normalizer(copy=True, norm=&apos;l2&apos;)</span><br><span class=\"line\">&gt;&gt;&gt; normalizer.transform(X)</span><br><span class=\"line\">array([[ 0.40824829, -0.40824829,  0.81649658],</span><br><span class=\"line\">       [ 1.        ,  0.        ,  0.        ],</span><br><span class=\"line\">       [ 0.        ,  0.70710678, -0.70710678]])</span><br><span class=\"line\">&gt;&gt;&gt; normalizer.transform([[-1,1,0]])</span><br><span class=\"line\">array([[-0.70710678,  0.70710678,  0.        ]])</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"5）二值化\"><a href=\"#5）二值化\" class=\"headerlink\" title=\"5）二值化\"></a>5）二值化</h2><p>将数据转换到0-1 之间<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; X</span><br><span class=\"line\">[[1.0, -1.0, 2.0], [2.0, 0.0, 0.0], [0.0, 1.0, -1.0]]</span><br><span class=\"line\">&gt;&gt;&gt; binarizer = preprocessing.Binarizer().fit(X)</span><br><span class=\"line\">&gt;&gt;&gt; binarizer</span><br><span class=\"line\">Binarizer(copy=True, threshold=0.0)</span><br><span class=\"line\">&gt;&gt;&gt; binarizer.transform(X)</span><br><span class=\"line\">array([[ 1.,  0.,  1.],</span><br><span class=\"line\">       [ 1.,  0.,  0.],</span><br><span class=\"line\">       [ 0.,  1.,  0.]])</span><br></pre></td></tr></table></figure></p>\n<p>可以调整二值化的门阀<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; binarizer = preprocessing.Binarizer(threshold=1.1)</span><br><span class=\"line\">&gt;&gt;&gt; binarizer.transform(X)</span><br><span class=\"line\">array([[ 0.,  0.,  1.],</span><br><span class=\"line\">       [ 1.,  0.,  0.],</span><br><span class=\"line\">       [ 0.,  0.,  0.]])</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"6）编码的分类特征\"><a href=\"#6）编码的分类特征\" class=\"headerlink\" title=\"6）编码的分类特征\"></a>6）编码的分类特征</h2><p>通常情况下，特征不是作为连续值给定的。例如一个人可以有<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[&quot;male&quot;, &quot;female&quot;], [&quot;from Europe&quot;, &quot;from US&quot;, &quot;from Asia&quot;], [&quot;uses Firefox&quot;, &quot;uses Chrome&quot;, &quot;uses Safari&quot;, &quot;uses Internet Explorer&quot;]</span><br></pre></td></tr></table></figure></p>\n<p>这些特征可以被有效的编码为整数，例如<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[&quot;male&quot;, &quot;from US&quot;, &quot;uses Internet Explorer&quot;] =&gt; [0, 1, 3]</span><br><span class=\"line\">[&quot;female&quot;, &quot;from Asia&quot;, &quot;uses Chrome&quot;] would be [1, 2, 1].</span><br></pre></td></tr></table></figure></p>\n<p>这样的整数不应该直接应用到scikit的算法中，可以通过one-of-k或者独热编码（OneHotEncorder），该种处理方式会把每个分类特征的m中可能值转换成m个二进制值。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; enc = preprocessing.OneHotEncoder()</span><br><span class=\"line\">&gt;&gt;&gt; enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])</span><br><span class=\"line\">OneHotEncoder(categorical_features=&apos;all&apos;, dtype=&lt;class &apos;numpy.float64&apos;&gt;,</span><br><span class=\"line\">       handle_unknown=&apos;error&apos;, n_values=&apos;auto&apos;, sparse=True)</span><br><span class=\"line\">&gt;&gt;&gt; enc.transform([[0,1,3]]).toarray()</span><br><span class=\"line\">array([[ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.]])</span><br></pre></td></tr></table></figure>\n<p>默认情况下，从数据集中自动推断出每个特征可以带多少个值。可以明确指定使用的参数n_values。在我们的数据集中有两种性别，三种可能的大陆和四种Web浏览器。然后，我们拟合估计量，并转换一个数据点。在结果中，前两个数字编码性别，下一组三个数字的大陆和最后四个Web浏览器。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; enc = preprocessing.OneHotEncoder(n_values=[2,3,4])</span><br><span class=\"line\">&gt;&gt;&gt; enc.fit([[1,2,3],[0,2,0]])</span><br><span class=\"line\">OneHotEncoder(categorical_features=&apos;all&apos;, dtype=&lt;class &apos;numpy.float64&apos;&gt;,</span><br><span class=\"line\">       handle_unknown=&apos;error&apos;, n_values=[2, 3, 4], sparse=True)</span><br><span class=\"line\">&gt;&gt;&gt; enc.transform([[1,0,0]]).toarray()</span><br><span class=\"line\">array([[ 0.,  1.,  1.,  0.,  0.,  1.,  0.,  0.,  0.]])</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"7）填补缺失值\"><a href=\"#7）填补缺失值\" class=\"headerlink\" title=\"7）填补缺失值\"></a>7）填补缺失值</h2><p>由于各种原因，真实数据中存在大量的空白值，这样的数据集，显然是不符合scikit的要求的，那么preprocessing模块提供这样一个功能，利用已知的数据来填补这些空白。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; import numpy as np</span><br><span class=\"line\">&gt;&gt;&gt; from sklearn.preprocessing import Imputer</span><br><span class=\"line\">&gt;&gt;&gt; imp = Imputer(missing_values=&apos;NaN&apos;,strategy=&apos;mean&apos;,verbose=0)</span><br><span class=\"line\">&gt;&gt;&gt; imp.fit([[1, 2], [np.nan, 3], [7, 6]])</span><br><span class=\"line\">Imputer(axis=0, copy=True, missing_values=&apos;NaN&apos;, strategy=&apos;mean&apos;, verbose=0)</span><br><span class=\"line\">&gt;&gt;&gt; X = [[np.nan, 2], [6, np.nan], [7, 6]]</span><br><span class=\"line\">&gt;&gt;&gt; print(imp.transform(X))                           </span><br><span class=\"line\">[[ 4.          2.        ]</span><br><span class=\"line\"> [ 6.          3.66666667]</span><br><span class=\"line\"> [ 7.          6.        ]]</span><br></pre></td></tr></table></figure></p>\n<p>Imputer同样支持稀疏矩阵<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; import scipy.sparse as sp</span><br><span class=\"line\">&gt;&gt;&gt; X = sp.csc_matrix([[1,2],[0,3],[7,6]])</span><br><span class=\"line\">&gt;&gt;&gt; imp = Imputer(missing_values=0,strategy=&apos;mean&apos;,axis=0)</span><br><span class=\"line\">&gt;&gt;&gt; imp.fit(X)</span><br><span class=\"line\">Imputer(axis=0, copy=True, missing_values=0, strategy=&apos;mean&apos;, verbose=0)</span><br><span class=\"line\">&gt;&gt;&gt; X_test = sp.csc</span><br><span class=\"line\">sp.csc          sp.csc_matrix(  </span><br><span class=\"line\">&gt;&gt;&gt; X_test = sp.csc_matrix([[0,2],[6,0],[7,6]])</span><br><span class=\"line\">&gt;&gt;&gt; print(imp.transform(X_test))</span><br><span class=\"line\">[[ 4.          2.        ]</span><br><span class=\"line\"> [ 6.          3.66666667]</span><br><span class=\"line\"> [ 7.          6.        ]]</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"8）生成多项式特征\"><a href=\"#8）生成多项式特征\" class=\"headerlink\" title=\"8）生成多项式特征\"></a>8）生成多项式特征</h2><p>通常，通过考虑输入数据的非线性特征来增加模型的复杂度是很有用的。一个简单而常用的方法是多项式特征，它可以得到特征的高阶和相互作用项。</p>\n<p>其遵循的原则是 </p>\n<script type=\"math/tex; mode=display\">\n(X_1, X_2) -> (1, X_1, X_2, X_1^2, X_1X_2, X_2^2)</script><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; import numpy as np</span><br><span class=\"line\">&gt;&gt;&gt; from sklearn.preprocessing import PolynomialFeatures</span><br><span class=\"line\">&gt;&gt;&gt; X = np.arange(6).reshape(3, 2)</span><br><span class=\"line\">&gt;&gt;&gt; X                                                 </span><br><span class=\"line\">array([[0, 1],</span><br><span class=\"line\">       [2, 3],</span><br><span class=\"line\">       [4, 5]])</span><br><span class=\"line\">&gt;&gt;&gt; poly = PolynomialFeatures(2)</span><br><span class=\"line\">&gt;&gt;&gt; poly.fit_transform(X)                             </span><br><span class=\"line\">array([[  1.,   0.,   1.,   0.,   0.,   1.],</span><br><span class=\"line\">       [  1.,   2.,   3.,   4.,   6.,   9.],</span><br><span class=\"line\">       [  1.,   4.,   5.,  16.,  20.,  25.]])</span><br></pre></td></tr></table></figure>\n<p>有些情况下，有相互关系的标签才是必须的，这个时候可以通过设置 interaction_only=True 来进行多项式特征的生成<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; X = np.arange(9).reshape(3, 3)</span><br><span class=\"line\">&gt;&gt;&gt; X                                                 </span><br><span class=\"line\">array([[0, 1, 2],</span><br><span class=\"line\">       [3, 4, 5],</span><br><span class=\"line\">       [6, 7, 8]])</span><br><span class=\"line\">&gt;&gt;&gt; poly = PolynomialFeatures(degree=3, interaction_only=True)</span><br><span class=\"line\">&gt;&gt;&gt; poly.fit_transform(X)                             </span><br><span class=\"line\">array([[   1.,    0.,    1.,    2.,    0.,    0.,    2.,    0.],</span><br><span class=\"line\">       [   1.,    3.,    4.,    5.,   12.,   15.,   20.,   60.],</span><br><span class=\"line\">       [   1.,    6.,    7.,    8.,   42.,   48.,   56.,  336.]])</span><br></pre></td></tr></table></figure></p>\n<p>其遵循的规则是：</p>\n<script type=\"math/tex; mode=display\">\n(X_1, X_2, X_3) -> (1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3)</script><hr>\n<p>对应的scikit-learn资料为： <a href=\"http://scikit-learn.org/stable/modules/preprocessing.html\" target=\"_blank\" rel=\"external\">http://scikit-learn.org/stable/modules/preprocessing.html</a></p>\n","site":{"data":{}},"excerpt":"<h1 id=\"一：数据归一化\"><a href=\"#一：数据归一化\" class=\"headerlink\" title=\"一：数据归一化\"></a>一：数据归一化</h1><p>数据归一化（标准化）处理是数据挖掘的一项基础工作，不同评价指标往往具有不同的量纲和量纲单位，这样的情况会影响到数据分析的结果，为了消除指标之间的量纲影响，需要进行数据标准化处理，以解决数据指标之间的可比性。原始数据经过数据标准化处理后，各指标处于同一数量级，适合进行综合对比评价。<br>","more":"<br>归一化方法有两种形式，一种是把数变为（0，1）之间的小数，一种是把有量纲表达式变为无量纲表达式。在机器学习中我们更关注的把数据变到0～1之间，接下来我们讨论的也是第一种形式。</p>\n<h2 id=\"1）min-max标准化\"><a href=\"#1）min-max标准化\" class=\"headerlink\" title=\"1）min-max标准化\"></a>1）min-max标准化</h2><p>min-max标准化也叫做离差标准化，是对原始数据的线性变换，使结果落到[0,1]区间，其对应的数学公式如下：</p>\n<script type=\"math/tex; mode=display\">\nX_{scale} = \\frac{x-min}{max-min}</script><p>对应的python实现为<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># x为数据 比如说 [1,2,1,3,2,4,1]</span><br><span class=\"line\">def Normalization(x):</span><br><span class=\"line\">    return [(float(i)-min(x))/float(max(x)-min(x)) for i in x]</span><br></pre></td></tr></table></figure></p>\n<p>如果要将数据转换到[-1,1]之间，可以修改其数学公式为：</p>\n<script type=\"math/tex; mode=display\">\nX_{scale} = \\frac{x-x_{mean}}{max-min}</script><p>x_mean 表示平均值。</p>\n<p>对应的python实现为<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import numpy as np</span><br><span class=\"line\"></span><br><span class=\"line\"># x为数据 比如说 [1,2,1,3,2,4,1]</span><br><span class=\"line\">def Normalization(x):</span><br><span class=\"line\">    return [(float(i)-np.mean(x))/float(max(x)-min(x)) for i in x]</span><br></pre></td></tr></table></figure></p>\n<p>其中max为样本数据的最大值，min为样本数据的最小值。这种方法有个缺陷就是当有新数据加入时，可能导致max和min的变化，需要重新定义。</p>\n<p>该标准化方法有一个缺点就是，如果数据中有一些偏离正常数据的异常点，就会导致标准化结果的不准确性。比如说一个公司员工（A，B，C，D）的薪水为6k,8k,7k,10w,这种情况下进行归一化对每个员工来讲都是不合理的。</p>\n<p>当然还有一些其他的办法也能实现数据的标准化。</p>\n<h2 id=\"2）z-score标准化\"><a href=\"#2）z-score标准化\" class=\"headerlink\" title=\"2）z-score标准化\"></a>2）z-score标准化</h2><p>z-score标准化也叫标准差标准化，代表的是分值偏离均值的程度，经过处理的数据符合标准正态分布，即均值为0，标准差为1。其转化函数为</p>\n<script type=\"math/tex; mode=display\">\nX_{scale} = \\frac{x-\\mu }{\\sigma }</script><p>其中μ为所有样本数据的均值，σ为所有样本数据的标准差。</p>\n<p>其对应的python实现为：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import numpy as np</span><br><span class=\"line\"></span><br><span class=\"line\">#x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</span><br><span class=\"line\">def z_score(x):</span><br><span class=\"line\">    return (x - np.mean(x) )/np.std(x, ddof = 1)</span><br></pre></td></tr></table></figure></p>\n<p>z-score标准化方法同样对于离群异常值的影响。接下来看一种改进的z-score标准化方法。</p>\n<h2 id=\"3）改进的z-score标准化\"><a href=\"#3）改进的z-score标准化\" class=\"headerlink\" title=\"3）改进的z-score标准化\"></a>3）改进的z-score标准化</h2><p>将标准分公式中的均值改为中位数，将标准差改为绝对偏差。</p>\n<script type=\"math/tex; mode=display\">\nX_{scale} = \\frac{x-x_{center} }{\\sigma_{1} }</script><p>中位数是指将所有数据进行排序，取中间的那个值，如数据量是偶数，则取中间两个数据的平均值。</p>\n<p>σ1为所有样本数据的绝对偏差,其计算公式为：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{1}{N} \\sum_{1}^{n}|x_{i} - x_{center}|</script><hr>\n<h1 id=\"二：sklearn中的归一化\"><a href=\"#二：sklearn中的归一化\" class=\"headerlink\" title=\"二：sklearn中的归一化\"></a>二：sklearn中的归一化</h1><p>sklearn.preprocessing 提供了一些实用的函数 用来处理数据的维度，以供算法使用。</p>\n<h2 id=\"1）均值-标准差缩放\"><a href=\"#1）均值-标准差缩放\" class=\"headerlink\" title=\"1）均值-标准差缩放\"></a>1）均值-标准差缩放</h2><p>即我们上边对应的z-score标准化。<br>在sklearn的学习中，数据集的标准化是很多机器学习模型算法的常见要求。如果个别特征看起来不是很符合正态分布，那么他们可能为表现不好。</p>\n<p>实际上，我们经常忽略分布的形状，只是通过减去整组数据的平均值，使之更靠近数据中心分布，然后通过将非连续数特征除以其标准偏差进行分类。</p>\n<p>例如，用于学习算法（例如支持向量机的RBF内核或线性模型的l1和l2正则化器）的目标函数中使用的许多元素假设所有特征都以零为中心并且具有相同顺序的方差。如果特征的方差大于其他数量级，则可能主导目标函数，使估计器无法按预期正确地学习其他特征。</p>\n<p>例子：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; from sklearn import preprocessing</span><br><span class=\"line\">&gt;&gt;&gt; import numpy as np</span><br><span class=\"line\">&gt;&gt;&gt; X_train = np.array([[ 1., -1.,  2.],</span><br><span class=\"line\">...                     [ 2.,  0.,  0.],</span><br><span class=\"line\">...                     [ 0.,  1., -1.]])</span><br><span class=\"line\">&gt;&gt;&gt; X_scaled = preprocessing.scale(X_train)</span><br><span class=\"line\">&gt;&gt;&gt; X_scaled</span><br><span class=\"line\">array([[ 0.        , -1.22474487,  1.33630621],</span><br><span class=\"line\">       [ 1.22474487,  0.        , -0.26726124],</span><br><span class=\"line\">       [-1.22474487,  1.22474487, -1.06904497]])</span><br></pre></td></tr></table></figure></p>\n<p>标准化后的数据符合标准正太分布<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; X_scaled.mean(axis=0)</span><br><span class=\"line\">array([ 0.,  0.,  0.])</span><br><span class=\"line\">&gt;&gt;&gt; X_scaled.std(axis=0)</span><br><span class=\"line\">array([ 1.,  1.,  1.])</span><br></pre></td></tr></table></figure></p>\n<p>预处理模块还提供了一个实用程序级StandardScaler，它实现了Transformer API来计算训练集上的平均值和标准偏差，以便能够稍后在测试集上重新应用相同的变换。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; scaler = preprocessing.StandardScaler().fit(X_train)</span><br><span class=\"line\">&gt;&gt;&gt; scaler</span><br><span class=\"line\">StandardScaler(copy=True, with_mean=True, with_std=True)</span><br><span class=\"line\">&gt;&gt;&gt; scaler.mean_</span><br><span class=\"line\">array([ 1.        ,  0.        ,  0.33333333])</span><br><span class=\"line\">&gt;&gt;&gt; scaler.scale_</span><br><span class=\"line\">array([ 0.81649658,  0.81649658,  1.24721913])</span><br><span class=\"line\">&gt;&gt;&gt; scaler.transform(X_train)</span><br><span class=\"line\">array([[ 0.        , -1.22474487,  1.33630621],</span><br><span class=\"line\">       [ 1.22474487,  0.        , -0.26726124],</span><br><span class=\"line\">       [-1.22474487,  1.22474487, -1.06904497]])</span><br></pre></td></tr></table></figure></p>\n<p>使用转换器可以对新数据进行转换<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; X_test = [[-1., 1., 0.]]</span><br><span class=\"line\">&gt;&gt;&gt; scaler.transform(X_test)</span><br><span class=\"line\">array([[-2.44948974,  1.22474487, -0.26726124]])</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"2）min-max标准化\"><a href=\"#2）min-max标准化\" class=\"headerlink\" title=\"2）min-max标准化\"></a>2）min-max标准化</h2><p>X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;&gt; X_train = np.array([[ 1., -1.,  2.],</span><br><span class=\"line\">...                      [ 2.,  0.,  0.],</span><br><span class=\"line\">...                      [ 0.,  1., -1.]])</span><br><span class=\"line\">&gt;&gt;&gt; min_max_scaler = preprocessing.MinMaxScaler()</span><br><span class=\"line\">&gt;&gt;&gt; X_train_minmax = min_max_scaler.fit_transform(X_train)</span><br><span class=\"line\">&gt;&gt;&gt; X_train_minmax</span><br><span class=\"line\">array([[ 0.5       ,  0.        ,  1.        ],</span><br><span class=\"line\">       [ 1.        ,  0.5       ,  0.33333333],</span><br><span class=\"line\">       [ 0.        ,  1.        ,  0.        ]])</span><br></pre></td></tr></table></figure>\n<p>上边我们创建的min_max_scaler 同样适用于新的测试数据<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; X_test = np.array([[ -3., -1.,  4.]])</span><br><span class=\"line\">&gt;&gt;&gt; X_test_minmax = min_max_scaler.transform(X_test)</span><br><span class=\"line\">&gt;&gt;&gt; X_test_minmax</span><br><span class=\"line\">array([[-1.5       ,  0.        ,  1.66666667]])</span><br></pre></td></tr></table></figure></p>\n<p>可以通过scale_和min方法查看标准差和最小值<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; min_max_scaler.scale_ </span><br><span class=\"line\">array([ 0.5       ,  0.5       ,  0.33333333])</span><br><span class=\"line\">&gt;&gt;&gt; min_max_scaler.min_</span><br><span class=\"line\">array([ 0.        ,  0.5       ,  0.33333333])</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"3）最大值标准化\"><a href=\"#3）最大值标准化\" class=\"headerlink\" title=\"3）最大值标准化\"></a>3）最大值标准化</h2><p>对于每个数值／每个维度的最大值</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; X_train</span><br><span class=\"line\">array([[ 1., -1.,  2.],</span><br><span class=\"line\">       [ 2.,  0.,  0.],</span><br><span class=\"line\">       [ 0.,  1., -1.]])</span><br><span class=\"line\">&gt;&gt;&gt; max_abs_scaler = preprocessing.MaxAbsScaler()</span><br><span class=\"line\">&gt;&gt;&gt; X_train_maxabs = max_abs_scaler.fit_transform(X_train)</span><br><span class=\"line\">&gt;&gt;&gt; X_train_maxabs</span><br><span class=\"line\">array([[ 0.5, -1. ,  1. ],</span><br><span class=\"line\">       [ 1. ,  0. ,  0. ],</span><br><span class=\"line\">       [ 0. ,  1. , -0.5]])</span><br><span class=\"line\">&gt;&gt;&gt; X_test = np.array([[ -3., -1.,  4.]])</span><br><span class=\"line\">&gt;&gt;&gt; X_test_maxabs = max_abs_scaler.transform(X_test)</span><br><span class=\"line\">&gt;&gt;&gt; X_test_maxabs                 </span><br><span class=\"line\">array([[-1.5, -1. ,  2. ]])</span><br><span class=\"line\">&gt;&gt;&gt; max_abs_scaler.scale_         </span><br><span class=\"line\">array([ 2.,  1.,  2.])</span><br></pre></td></tr></table></figure>\n<h2 id=\"4）规范化\"><a href=\"#4）规范化\" class=\"headerlink\" title=\"4）规范化\"></a>4）规范化</h2><p>规范化是文本分类和聚类中向量空间模型的基础</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; X = [[ 1., -1.,  2.],</span><br><span class=\"line\">...      [ 2.,  0.,  0.],</span><br><span class=\"line\">...      [ 0.,  1., -1.]]</span><br><span class=\"line\">&gt;&gt;&gt; X_normalized = preprocessing.normalize(X, norm=&apos;l2&apos;)</span><br><span class=\"line\">&gt;&gt;&gt; X_normalized</span><br><span class=\"line\">array([[ 0.40824829, -0.40824829,  0.81649658],</span><br><span class=\"line\">       [ 1.        ,  0.        ,  0.        ],</span><br><span class=\"line\">       [ 0.        ,  0.70710678, -0.70710678]])</span><br></pre></td></tr></table></figure>\n<p>解释：norm 该参数是可选的，默认值是l2（向量各元素的平方和然后求平方根），用来规范化每个非零向量，如果axis参数设置为0，则表示的是规范化每个非零的特征维度。</p>\n<p>机器学习中的范数规则：<a href=\"http://blog.csdn.net/zouxy09/article/details/24971995/\" target=\"_blank\" rel=\"external\">点击阅读</a><br><br>其他对应参数：<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html#sklearn.preprocessing.normalize\" target=\"_blank\" rel=\"external\">点击查看</a></p>\n<p>preprocessing模块提供了训练种子的功能，我们可通过以下方式得到一个新的种子，并对新数据进行规范化处理。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; normalizer = preprocessing.Normalizer().fit(X)</span><br><span class=\"line\">&gt;&gt;&gt; normalizer</span><br><span class=\"line\">Normalizer(copy=True, norm=&apos;l2&apos;)</span><br><span class=\"line\">&gt;&gt;&gt; normalizer.transform(X)</span><br><span class=\"line\">array([[ 0.40824829, -0.40824829,  0.81649658],</span><br><span class=\"line\">       [ 1.        ,  0.        ,  0.        ],</span><br><span class=\"line\">       [ 0.        ,  0.70710678, -0.70710678]])</span><br><span class=\"line\">&gt;&gt;&gt; normalizer.transform([[-1,1,0]])</span><br><span class=\"line\">array([[-0.70710678,  0.70710678,  0.        ]])</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"5）二值化\"><a href=\"#5）二值化\" class=\"headerlink\" title=\"5）二值化\"></a>5）二值化</h2><p>将数据转换到0-1 之间<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; X</span><br><span class=\"line\">[[1.0, -1.0, 2.0], [2.0, 0.0, 0.0], [0.0, 1.0, -1.0]]</span><br><span class=\"line\">&gt;&gt;&gt; binarizer = preprocessing.Binarizer().fit(X)</span><br><span class=\"line\">&gt;&gt;&gt; binarizer</span><br><span class=\"line\">Binarizer(copy=True, threshold=0.0)</span><br><span class=\"line\">&gt;&gt;&gt; binarizer.transform(X)</span><br><span class=\"line\">array([[ 1.,  0.,  1.],</span><br><span class=\"line\">       [ 1.,  0.,  0.],</span><br><span class=\"line\">       [ 0.,  1.,  0.]])</span><br></pre></td></tr></table></figure></p>\n<p>可以调整二值化的门阀<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; binarizer = preprocessing.Binarizer(threshold=1.1)</span><br><span class=\"line\">&gt;&gt;&gt; binarizer.transform(X)</span><br><span class=\"line\">array([[ 0.,  0.,  1.],</span><br><span class=\"line\">       [ 1.,  0.,  0.],</span><br><span class=\"line\">       [ 0.,  0.,  0.]])</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"6）编码的分类特征\"><a href=\"#6）编码的分类特征\" class=\"headerlink\" title=\"6）编码的分类特征\"></a>6）编码的分类特征</h2><p>通常情况下，特征不是作为连续值给定的。例如一个人可以有<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[&quot;male&quot;, &quot;female&quot;], [&quot;from Europe&quot;, &quot;from US&quot;, &quot;from Asia&quot;], [&quot;uses Firefox&quot;, &quot;uses Chrome&quot;, &quot;uses Safari&quot;, &quot;uses Internet Explorer&quot;]</span><br></pre></td></tr></table></figure></p>\n<p>这些特征可以被有效的编码为整数，例如<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[&quot;male&quot;, &quot;from US&quot;, &quot;uses Internet Explorer&quot;] =&gt; [0, 1, 3]</span><br><span class=\"line\">[&quot;female&quot;, &quot;from Asia&quot;, &quot;uses Chrome&quot;] would be [1, 2, 1].</span><br></pre></td></tr></table></figure></p>\n<p>这样的整数不应该直接应用到scikit的算法中，可以通过one-of-k或者独热编码（OneHotEncorder），该种处理方式会把每个分类特征的m中可能值转换成m个二进制值。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; enc = preprocessing.OneHotEncoder()</span><br><span class=\"line\">&gt;&gt;&gt; enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])</span><br><span class=\"line\">OneHotEncoder(categorical_features=&apos;all&apos;, dtype=&lt;class &apos;numpy.float64&apos;&gt;,</span><br><span class=\"line\">       handle_unknown=&apos;error&apos;, n_values=&apos;auto&apos;, sparse=True)</span><br><span class=\"line\">&gt;&gt;&gt; enc.transform([[0,1,3]]).toarray()</span><br><span class=\"line\">array([[ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.]])</span><br></pre></td></tr></table></figure>\n<p>默认情况下，从数据集中自动推断出每个特征可以带多少个值。可以明确指定使用的参数n_values。在我们的数据集中有两种性别，三种可能的大陆和四种Web浏览器。然后，我们拟合估计量，并转换一个数据点。在结果中，前两个数字编码性别，下一组三个数字的大陆和最后四个Web浏览器。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; enc = preprocessing.OneHotEncoder(n_values=[2,3,4])</span><br><span class=\"line\">&gt;&gt;&gt; enc.fit([[1,2,3],[0,2,0]])</span><br><span class=\"line\">OneHotEncoder(categorical_features=&apos;all&apos;, dtype=&lt;class &apos;numpy.float64&apos;&gt;,</span><br><span class=\"line\">       handle_unknown=&apos;error&apos;, n_values=[2, 3, 4], sparse=True)</span><br><span class=\"line\">&gt;&gt;&gt; enc.transform([[1,0,0]]).toarray()</span><br><span class=\"line\">array([[ 0.,  1.,  1.,  0.,  0.,  1.,  0.,  0.,  0.]])</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"7）填补缺失值\"><a href=\"#7）填补缺失值\" class=\"headerlink\" title=\"7）填补缺失值\"></a>7）填补缺失值</h2><p>由于各种原因，真实数据中存在大量的空白值，这样的数据集，显然是不符合scikit的要求的，那么preprocessing模块提供这样一个功能，利用已知的数据来填补这些空白。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; import numpy as np</span><br><span class=\"line\">&gt;&gt;&gt; from sklearn.preprocessing import Imputer</span><br><span class=\"line\">&gt;&gt;&gt; imp = Imputer(missing_values=&apos;NaN&apos;,strategy=&apos;mean&apos;,verbose=0)</span><br><span class=\"line\">&gt;&gt;&gt; imp.fit([[1, 2], [np.nan, 3], [7, 6]])</span><br><span class=\"line\">Imputer(axis=0, copy=True, missing_values=&apos;NaN&apos;, strategy=&apos;mean&apos;, verbose=0)</span><br><span class=\"line\">&gt;&gt;&gt; X = [[np.nan, 2], [6, np.nan], [7, 6]]</span><br><span class=\"line\">&gt;&gt;&gt; print(imp.transform(X))                           </span><br><span class=\"line\">[[ 4.          2.        ]</span><br><span class=\"line\"> [ 6.          3.66666667]</span><br><span class=\"line\"> [ 7.          6.        ]]</span><br></pre></td></tr></table></figure></p>\n<p>Imputer同样支持稀疏矩阵<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; import scipy.sparse as sp</span><br><span class=\"line\">&gt;&gt;&gt; X = sp.csc_matrix([[1,2],[0,3],[7,6]])</span><br><span class=\"line\">&gt;&gt;&gt; imp = Imputer(missing_values=0,strategy=&apos;mean&apos;,axis=0)</span><br><span class=\"line\">&gt;&gt;&gt; imp.fit(X)</span><br><span class=\"line\">Imputer(axis=0, copy=True, missing_values=0, strategy=&apos;mean&apos;, verbose=0)</span><br><span class=\"line\">&gt;&gt;&gt; X_test = sp.csc</span><br><span class=\"line\">sp.csc          sp.csc_matrix(  </span><br><span class=\"line\">&gt;&gt;&gt; X_test = sp.csc_matrix([[0,2],[6,0],[7,6]])</span><br><span class=\"line\">&gt;&gt;&gt; print(imp.transform(X_test))</span><br><span class=\"line\">[[ 4.          2.        ]</span><br><span class=\"line\"> [ 6.          3.66666667]</span><br><span class=\"line\"> [ 7.          6.        ]]</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"8）生成多项式特征\"><a href=\"#8）生成多项式特征\" class=\"headerlink\" title=\"8）生成多项式特征\"></a>8）生成多项式特征</h2><p>通常，通过考虑输入数据的非线性特征来增加模型的复杂度是很有用的。一个简单而常用的方法是多项式特征，它可以得到特征的高阶和相互作用项。</p>\n<p>其遵循的原则是 </p>\n<script type=\"math/tex; mode=display\">\n(X_1, X_2) -> (1, X_1, X_2, X_1^2, X_1X_2, X_2^2)</script><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; import numpy as np</span><br><span class=\"line\">&gt;&gt;&gt; from sklearn.preprocessing import PolynomialFeatures</span><br><span class=\"line\">&gt;&gt;&gt; X = np.arange(6).reshape(3, 2)</span><br><span class=\"line\">&gt;&gt;&gt; X                                                 </span><br><span class=\"line\">array([[0, 1],</span><br><span class=\"line\">       [2, 3],</span><br><span class=\"line\">       [4, 5]])</span><br><span class=\"line\">&gt;&gt;&gt; poly = PolynomialFeatures(2)</span><br><span class=\"line\">&gt;&gt;&gt; poly.fit_transform(X)                             </span><br><span class=\"line\">array([[  1.,   0.,   1.,   0.,   0.,   1.],</span><br><span class=\"line\">       [  1.,   2.,   3.,   4.,   6.,   9.],</span><br><span class=\"line\">       [  1.,   4.,   5.,  16.,  20.,  25.]])</span><br></pre></td></tr></table></figure>\n<p>有些情况下，有相互关系的标签才是必须的，这个时候可以通过设置 interaction_only=True 来进行多项式特征的生成<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; X = np.arange(9).reshape(3, 3)</span><br><span class=\"line\">&gt;&gt;&gt; X                                                 </span><br><span class=\"line\">array([[0, 1, 2],</span><br><span class=\"line\">       [3, 4, 5],</span><br><span class=\"line\">       [6, 7, 8]])</span><br><span class=\"line\">&gt;&gt;&gt; poly = PolynomialFeatures(degree=3, interaction_only=True)</span><br><span class=\"line\">&gt;&gt;&gt; poly.fit_transform(X)                             </span><br><span class=\"line\">array([[   1.,    0.,    1.,    2.,    0.,    0.,    2.,    0.],</span><br><span class=\"line\">       [   1.,    3.,    4.,    5.,   12.,   15.,   20.,   60.],</span><br><span class=\"line\">       [   1.,    6.,    7.,    8.,   42.,   48.,   56.,  336.]])</span><br></pre></td></tr></table></figure></p>\n<p>其遵循的规则是：</p>\n<script type=\"math/tex; mode=display\">\n(X_1, X_2, X_3) -> (1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3)</script><hr>\n<p>对应的scikit-learn资料为： <a href=\"http://scikit-learn.org/stable/modules/preprocessing.html\" target=\"_blank\" rel=\"external\">http://scikit-learn.org/stable/modules/preprocessing.html</a></p>"},{"title":"梯度算法之批量梯度下降，随机梯度下降和小批量梯度下降","date":"2017-12-14T06:40:43.000Z","_content":"\n在机器学习领域，体梯度下降算法分为三种\n\n- 批量梯度下降算法（BGD，Batch gradient descent algorithm）\n- 随机梯度下降算法（SGD，Stochastic gradient descent algorithm）\n- 小批量梯度下降算法（MBGD，Mini-batch gradient descent algorithm）\n\n<!--More-->\n\n# 批量梯度下降算法\nBGD是最原始的梯度下降算法，每一次迭代使用全部的样本，即权重的迭代公式中(公式中用$\\theta$代替$\\theta_i$)，\n$$\n\\jmath (\\theta _0,\\theta _1,...,\\theta _n)=\\sum_{i=0}^{m}( h_\\theta(x_0,x_1,...,x_n)-y_i )^2        \n$$\n$$\n\\theta _i = \\theta _i - \\alpha \\frac{\\partial \\jmath (\\theta _1,\\theta _2,...,\\theta _n)}{\\partial \\theta _i}\n$$\n$$\n公式(1)\n$$\n\n这里的m代表所有的样本，表示从第一个样本遍历到最后一个样本。\n\n特点：\n\n- 能达到全局最优解，易于并行实现\n- 当样本数目很多时，训练过程缓慢\n\n# 随机梯度下降算法\nSGD的思想是更新每一个参数时都使用一个样本来进行更新，即公式（1）中m为1。每次更新参数都只使用一个样本，进行多次更新。这样在样本量很大的情况下，可能只用到其中的一部分样本就能得到最优解了。\n但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。\n\n特点：\n- 训练速度快\n- 准确度下降，并不是最优解，不易于并行实现\n\n# 小批量梯度下降算法\nMBGD的算法思想就是在更新每一参数时都使用一部分样本来进行更新，也就是公式（1）中的m的值大于1小于所有样本的数量。\n\n相对于随机梯度下降，Mini-batch梯度下降降低了收敛波动性，即降低了参数更新的方差，使得更新更加稳定。相对于批量梯度下降，其提高了每次学习的速度。并且其不用担心内存瓶颈从而可以利用矩阵运算进行高效计算。一般而言每次更新随机选择[50,256]个样本进行学习，但是也要根据具体问题而选择，实践中可以进行多次试验，选择一个更新速度与更次次数都较适合的样本数。mini-batch梯度下降可以保证收敛性，常用于神经网络中。\n\n\n# 补充\n在样本量较小的情况下，可以使用批量梯度下降算法，样本量较大的情况或者线上，可以使用随机梯度下降算法或者小批量梯度下降算法。\n\n在机器学习中的无约束优化算法，除了梯度下降以外，还有前面提到的最小二乘法，此外还有牛顿法和拟牛顿法。\n\n梯度下降法和最小二乘法相比，梯度下降法需要选择步长，而最小二乘法不需要。梯度下降法是迭代求解，最小二乘法是计算解析解。如果样本量不算很大，且存在解析解，最小二乘法比起梯度下降法要有优势，计算速度很快。但是如果样本量很大，用最小二乘法由于需要求一个超级大的逆矩阵，这时就很难或者很慢才能求解解析解了，使用迭代的梯度下降法比较有优势。\n\n梯度下降法和牛顿法/拟牛顿法相比，两者都是迭代求解，不过梯度下降法是梯度求解，而牛顿法/拟牛顿法是用二阶的海森矩阵的逆矩阵或伪逆矩阵求解。相对而言，使用牛顿法/拟牛顿法收敛更快。但是每次迭代的时间比梯度下降法长。\n\n# sklearn中的SGD\nsklearn官网上查了一下，并没有找到BGD和MBGD的相关文档，只是看到可SGD的，感兴趣的可以直接去官网看英文文档，点击SGD查看：[SGD](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)，这也有一个中文的 [SGD](http://sklearn.lzjqsdd.com/modules/sgd.html)\n\n```\nIn [1]: from sklearn.linear_model import SGDClassifier\n\nIn [2]: X = [[0., 0.], [1., 1.]]\n\nIn [3]: y = [0, 1]\n\nIn [4]: clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\")\n\nIn [5]: clf.fit(X, y)\nOut[5]: \nSGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,\n       penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n       verbose=0, warm_start=False)\n\nIn [6]:  clf.predict([[2., 2.]])\nOut[6]: array([1])\n\nIn [7]: clf.coef_ \nOut[7]: array([[ 9.91080278,  9.91080278]])\n\nIn [8]: clf.intercept_ \nOut[8]: array([-9.97004991])\n```\n参考：\n\n- https://www.cnblogs.com/pinard/p/5970503.html\n- http://blog.csdn.net/uestc_c2_403/article/details/74910107","source":"_posts/机器学习/梯度算法之批量梯度下降，随机梯度下降和小批量梯度下降.md","raw":"---\ntitle: 梯度算法之批量梯度下降，随机梯度下降和小批量梯度下降\ndate: 2017-12-14 14:40:43\ntags: [梯度下降]\ncategories: 技术篇\n---\n\n在机器学习领域，体梯度下降算法分为三种\n\n- 批量梯度下降算法（BGD，Batch gradient descent algorithm）\n- 随机梯度下降算法（SGD，Stochastic gradient descent algorithm）\n- 小批量梯度下降算法（MBGD，Mini-batch gradient descent algorithm）\n\n<!--More-->\n\n# 批量梯度下降算法\nBGD是最原始的梯度下降算法，每一次迭代使用全部的样本，即权重的迭代公式中(公式中用$\\theta$代替$\\theta_i$)，\n$$\n\\jmath (\\theta _0,\\theta _1,...,\\theta _n)=\\sum_{i=0}^{m}( h_\\theta(x_0,x_1,...,x_n)-y_i )^2        \n$$\n$$\n\\theta _i = \\theta _i - \\alpha \\frac{\\partial \\jmath (\\theta _1,\\theta _2,...,\\theta _n)}{\\partial \\theta _i}\n$$\n$$\n公式(1)\n$$\n\n这里的m代表所有的样本，表示从第一个样本遍历到最后一个样本。\n\n特点：\n\n- 能达到全局最优解，易于并行实现\n- 当样本数目很多时，训练过程缓慢\n\n# 随机梯度下降算法\nSGD的思想是更新每一个参数时都使用一个样本来进行更新，即公式（1）中m为1。每次更新参数都只使用一个样本，进行多次更新。这样在样本量很大的情况下，可能只用到其中的一部分样本就能得到最优解了。\n但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。\n\n特点：\n- 训练速度快\n- 准确度下降，并不是最优解，不易于并行实现\n\n# 小批量梯度下降算法\nMBGD的算法思想就是在更新每一参数时都使用一部分样本来进行更新，也就是公式（1）中的m的值大于1小于所有样本的数量。\n\n相对于随机梯度下降，Mini-batch梯度下降降低了收敛波动性，即降低了参数更新的方差，使得更新更加稳定。相对于批量梯度下降，其提高了每次学习的速度。并且其不用担心内存瓶颈从而可以利用矩阵运算进行高效计算。一般而言每次更新随机选择[50,256]个样本进行学习，但是也要根据具体问题而选择，实践中可以进行多次试验，选择一个更新速度与更次次数都较适合的样本数。mini-batch梯度下降可以保证收敛性，常用于神经网络中。\n\n\n# 补充\n在样本量较小的情况下，可以使用批量梯度下降算法，样本量较大的情况或者线上，可以使用随机梯度下降算法或者小批量梯度下降算法。\n\n在机器学习中的无约束优化算法，除了梯度下降以外，还有前面提到的最小二乘法，此外还有牛顿法和拟牛顿法。\n\n梯度下降法和最小二乘法相比，梯度下降法需要选择步长，而最小二乘法不需要。梯度下降法是迭代求解，最小二乘法是计算解析解。如果样本量不算很大，且存在解析解，最小二乘法比起梯度下降法要有优势，计算速度很快。但是如果样本量很大，用最小二乘法由于需要求一个超级大的逆矩阵，这时就很难或者很慢才能求解解析解了，使用迭代的梯度下降法比较有优势。\n\n梯度下降法和牛顿法/拟牛顿法相比，两者都是迭代求解，不过梯度下降法是梯度求解，而牛顿法/拟牛顿法是用二阶的海森矩阵的逆矩阵或伪逆矩阵求解。相对而言，使用牛顿法/拟牛顿法收敛更快。但是每次迭代的时间比梯度下降法长。\n\n# sklearn中的SGD\nsklearn官网上查了一下，并没有找到BGD和MBGD的相关文档，只是看到可SGD的，感兴趣的可以直接去官网看英文文档，点击SGD查看：[SGD](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)，这也有一个中文的 [SGD](http://sklearn.lzjqsdd.com/modules/sgd.html)\n\n```\nIn [1]: from sklearn.linear_model import SGDClassifier\n\nIn [2]: X = [[0., 0.], [1., 1.]]\n\nIn [3]: y = [0, 1]\n\nIn [4]: clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\")\n\nIn [5]: clf.fit(X, y)\nOut[5]: \nSGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,\n       penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n       verbose=0, warm_start=False)\n\nIn [6]:  clf.predict([[2., 2.]])\nOut[6]: array([1])\n\nIn [7]: clf.coef_ \nOut[7]: array([[ 9.91080278,  9.91080278]])\n\nIn [8]: clf.intercept_ \nOut[8]: array([-9.97004991])\n```\n参考：\n\n- https://www.cnblogs.com/pinard/p/5970503.html\n- http://blog.csdn.net/uestc_c2_403/article/details/74910107","slug":"机器学习/梯度算法之批量梯度下降，随机梯度下降和小批量梯度下降","published":1,"updated":"2017-12-19T02:54:56.787Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjbd1r26m0014kxuwm2uydy1d","content":"<p>在机器学习领域，体梯度下降算法分为三种</p>\n<ul>\n<li>批量梯度下降算法（BGD，Batch gradient descent algorithm）</li>\n<li>随机梯度下降算法（SGD，Stochastic gradient descent algorithm）</li>\n<li>小批量梯度下降算法（MBGD，Mini-batch gradient descent algorithm）</li>\n</ul>\n<a id=\"more\"></a>\n<h1 id=\"批量梯度下降算法\"><a href=\"#批量梯度下降算法\" class=\"headerlink\" title=\"批量梯度下降算法\"></a>批量梯度下降算法</h1><p>BGD是最原始的梯度下降算法，每一次迭代使用全部的样本，即权重的迭代公式中(公式中用$\\theta$代替$\\theta_i$)，</p>\n<script type=\"math/tex; mode=display\">\n\\jmath (\\theta _0,\\theta _1,...,\\theta _n)=\\sum_{i=0}^{m}( h_\\theta(x_0,x_1,...,x_n)-y_i )^2</script><script type=\"math/tex; mode=display\">\n\\theta _i = \\theta _i - \\alpha \\frac{\\partial \\jmath (\\theta _1,\\theta _2,...,\\theta _n)}{\\partial \\theta _i}</script><script type=\"math/tex; mode=display\">\n公式(1)</script><p>这里的m代表所有的样本，表示从第一个样本遍历到最后一个样本。</p>\n<p>特点：</p>\n<ul>\n<li>能达到全局最优解，易于并行实现</li>\n<li>当样本数目很多时，训练过程缓慢</li>\n</ul>\n<h1 id=\"随机梯度下降算法\"><a href=\"#随机梯度下降算法\" class=\"headerlink\" title=\"随机梯度下降算法\"></a>随机梯度下降算法</h1><p>SGD的思想是更新每一个参数时都使用一个样本来进行更新，即公式（1）中m为1。每次更新参数都只使用一个样本，进行多次更新。这样在样本量很大的情况下，可能只用到其中的一部分样本就能得到最优解了。<br>但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。</p>\n<p>特点：</p>\n<ul>\n<li>训练速度快</li>\n<li>准确度下降，并不是最优解，不易于并行实现</li>\n</ul>\n<h1 id=\"小批量梯度下降算法\"><a href=\"#小批量梯度下降算法\" class=\"headerlink\" title=\"小批量梯度下降算法\"></a>小批量梯度下降算法</h1><p>MBGD的算法思想就是在更新每一参数时都使用一部分样本来进行更新，也就是公式（1）中的m的值大于1小于所有样本的数量。</p>\n<p>相对于随机梯度下降，Mini-batch梯度下降降低了收敛波动性，即降低了参数更新的方差，使得更新更加稳定。相对于批量梯度下降，其提高了每次学习的速度。并且其不用担心内存瓶颈从而可以利用矩阵运算进行高效计算。一般而言每次更新随机选择[50,256]个样本进行学习，但是也要根据具体问题而选择，实践中可以进行多次试验，选择一个更新速度与更次次数都较适合的样本数。mini-batch梯度下降可以保证收敛性，常用于神经网络中。</p>\n<h1 id=\"补充\"><a href=\"#补充\" class=\"headerlink\" title=\"补充\"></a>补充</h1><p>在样本量较小的情况下，可以使用批量梯度下降算法，样本量较大的情况或者线上，可以使用随机梯度下降算法或者小批量梯度下降算法。</p>\n<p>在机器学习中的无约束优化算法，除了梯度下降以外，还有前面提到的最小二乘法，此外还有牛顿法和拟牛顿法。</p>\n<p>梯度下降法和最小二乘法相比，梯度下降法需要选择步长，而最小二乘法不需要。梯度下降法是迭代求解，最小二乘法是计算解析解。如果样本量不算很大，且存在解析解，最小二乘法比起梯度下降法要有优势，计算速度很快。但是如果样本量很大，用最小二乘法由于需要求一个超级大的逆矩阵，这时就很难或者很慢才能求解解析解了，使用迭代的梯度下降法比较有优势。</p>\n<p>梯度下降法和牛顿法/拟牛顿法相比，两者都是迭代求解，不过梯度下降法是梯度求解，而牛顿法/拟牛顿法是用二阶的海森矩阵的逆矩阵或伪逆矩阵求解。相对而言，使用牛顿法/拟牛顿法收敛更快。但是每次迭代的时间比梯度下降法长。</p>\n<h1 id=\"sklearn中的SGD\"><a href=\"#sklearn中的SGD\" class=\"headerlink\" title=\"sklearn中的SGD\"></a>sklearn中的SGD</h1><p>sklearn官网上查了一下，并没有找到BGD和MBGD的相关文档，只是看到可SGD的，感兴趣的可以直接去官网看英文文档，点击SGD查看：<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\" target=\"_blank\" rel=\"external\">SGD</a>，这也有一个中文的 <a href=\"http://sklearn.lzjqsdd.com/modules/sgd.html\" target=\"_blank\" rel=\"external\">SGD</a></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [1]: from sklearn.linear_model import SGDClassifier</span><br><span class=\"line\"></span><br><span class=\"line\">In [2]: X = [[0., 0.], [1., 1.]]</span><br><span class=\"line\"></span><br><span class=\"line\">In [3]: y = [0, 1]</span><br><span class=\"line\"></span><br><span class=\"line\">In [4]: clf = SGDClassifier(loss=&quot;hinge&quot;, penalty=&quot;l2&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">In [5]: clf.fit(X, y)</span><br><span class=\"line\">Out[5]: </span><br><span class=\"line\">SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,</span><br><span class=\"line\">       eta0=0.0, fit_intercept=True, l1_ratio=0.15,</span><br><span class=\"line\">       learning_rate=&apos;optimal&apos;, loss=&apos;hinge&apos;, n_iter=5, n_jobs=1,</span><br><span class=\"line\">       penalty=&apos;l2&apos;, power_t=0.5, random_state=None, shuffle=True,</span><br><span class=\"line\">       verbose=0, warm_start=False)</span><br><span class=\"line\"></span><br><span class=\"line\">In [6]:  clf.predict([[2., 2.]])</span><br><span class=\"line\">Out[6]: array([1])</span><br><span class=\"line\"></span><br><span class=\"line\">In [7]: clf.coef_ </span><br><span class=\"line\">Out[7]: array([[ 9.91080278,  9.91080278]])</span><br><span class=\"line\"></span><br><span class=\"line\">In [8]: clf.intercept_ </span><br><span class=\"line\">Out[8]: array([-9.97004991])</span><br></pre></td></tr></table></figure>\n<p>参考：</p>\n<ul>\n<li><a href=\"https://www.cnblogs.com/pinard/p/5970503.html\" target=\"_blank\" rel=\"external\">https://www.cnblogs.com/pinard/p/5970503.html</a></li>\n<li><a href=\"http://blog.csdn.net/uestc_c2_403/article/details/74910107\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/uestc_c2_403/article/details/74910107</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>在机器学习领域，体梯度下降算法分为三种</p>\n<ul>\n<li>批量梯度下降算法（BGD，Batch gradient descent algorithm）</li>\n<li>随机梯度下降算法（SGD，Stochastic gradient descent algorithm）</li>\n<li>小批量梯度下降算法（MBGD，Mini-batch gradient descent algorithm）</li>\n</ul>","more":"<h1 id=\"批量梯度下降算法\"><a href=\"#批量梯度下降算法\" class=\"headerlink\" title=\"批量梯度下降算法\"></a>批量梯度下降算法</h1><p>BGD是最原始的梯度下降算法，每一次迭代使用全部的样本，即权重的迭代公式中(公式中用$\\theta$代替$\\theta_i$)，</p>\n<script type=\"math/tex; mode=display\">\n\\jmath (\\theta _0,\\theta _1,...,\\theta _n)=\\sum_{i=0}^{m}( h_\\theta(x_0,x_1,...,x_n)-y_i )^2</script><script type=\"math/tex; mode=display\">\n\\theta _i = \\theta _i - \\alpha \\frac{\\partial \\jmath (\\theta _1,\\theta _2,...,\\theta _n)}{\\partial \\theta _i}</script><script type=\"math/tex; mode=display\">\n公式(1)</script><p>这里的m代表所有的样本，表示从第一个样本遍历到最后一个样本。</p>\n<p>特点：</p>\n<ul>\n<li>能达到全局最优解，易于并行实现</li>\n<li>当样本数目很多时，训练过程缓慢</li>\n</ul>\n<h1 id=\"随机梯度下降算法\"><a href=\"#随机梯度下降算法\" class=\"headerlink\" title=\"随机梯度下降算法\"></a>随机梯度下降算法</h1><p>SGD的思想是更新每一个参数时都使用一个样本来进行更新，即公式（1）中m为1。每次更新参数都只使用一个样本，进行多次更新。这样在样本量很大的情况下，可能只用到其中的一部分样本就能得到最优解了。<br>但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。</p>\n<p>特点：</p>\n<ul>\n<li>训练速度快</li>\n<li>准确度下降，并不是最优解，不易于并行实现</li>\n</ul>\n<h1 id=\"小批量梯度下降算法\"><a href=\"#小批量梯度下降算法\" class=\"headerlink\" title=\"小批量梯度下降算法\"></a>小批量梯度下降算法</h1><p>MBGD的算法思想就是在更新每一参数时都使用一部分样本来进行更新，也就是公式（1）中的m的值大于1小于所有样本的数量。</p>\n<p>相对于随机梯度下降，Mini-batch梯度下降降低了收敛波动性，即降低了参数更新的方差，使得更新更加稳定。相对于批量梯度下降，其提高了每次学习的速度。并且其不用担心内存瓶颈从而可以利用矩阵运算进行高效计算。一般而言每次更新随机选择[50,256]个样本进行学习，但是也要根据具体问题而选择，实践中可以进行多次试验，选择一个更新速度与更次次数都较适合的样本数。mini-batch梯度下降可以保证收敛性，常用于神经网络中。</p>\n<h1 id=\"补充\"><a href=\"#补充\" class=\"headerlink\" title=\"补充\"></a>补充</h1><p>在样本量较小的情况下，可以使用批量梯度下降算法，样本量较大的情况或者线上，可以使用随机梯度下降算法或者小批量梯度下降算法。</p>\n<p>在机器学习中的无约束优化算法，除了梯度下降以外，还有前面提到的最小二乘法，此外还有牛顿法和拟牛顿法。</p>\n<p>梯度下降法和最小二乘法相比，梯度下降法需要选择步长，而最小二乘法不需要。梯度下降法是迭代求解，最小二乘法是计算解析解。如果样本量不算很大，且存在解析解，最小二乘法比起梯度下降法要有优势，计算速度很快。但是如果样本量很大，用最小二乘法由于需要求一个超级大的逆矩阵，这时就很难或者很慢才能求解解析解了，使用迭代的梯度下降法比较有优势。</p>\n<p>梯度下降法和牛顿法/拟牛顿法相比，两者都是迭代求解，不过梯度下降法是梯度求解，而牛顿法/拟牛顿法是用二阶的海森矩阵的逆矩阵或伪逆矩阵求解。相对而言，使用牛顿法/拟牛顿法收敛更快。但是每次迭代的时间比梯度下降法长。</p>\n<h1 id=\"sklearn中的SGD\"><a href=\"#sklearn中的SGD\" class=\"headerlink\" title=\"sklearn中的SGD\"></a>sklearn中的SGD</h1><p>sklearn官网上查了一下，并没有找到BGD和MBGD的相关文档，只是看到可SGD的，感兴趣的可以直接去官网看英文文档，点击SGD查看：<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\" target=\"_blank\" rel=\"external\">SGD</a>，这也有一个中文的 <a href=\"http://sklearn.lzjqsdd.com/modules/sgd.html\" target=\"_blank\" rel=\"external\">SGD</a></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [1]: from sklearn.linear_model import SGDClassifier</span><br><span class=\"line\"></span><br><span class=\"line\">In [2]: X = [[0., 0.], [1., 1.]]</span><br><span class=\"line\"></span><br><span class=\"line\">In [3]: y = [0, 1]</span><br><span class=\"line\"></span><br><span class=\"line\">In [4]: clf = SGDClassifier(loss=&quot;hinge&quot;, penalty=&quot;l2&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">In [5]: clf.fit(X, y)</span><br><span class=\"line\">Out[5]: </span><br><span class=\"line\">SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,</span><br><span class=\"line\">       eta0=0.0, fit_intercept=True, l1_ratio=0.15,</span><br><span class=\"line\">       learning_rate=&apos;optimal&apos;, loss=&apos;hinge&apos;, n_iter=5, n_jobs=1,</span><br><span class=\"line\">       penalty=&apos;l2&apos;, power_t=0.5, random_state=None, shuffle=True,</span><br><span class=\"line\">       verbose=0, warm_start=False)</span><br><span class=\"line\"></span><br><span class=\"line\">In [6]:  clf.predict([[2., 2.]])</span><br><span class=\"line\">Out[6]: array([1])</span><br><span class=\"line\"></span><br><span class=\"line\">In [7]: clf.coef_ </span><br><span class=\"line\">Out[7]: array([[ 9.91080278,  9.91080278]])</span><br><span class=\"line\"></span><br><span class=\"line\">In [8]: clf.intercept_ </span><br><span class=\"line\">Out[8]: array([-9.97004991])</span><br></pre></td></tr></table></figure>\n<p>参考：</p>\n<ul>\n<li><a href=\"https://www.cnblogs.com/pinard/p/5970503.html\" target=\"_blank\" rel=\"external\">https://www.cnblogs.com/pinard/p/5970503.html</a></li>\n<li><a href=\"http://blog.csdn.net/uestc_c2_403/article/details/74910107\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/uestc_c2_403/article/details/74910107</a></li>\n</ul>"},{"title":"梯度算法之梯度上升和梯度下降","date":"2017-12-14T06:11:11.000Z","_content":"\n第一次看见随机梯度上升算法是看《机器学习实战》这本书，当时也是一知半解，只是大概知道和高等数学中的函数求导有一定的关系。下边我们就好好研究下随机梯度上升（下降）和梯度上升（下降）。\n<!--More-->\n# 高数中的导数\n设导数 y = f(x) 在 $ x_0 $的某个邻域内有定义，当自变量从 $ x_0 $ 变成\n$$\nx_{0} + \\Delta x\n$$\n函数y=f(x)的增量\n\n$$\n\\Delta y = f(x_0 + \\Delta x) - f(x_0)\n$$\n与自变量的增量 $ \\Delta x $ 之比：\n\n\n$$\n\\frac{ \\Delta y }{ \\Delta x } = \\frac{ f(x_0 + \\Delta x)-f(x_0) }{ \\Delta x }\n$$\n称为f(x)的平均变化率。\n如 $ \\Delta x \\rightarrow 0 $ 平均变化率的极限\n$$\n\\lim_{\\Delta x \\rightarrow 0} \\frac{ \\Delta y }{ \\Delta x } = \\lim_{\\Delta x  \\rightarrow 0} \\frac{ f(x_0 + \\Delta x)-f(x_0) }{ \\Delta x }\n$$\n存在，则称极限值为f(x)在$ x_0 $ 处的导数，并说f(x)在$ x_0 $ 处可导或有导数。当平均变化率极限不存在时，就说f(x)在 $ x_0 $ 处不可导或没有导数。\n\n关于导数的说明\n\n1）点导数是因变量在$ x_0 $ 处的变化率，它反映了因变量随自变量的变化而变化的快慢成都\n\n2）如果函数y = f(x)在开区间 I 内的每点都可导，就称f(x)在开区间 I 内可导\n\n3）对于任一 x 属于 I ，都对应着函数f(x)的一个导数，这个函数叫做原来函数f(x)的导函数\n\n4）导函数在x1 处 为 0，若 x<1 时，f'(x) > 0 ，这 f(x) 递增，若f'(x)<0 ，f(x)递减\n\n5）f'(x0) 表示曲线y=f(x)在点 （x0,f($x_0$)）处的切线斜率\n\n\n# 偏导数\n\n函数z=f(x,y)在点(x0,y0)的某一邻域内有定义，当y固定在y0而x在 $x_0$ 处有增量$ \\Delta x $ 时，相应的有函数增量\n$$\nf(x_0 + \\Delta x, y_0) - f(x_0,y_0)\n$$\n如果\n$$\n\\lim_{\\Delta x\\rightarrow 0 } \\frac {f(x_0 + \\Delta x, y_0) - f(x_0,y_0)}{\\Delta x}\n$$\n存在，则称z=f(x,y)在点($x_0$,$y_0$)处对x的偏导数，记为：$ f_x(x_0,y_0) $\n\n\n如果函数z=f(x,y)在区域D内任一点(x,y)处对x的偏导数都存在，那么这个偏导数就是x,y的函数，它就称为函数z=f(x,y)对自变量x的偏导数，记做\n$$\n\\frac{ \\partial z }{ \\partial x } , \\frac{ \\partial f }{ \\partial x } , z_x , f_x(x,y), \n$$\n\n偏导数的概念可以推广到二元以上的函数，如 u = f(x,y,z)在x,y,z处\n$$\nf_x(x,y,z)=\\lim_{\\Delta x \\rightarrow 0} \\frac{f(x + \\Delta x,y,z) -f(x,y,z)}{\\Delta x}\n$$\n$$\nf_y(x,y,z)=\\lim_{\\Delta y \\rightarrow 0} \\frac{f(x,y + \\Delta y,z) -f(x,y,z)}{\\Delta y}\n$$\n$$\nf_z(x,y,z)=\\lim_{\\Delta z \\rightarrow 0} \\frac{f(x,y,z + \\Delta z) -f(x,y,z)}{\\Delta z}\n$$\n可以看出导数与偏导数本质是一致的，都是自变量趋近于0时，函数值的变化与自变量的变化量比值的极限，直观的说，偏导数也就是函数在某一点沿坐标轴正方向的变化率。\n\n区别：\n导数指的是一元函数中，函数y=f(x)某一点沿x轴正方向的的变化率；\n偏导数指的是多元函数中，函数y=f(x,y,z)在某一点沿某一坐标轴正方向的变化率。\n\n偏导数的几何意义：\n偏导数$ z = f_x(x_0,y_0)$表示的是曲面被 $ y=y_0 $ 所截得的曲线在点M处的切线$ M_0T_x $对x轴的斜率\n偏导数$ z = f_y(x_0,y_0)$表示的是曲面被 $ x=x_0 $ 所截得的曲线在点M处的切线$ M_0T_y $对y轴的斜率\n\n例子：\n求 $z = x^2 + 3 xy+y^2 $在点(1,2)处的偏导数。\n$$\n\\frac{ \\partial z}{\\partial x} = 2x +3y\n$$\n$$\n\\frac{ \\partial z}{\\partial y} = 2y +3x\n$$\n所以:\n$z_x(x=1,y=2) = 8$  \n$z_y(x=1,y=2) = 7$\n\n# 方向导数\n$$\n\\frac{ \\partial }{ \\partial l }  f(x_0,x_1,...,x_n) = \\lim_{\\rho \\rightarrow 0} \\frac{\\Delta y}{ \\Delta x } = \\lim_{\\rho \\rightarrow 0} \\frac{ f(x_0 + \\Delta x_0,...,x_j + \\Delta x_j,...,x_n + \\Delta x_n)-f(x_0,...,x_j,...,x_n)}{ \\rho }\n$$\n$$\n\\rho = \\sqrt{ (\\Delta x_0)^{2} +...+(\\Delta x_j)^{2}+...+(\\Delta x_n)^{2}}\n$$\n前边导数和偏导数的定义中，均是沿坐标轴正方向讨论函数的变化率。那么当讨论函数沿任意方向的变化率时，也就引出了方向导数的定义，即：某一点在某一趋近方向上的导数值。\n\n通俗的解释是： 我们不仅要知道函数在坐标轴正方向上的变化率（即偏导数），而且还要设法求得函数在其他特定方向上的变化率。而方向导数就是函数在其他特定方向上的变化率。 \n　\n\n# 梯度\n与方向导数有一定的关联，在微积分里面，对多元函数的参数求 $ \\partial  $ 偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。比如函数f(x,y), 分别对x,y求偏导数，求得的梯度向量就是 $\n( \\frac{ \\partial f }{ \\partial x },\\frac{ \\partial f }{ \\partial y })^T\n$ ,简称grad f(x,y)或者 $▽f(x,y)$。对于在点$(x_0,y_0)$的具体梯度向量就是$( \\frac{ \\partial f }{ \\partial x_0 },\\frac{ \\partial f }{ \\partial y_0 })^T$.或者$▽f(x_0,y_0)$，如果是3个参数的向量梯度，就是 $( \\frac{ \\partial f }{ \\partial x },\\frac{ \\partial f }{ \\partial y },\\frac{ \\partial f }{ \\partial z })^T$,以此类推。\n\n那么这个梯度向量求出来有什么意义呢？他的意义从几何意义上讲，就是函数变化增加最快的地方。具体来说，对于函数f(x,y),在点$(x_0,y_0)$，沿着梯度向量的方向就是$( \\frac{ \\partial f }{ \\partial x_0 },\\frac{ \\partial f }{ \\partial y_0 })^T$的方向是f(x,y)增加最快的地方。或者说，沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向，也就是 $-( \\frac{ \\partial f }{ \\partial x_0 },\\frac{ \\partial f }{ \\partial y_0 })^T$的方向，梯度减少最快，也就是更加容易找到函数的最小值。\n\n例如：\n函数 $f(x,y) = \\frac{1}{x^2+y^2} $ ，分别对x，y求偏导数得：\n$$\n \\frac{ \\partial f }{ \\partial x}=-\\frac{2x}{ (x^2+y^2)^2}\n$$\n$$\n \\frac{ \\partial f }{ \\partial y}=-\\frac{2y}{ (x^2+y^2)^2}\n$$\n所以\n$$\ngrad( \\frac{1}{x^2+y^2} ) = (-\\frac{2x}{ (x^2+y^2)^2} ,-\\frac{2y}{ (x^2+y^2)^2})\n$$\n函数在某一点的梯度是这样一个向量，它的方向与取得最大方向导数的方向一致，而它的模为方向导数的最大值。 \n\n\n注意点：\n1）梯度是一个向量\n2）梯度的方向是最大方向导数的方向\n3）梯度的值是最大方向导数的值\n\n# 梯度下降与梯度上升\n在机器学习算法中，在最小化损失函数时，可以通过梯度下降思想来求得最小化的损失函数和对应的参数值，反过来，如果要求最大化的损失函数，可以通过梯度上升思想来求取。\n\n\n\n## 梯度下降\n### 关于梯度下降的几个概念\n1）步长（learning rate）：步长决定了在梯度下降迭代过程中，每一步沿梯度负方向前进的长度\n2）特征（feature）：指的是样本中输入部门，比如样本（x0，y0），（x1，y1），则样本特征为x，样本输出为y\n3）假设函数（hypothesis function）：在监督学习中，为了拟合输入样本，而使用的假设函数，记为$h_θ(x)$。比如对于样本$（x_i,y_i）(i=1,2,...n)$,可以采用拟合函数如下： $h_θ(x) = θ0+θ1_x$。\n4）损失函数（loss function）：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小化，意味着拟合程度最好，对应的模型参数即为最优参数。在线性回归中，损失函数通常为样本输出和假设函数的差取平方。比如对于样本（xi,yi）(i=1,2,...n),采用线性回归，损失函数为：\n$$\n\\jmath (\\theta _0,\\theta _1)=\\sum_{i=0}^{m}( h_\\theta(x_i)-y_i )^2\n$$\n其中$x_i$表示样本特征x的第i个元素，$y_i$表示样本输出y的第i个元素，$h_\\theta(x_i)$ 为假设函数。\n\n### 梯度下降的代数方法描述\n\n 1. 先决条件：确定优化模型的假设函数和损失函数\n    这里假定线性回归的假设函数为$h_\\theta(x_1,x_2,...x_n)=\\theta_0+\\theta_1x_1+...+\\theta_nx_n$，其中 $\\theta _i(i=0,1,2...n)$ 为模型参数(公式中用$\\theta$代替)，$x_i(i=0,1,2...n)$为每个样本的n个特征值。\n    \n    则对应选定得损失函数为：\n    $$\n    \\jmath (\\theta _0,\\theta _1,...,,\\theta _n)=\\sum_{i=0}^{m}( h_\\theta(x_0,x_1,...,x_n)-y_i )^2\n    $$\n\n 2. 算法相关参数的初始化\n    主要是初始化 $ \\theta _0,\\theta _1...,\\theta _n$，算法终止距离 $\\varepsilon $ 以及步长 $ \\alpha $。在没有任何先验知识的时候，我喜欢将所有的 $\\theta$ 初始化为0， 将步长初始化为1。在调优的时候再优化。\n \n 3. 算法过程\n \n - 1)：确定当前损失函数的梯度，对于$\\theta _i $，其梯度表达式为：\n $$\n\\frac{\\partial }{\\partial \\theta _i}\\jmath (\\theta _1,\\theta _2,...,\\theta _n)\n $$\n \n - 2)：用步长乘以损失函数的梯度，得到当前位置的下降距离，即\n $$\n \\alpha \\frac{\\partial \\jmath (\\theta _1,\\theta _2,...,\\theta _n)}{\\partial \\theta _i}\n $$\n \n - 3)：确定是否所有的$\\theta _i$ ，梯度下降的距离都小于 $ \\varepsilon $，如果小于$ \\varepsilon $，则算法停止，当前所有的 $\\theta _i(i=1,2,3,...,n)$ 即为最终结果。否则执行下一步。\n \n - 4)：更新所有的 $\\theta$，对于$\\theta _i $，其更新表达式如下。更新完毕后继续转入步骤1)。\n $$\n\\theta _i = \\theta _i - \\alpha \\frac{\\partial \\jmath (\\theta _1,\\theta _2,...,\\theta _n)}{\\partial \\theta _i}\n $$\n \n ### 梯度下降的矩阵方式描述\n  \n  1. 先决条件：确定优化模型的假设函数和损失函数\n    这里假定线性回归的假设函数为$h_\\theta(x_1,x_2,...x_n)=\\theta_0+\\theta_1x_1+...+\\theta_nx_n$，其中 $\\theta _i(i=0,1,2...n)$ 为模型参数，$x_i(i=0,1,2...n)$为每个样本的n个特征值。\n    假设函数对应的矩阵表示为：$ h_\\theta (x) = X \\theta $，假设函数 $h_\\theta(x)$ 为mx1的向量，$\\theta $ 为nx1的向量，里面有n个代数法的模型参数。X为mxn维的矩阵。m代表样本的个数，n代表样本的特征数。\n    则对应选定得损失函数为：\n    $$\n    \\jmath (\\theta)=(X \\theta −Y)^T (X \\theta−Y)\n    $$\n 其中YY是样本的输出向量，维度为m*1\n <br>\n 2.算法相关参数初始化:  \n  $\\theta$ 向量可以初始化为默认值，或者调优后的值。算法终止距离 $\\varepsilon $ ，步长 $\\alpha$ 和 “梯度下降的代数方法”描述中一致。\n <br>\n 3.算法过程\n  \n - 1)：确定当前位置的损失函数的梯度，对于 $ \\theta $ 向量,其梯度表达式如下：\n$$\n\\frac{ \\partial }{\\partial \\theta } \\jmath (\\theta)\n$$\n - 2)：用步长乘以损失函数的梯度，得到当前位置下降的距离，即 $\\alpha \\frac{ \\partial }{\\partial \\theta } \\jmath (\\theta)$ \n - 3)：确定 $\\theta$ 向量里面的每个值,梯度下降的距离都小于 $\\varepsilon$，如果小于 $\\varepsilon$ 则算法终止，当前 $\\theta$ 向量即为最终结果。否则进入步骤4)\n - 4)：更新 $\\theta$ 向量，其更新表达式如下。更新完毕后继续转入步骤1)\n $$\n \\theta =\\theta - \\alpha \\frac{ \\partial }{\\partial \\theta } \\jmath (\\theta)\n $$\n \n## 梯度上升\n梯度上升和梯度下降的分析方式是一致的，只不过把 $ \\theta $ 的更新中 减号变为加号。\n\n## 梯度下降的算法优化\n1. 算法的步长选择。在前面的算法描述中，我提到取步长为1，但是实际上取值取决于数据样本，可以多取一些值，从大到小，分别运行算法，看看迭代效果，如果损失函数在变小，说明取值有效，否则要增大步长。前面说了。步长太大，会导致迭代过快，甚至有可能错过最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较为优的值。\n\n2. 算法参数的初始值选择。 初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；当然如果损失函数是凸函数则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。\n\n3.归一化。由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据归一化，也就是对于每个特征x，求出它的均值 $\\bar{x}$ 和标准差std(x)，然后转化为：\n$$\n\\frac{x - \\bar{x}}{std(x)}\n$$\n这样特征的新期望为0，新方差为1，迭代次数可以大大加快。\n \n \n \n \n \n \n \n \n\n\n----------\n\n\nhttp://blog.csdn.net/walilk/article/details/50978864\n\nhttps://www.zhihu.com/question/24658302\n\nhttps://www.cnblogs.com/pinard/p/5970503.html\n\nhttp://www.doc88.com/p-7844239247737.html\n","source":"_posts/机器学习/梯度算法之梯度上升和梯度下降.md","raw":"---\ntitle: 梯度算法之梯度上升和梯度下降\ndate: 2017-12-14 14:11:11\ntags: [梯度下降]\ncategories: 技术篇\n---\n\n第一次看见随机梯度上升算法是看《机器学习实战》这本书，当时也是一知半解，只是大概知道和高等数学中的函数求导有一定的关系。下边我们就好好研究下随机梯度上升（下降）和梯度上升（下降）。\n<!--More-->\n# 高数中的导数\n设导数 y = f(x) 在 $ x_0 $的某个邻域内有定义，当自变量从 $ x_0 $ 变成\n$$\nx_{0} + \\Delta x\n$$\n函数y=f(x)的增量\n\n$$\n\\Delta y = f(x_0 + \\Delta x) - f(x_0)\n$$\n与自变量的增量 $ \\Delta x $ 之比：\n\n\n$$\n\\frac{ \\Delta y }{ \\Delta x } = \\frac{ f(x_0 + \\Delta x)-f(x_0) }{ \\Delta x }\n$$\n称为f(x)的平均变化率。\n如 $ \\Delta x \\rightarrow 0 $ 平均变化率的极限\n$$\n\\lim_{\\Delta x \\rightarrow 0} \\frac{ \\Delta y }{ \\Delta x } = \\lim_{\\Delta x  \\rightarrow 0} \\frac{ f(x_0 + \\Delta x)-f(x_0) }{ \\Delta x }\n$$\n存在，则称极限值为f(x)在$ x_0 $ 处的导数，并说f(x)在$ x_0 $ 处可导或有导数。当平均变化率极限不存在时，就说f(x)在 $ x_0 $ 处不可导或没有导数。\n\n关于导数的说明\n\n1）点导数是因变量在$ x_0 $ 处的变化率，它反映了因变量随自变量的变化而变化的快慢成都\n\n2）如果函数y = f(x)在开区间 I 内的每点都可导，就称f(x)在开区间 I 内可导\n\n3）对于任一 x 属于 I ，都对应着函数f(x)的一个导数，这个函数叫做原来函数f(x)的导函数\n\n4）导函数在x1 处 为 0，若 x<1 时，f'(x) > 0 ，这 f(x) 递增，若f'(x)<0 ，f(x)递减\n\n5）f'(x0) 表示曲线y=f(x)在点 （x0,f($x_0$)）处的切线斜率\n\n\n# 偏导数\n\n函数z=f(x,y)在点(x0,y0)的某一邻域内有定义，当y固定在y0而x在 $x_0$ 处有增量$ \\Delta x $ 时，相应的有函数增量\n$$\nf(x_0 + \\Delta x, y_0) - f(x_0,y_0)\n$$\n如果\n$$\n\\lim_{\\Delta x\\rightarrow 0 } \\frac {f(x_0 + \\Delta x, y_0) - f(x_0,y_0)}{\\Delta x}\n$$\n存在，则称z=f(x,y)在点($x_0$,$y_0$)处对x的偏导数，记为：$ f_x(x_0,y_0) $\n\n\n如果函数z=f(x,y)在区域D内任一点(x,y)处对x的偏导数都存在，那么这个偏导数就是x,y的函数，它就称为函数z=f(x,y)对自变量x的偏导数，记做\n$$\n\\frac{ \\partial z }{ \\partial x } , \\frac{ \\partial f }{ \\partial x } , z_x , f_x(x,y), \n$$\n\n偏导数的概念可以推广到二元以上的函数，如 u = f(x,y,z)在x,y,z处\n$$\nf_x(x,y,z)=\\lim_{\\Delta x \\rightarrow 0} \\frac{f(x + \\Delta x,y,z) -f(x,y,z)}{\\Delta x}\n$$\n$$\nf_y(x,y,z)=\\lim_{\\Delta y \\rightarrow 0} \\frac{f(x,y + \\Delta y,z) -f(x,y,z)}{\\Delta y}\n$$\n$$\nf_z(x,y,z)=\\lim_{\\Delta z \\rightarrow 0} \\frac{f(x,y,z + \\Delta z) -f(x,y,z)}{\\Delta z}\n$$\n可以看出导数与偏导数本质是一致的，都是自变量趋近于0时，函数值的变化与自变量的变化量比值的极限，直观的说，偏导数也就是函数在某一点沿坐标轴正方向的变化率。\n\n区别：\n导数指的是一元函数中，函数y=f(x)某一点沿x轴正方向的的变化率；\n偏导数指的是多元函数中，函数y=f(x,y,z)在某一点沿某一坐标轴正方向的变化率。\n\n偏导数的几何意义：\n偏导数$ z = f_x(x_0,y_0)$表示的是曲面被 $ y=y_0 $ 所截得的曲线在点M处的切线$ M_0T_x $对x轴的斜率\n偏导数$ z = f_y(x_0,y_0)$表示的是曲面被 $ x=x_0 $ 所截得的曲线在点M处的切线$ M_0T_y $对y轴的斜率\n\n例子：\n求 $z = x^2 + 3 xy+y^2 $在点(1,2)处的偏导数。\n$$\n\\frac{ \\partial z}{\\partial x} = 2x +3y\n$$\n$$\n\\frac{ \\partial z}{\\partial y} = 2y +3x\n$$\n所以:\n$z_x(x=1,y=2) = 8$  \n$z_y(x=1,y=2) = 7$\n\n# 方向导数\n$$\n\\frac{ \\partial }{ \\partial l }  f(x_0,x_1,...,x_n) = \\lim_{\\rho \\rightarrow 0} \\frac{\\Delta y}{ \\Delta x } = \\lim_{\\rho \\rightarrow 0} \\frac{ f(x_0 + \\Delta x_0,...,x_j + \\Delta x_j,...,x_n + \\Delta x_n)-f(x_0,...,x_j,...,x_n)}{ \\rho }\n$$\n$$\n\\rho = \\sqrt{ (\\Delta x_0)^{2} +...+(\\Delta x_j)^{2}+...+(\\Delta x_n)^{2}}\n$$\n前边导数和偏导数的定义中，均是沿坐标轴正方向讨论函数的变化率。那么当讨论函数沿任意方向的变化率时，也就引出了方向导数的定义，即：某一点在某一趋近方向上的导数值。\n\n通俗的解释是： 我们不仅要知道函数在坐标轴正方向上的变化率（即偏导数），而且还要设法求得函数在其他特定方向上的变化率。而方向导数就是函数在其他特定方向上的变化率。 \n　\n\n# 梯度\n与方向导数有一定的关联，在微积分里面，对多元函数的参数求 $ \\partial  $ 偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。比如函数f(x,y), 分别对x,y求偏导数，求得的梯度向量就是 $\n( \\frac{ \\partial f }{ \\partial x },\\frac{ \\partial f }{ \\partial y })^T\n$ ,简称grad f(x,y)或者 $▽f(x,y)$。对于在点$(x_0,y_0)$的具体梯度向量就是$( \\frac{ \\partial f }{ \\partial x_0 },\\frac{ \\partial f }{ \\partial y_0 })^T$.或者$▽f(x_0,y_0)$，如果是3个参数的向量梯度，就是 $( \\frac{ \\partial f }{ \\partial x },\\frac{ \\partial f }{ \\partial y },\\frac{ \\partial f }{ \\partial z })^T$,以此类推。\n\n那么这个梯度向量求出来有什么意义呢？他的意义从几何意义上讲，就是函数变化增加最快的地方。具体来说，对于函数f(x,y),在点$(x_0,y_0)$，沿着梯度向量的方向就是$( \\frac{ \\partial f }{ \\partial x_0 },\\frac{ \\partial f }{ \\partial y_0 })^T$的方向是f(x,y)增加最快的地方。或者说，沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向，也就是 $-( \\frac{ \\partial f }{ \\partial x_0 },\\frac{ \\partial f }{ \\partial y_0 })^T$的方向，梯度减少最快，也就是更加容易找到函数的最小值。\n\n例如：\n函数 $f(x,y) = \\frac{1}{x^2+y^2} $ ，分别对x，y求偏导数得：\n$$\n \\frac{ \\partial f }{ \\partial x}=-\\frac{2x}{ (x^2+y^2)^2}\n$$\n$$\n \\frac{ \\partial f }{ \\partial y}=-\\frac{2y}{ (x^2+y^2)^2}\n$$\n所以\n$$\ngrad( \\frac{1}{x^2+y^2} ) = (-\\frac{2x}{ (x^2+y^2)^2} ,-\\frac{2y}{ (x^2+y^2)^2})\n$$\n函数在某一点的梯度是这样一个向量，它的方向与取得最大方向导数的方向一致，而它的模为方向导数的最大值。 \n\n\n注意点：\n1）梯度是一个向量\n2）梯度的方向是最大方向导数的方向\n3）梯度的值是最大方向导数的值\n\n# 梯度下降与梯度上升\n在机器学习算法中，在最小化损失函数时，可以通过梯度下降思想来求得最小化的损失函数和对应的参数值，反过来，如果要求最大化的损失函数，可以通过梯度上升思想来求取。\n\n\n\n## 梯度下降\n### 关于梯度下降的几个概念\n1）步长（learning rate）：步长决定了在梯度下降迭代过程中，每一步沿梯度负方向前进的长度\n2）特征（feature）：指的是样本中输入部门，比如样本（x0，y0），（x1，y1），则样本特征为x，样本输出为y\n3）假设函数（hypothesis function）：在监督学习中，为了拟合输入样本，而使用的假设函数，记为$h_θ(x)$。比如对于样本$（x_i,y_i）(i=1,2,...n)$,可以采用拟合函数如下： $h_θ(x) = θ0+θ1_x$。\n4）损失函数（loss function）：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小化，意味着拟合程度最好，对应的模型参数即为最优参数。在线性回归中，损失函数通常为样本输出和假设函数的差取平方。比如对于样本（xi,yi）(i=1,2,...n),采用线性回归，损失函数为：\n$$\n\\jmath (\\theta _0,\\theta _1)=\\sum_{i=0}^{m}( h_\\theta(x_i)-y_i )^2\n$$\n其中$x_i$表示样本特征x的第i个元素，$y_i$表示样本输出y的第i个元素，$h_\\theta(x_i)$ 为假设函数。\n\n### 梯度下降的代数方法描述\n\n 1. 先决条件：确定优化模型的假设函数和损失函数\n    这里假定线性回归的假设函数为$h_\\theta(x_1,x_2,...x_n)=\\theta_0+\\theta_1x_1+...+\\theta_nx_n$，其中 $\\theta _i(i=0,1,2...n)$ 为模型参数(公式中用$\\theta$代替)，$x_i(i=0,1,2...n)$为每个样本的n个特征值。\n    \n    则对应选定得损失函数为：\n    $$\n    \\jmath (\\theta _0,\\theta _1,...,,\\theta _n)=\\sum_{i=0}^{m}( h_\\theta(x_0,x_1,...,x_n)-y_i )^2\n    $$\n\n 2. 算法相关参数的初始化\n    主要是初始化 $ \\theta _0,\\theta _1...,\\theta _n$，算法终止距离 $\\varepsilon $ 以及步长 $ \\alpha $。在没有任何先验知识的时候，我喜欢将所有的 $\\theta$ 初始化为0， 将步长初始化为1。在调优的时候再优化。\n \n 3. 算法过程\n \n - 1)：确定当前损失函数的梯度，对于$\\theta _i $，其梯度表达式为：\n $$\n\\frac{\\partial }{\\partial \\theta _i}\\jmath (\\theta _1,\\theta _2,...,\\theta _n)\n $$\n \n - 2)：用步长乘以损失函数的梯度，得到当前位置的下降距离，即\n $$\n \\alpha \\frac{\\partial \\jmath (\\theta _1,\\theta _2,...,\\theta _n)}{\\partial \\theta _i}\n $$\n \n - 3)：确定是否所有的$\\theta _i$ ，梯度下降的距离都小于 $ \\varepsilon $，如果小于$ \\varepsilon $，则算法停止，当前所有的 $\\theta _i(i=1,2,3,...,n)$ 即为最终结果。否则执行下一步。\n \n - 4)：更新所有的 $\\theta$，对于$\\theta _i $，其更新表达式如下。更新完毕后继续转入步骤1)。\n $$\n\\theta _i = \\theta _i - \\alpha \\frac{\\partial \\jmath (\\theta _1,\\theta _2,...,\\theta _n)}{\\partial \\theta _i}\n $$\n \n ### 梯度下降的矩阵方式描述\n  \n  1. 先决条件：确定优化模型的假设函数和损失函数\n    这里假定线性回归的假设函数为$h_\\theta(x_1,x_2,...x_n)=\\theta_0+\\theta_1x_1+...+\\theta_nx_n$，其中 $\\theta _i(i=0,1,2...n)$ 为模型参数，$x_i(i=0,1,2...n)$为每个样本的n个特征值。\n    假设函数对应的矩阵表示为：$ h_\\theta (x) = X \\theta $，假设函数 $h_\\theta(x)$ 为mx1的向量，$\\theta $ 为nx1的向量，里面有n个代数法的模型参数。X为mxn维的矩阵。m代表样本的个数，n代表样本的特征数。\n    则对应选定得损失函数为：\n    $$\n    \\jmath (\\theta)=(X \\theta −Y)^T (X \\theta−Y)\n    $$\n 其中YY是样本的输出向量，维度为m*1\n <br>\n 2.算法相关参数初始化:  \n  $\\theta$ 向量可以初始化为默认值，或者调优后的值。算法终止距离 $\\varepsilon $ ，步长 $\\alpha$ 和 “梯度下降的代数方法”描述中一致。\n <br>\n 3.算法过程\n  \n - 1)：确定当前位置的损失函数的梯度，对于 $ \\theta $ 向量,其梯度表达式如下：\n$$\n\\frac{ \\partial }{\\partial \\theta } \\jmath (\\theta)\n$$\n - 2)：用步长乘以损失函数的梯度，得到当前位置下降的距离，即 $\\alpha \\frac{ \\partial }{\\partial \\theta } \\jmath (\\theta)$ \n - 3)：确定 $\\theta$ 向量里面的每个值,梯度下降的距离都小于 $\\varepsilon$，如果小于 $\\varepsilon$ 则算法终止，当前 $\\theta$ 向量即为最终结果。否则进入步骤4)\n - 4)：更新 $\\theta$ 向量，其更新表达式如下。更新完毕后继续转入步骤1)\n $$\n \\theta =\\theta - \\alpha \\frac{ \\partial }{\\partial \\theta } \\jmath (\\theta)\n $$\n \n## 梯度上升\n梯度上升和梯度下降的分析方式是一致的，只不过把 $ \\theta $ 的更新中 减号变为加号。\n\n## 梯度下降的算法优化\n1. 算法的步长选择。在前面的算法描述中，我提到取步长为1，但是实际上取值取决于数据样本，可以多取一些值，从大到小，分别运行算法，看看迭代效果，如果损失函数在变小，说明取值有效，否则要增大步长。前面说了。步长太大，会导致迭代过快，甚至有可能错过最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较为优的值。\n\n2. 算法参数的初始值选择。 初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；当然如果损失函数是凸函数则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。\n\n3.归一化。由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据归一化，也就是对于每个特征x，求出它的均值 $\\bar{x}$ 和标准差std(x)，然后转化为：\n$$\n\\frac{x - \\bar{x}}{std(x)}\n$$\n这样特征的新期望为0，新方差为1，迭代次数可以大大加快。\n \n \n \n \n \n \n \n \n\n\n----------\n\n\nhttp://blog.csdn.net/walilk/article/details/50978864\n\nhttps://www.zhihu.com/question/24658302\n\nhttps://www.cnblogs.com/pinard/p/5970503.html\n\nhttp://www.doc88.com/p-7844239247737.html\n","slug":"机器学习/梯度算法之梯度上升和梯度下降","published":1,"updated":"2017-12-19T02:54:56.791Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjbd1r26o0018kxuw488o6vxs","content":"<p>第一次看见随机梯度上升算法是看《机器学习实战》这本书，当时也是一知半解，只是大概知道和高等数学中的函数求导有一定的关系。下边我们就好好研究下随机梯度上升（下降）和梯度上升（下降）。<br><a id=\"more\"></a></p>\n<h1 id=\"高数中的导数\"><a href=\"#高数中的导数\" class=\"headerlink\" title=\"高数中的导数\"></a>高数中的导数</h1><p>设导数 y = f(x) 在 $ x_0 $的某个邻域内有定义，当自变量从 $ x_0 $ 变成</p>\n<script type=\"math/tex; mode=display\">\nx_{0} + \\Delta x</script><p>函数y=f(x)的增量</p>\n<script type=\"math/tex; mode=display\">\n\\Delta y = f(x_0 + \\Delta x) - f(x_0)</script><p>与自变量的增量 $ \\Delta x $ 之比：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{ \\Delta y }{ \\Delta x } = \\frac{ f(x_0 + \\Delta x)-f(x_0) }{ \\Delta x }</script><p>称为f(x)的平均变化率。<br>如 $ \\Delta x \\rightarrow 0 $ 平均变化率的极限</p>\n<script type=\"math/tex; mode=display\">\n\\lim_{\\Delta x \\rightarrow 0} \\frac{ \\Delta y }{ \\Delta x } = \\lim_{\\Delta x  \\rightarrow 0} \\frac{ f(x_0 + \\Delta x)-f(x_0) }{ \\Delta x }</script><p>存在，则称极限值为f(x)在$ x_0 $ 处的导数，并说f(x)在$ x_0 $ 处可导或有导数。当平均变化率极限不存在时，就说f(x)在 $ x_0 $ 处不可导或没有导数。</p>\n<p>关于导数的说明</p>\n<p>1）点导数是因变量在$ x_0 $ 处的变化率，它反映了因变量随自变量的变化而变化的快慢成都</p>\n<p>2）如果函数y = f(x)在开区间 I 内的每点都可导，就称f(x)在开区间 I 内可导</p>\n<p>3）对于任一 x 属于 I ，都对应着函数f(x)的一个导数，这个函数叫做原来函数f(x)的导函数</p>\n<p>4）导函数在x1 处 为 0，若 x&lt;1 时，f’(x) &gt; 0 ，这 f(x) 递增，若f’(x)&lt;0 ，f(x)递减</p>\n<p>5）f’(x0) 表示曲线y=f(x)在点 （x0,f($x_0$)）处的切线斜率</p>\n<h1 id=\"偏导数\"><a href=\"#偏导数\" class=\"headerlink\" title=\"偏导数\"></a>偏导数</h1><p>函数z=f(x,y)在点(x0,y0)的某一邻域内有定义，当y固定在y0而x在 $x_0$ 处有增量$ \\Delta x $ 时，相应的有函数增量</p>\n<script type=\"math/tex; mode=display\">\nf(x_0 + \\Delta x, y_0) - f(x_0,y_0)</script><p>如果</p>\n<script type=\"math/tex; mode=display\">\n\\lim_{\\Delta x\\rightarrow 0 } \\frac {f(x_0 + \\Delta x, y_0) - f(x_0,y_0)}{\\Delta x}</script><p>存在，则称z=f(x,y)在点($x_0$,$y_0$)处对x的偏导数，记为：$ f_x(x_0,y_0) $</p>\n<p>如果函数z=f(x,y)在区域D内任一点(x,y)处对x的偏导数都存在，那么这个偏导数就是x,y的函数，它就称为函数z=f(x,y)对自变量x的偏导数，记做</p>\n<script type=\"math/tex; mode=display\">\n\\frac{ \\partial z }{ \\partial x } , \\frac{ \\partial f }{ \\partial x } , z_x , f_x(x,y),</script><p>偏导数的概念可以推广到二元以上的函数，如 u = f(x,y,z)在x,y,z处</p>\n<script type=\"math/tex; mode=display\">\nf_x(x,y,z)=\\lim_{\\Delta x \\rightarrow 0} \\frac{f(x + \\Delta x,y,z) -f(x,y,z)}{\\Delta x}</script><script type=\"math/tex; mode=display\">\nf_y(x,y,z)=\\lim_{\\Delta y \\rightarrow 0} \\frac{f(x,y + \\Delta y,z) -f(x,y,z)}{\\Delta y}</script><script type=\"math/tex; mode=display\">\nf_z(x,y,z)=\\lim_{\\Delta z \\rightarrow 0} \\frac{f(x,y,z + \\Delta z) -f(x,y,z)}{\\Delta z}</script><p>可以看出导数与偏导数本质是一致的，都是自变量趋近于0时，函数值的变化与自变量的变化量比值的极限，直观的说，偏导数也就是函数在某一点沿坐标轴正方向的变化率。</p>\n<p>区别：<br>导数指的是一元函数中，函数y=f(x)某一点沿x轴正方向的的变化率；<br>偏导数指的是多元函数中，函数y=f(x,y,z)在某一点沿某一坐标轴正方向的变化率。</p>\n<p>偏导数的几何意义：<br>偏导数$ z = f_x(x_0,y_0)$表示的是曲面被 $ y=y_0 $ 所截得的曲线在点M处的切线$ M_0T_x $对x轴的斜率<br>偏导数$ z = f_y(x_0,y_0)$表示的是曲面被 $ x=x_0 $ 所截得的曲线在点M处的切线$ M_0T_y $对y轴的斜率</p>\n<p>例子：<br>求 $z = x^2 + 3 xy+y^2 $在点(1,2)处的偏导数。</p>\n<script type=\"math/tex; mode=display\">\n\\frac{ \\partial z}{\\partial x} = 2x +3y</script><script type=\"math/tex; mode=display\">\n\\frac{ \\partial z}{\\partial y} = 2y +3x</script><p>所以:<br>$z_x(x=1,y=2) = 8$<br>$z_y(x=1,y=2) = 7$</p>\n<h1 id=\"方向导数\"><a href=\"#方向导数\" class=\"headerlink\" title=\"方向导数\"></a>方向导数</h1><script type=\"math/tex; mode=display\">\n\\frac{ \\partial }{ \\partial l }  f(x_0,x_1,...,x_n) = \\lim_{\\rho \\rightarrow 0} \\frac{\\Delta y}{ \\Delta x } = \\lim_{\\rho \\rightarrow 0} \\frac{ f(x_0 + \\Delta x_0,...,x_j + \\Delta x_j,...,x_n + \\Delta x_n)-f(x_0,...,x_j,...,x_n)}{ \\rho }</script><script type=\"math/tex; mode=display\">\n\\rho = \\sqrt{ (\\Delta x_0)^{2} +...+(\\Delta x_j)^{2}+...+(\\Delta x_n)^{2}}</script><p>前边导数和偏导数的定义中，均是沿坐标轴正方向讨论函数的变化率。那么当讨论函数沿任意方向的变化率时，也就引出了方向导数的定义，即：某一点在某一趋近方向上的导数值。</p>\n<p>通俗的解释是： 我们不仅要知道函数在坐标轴正方向上的变化率（即偏导数），而且还要设法求得函数在其他特定方向上的变化率。而方向导数就是函数在其他特定方向上的变化率。 \n　</p>\n<h1 id=\"梯度\"><a href=\"#梯度\" class=\"headerlink\" title=\"梯度\"></a>梯度</h1><p>与方向导数有一定的关联，在微积分里面，对多元函数的参数求 $ \\partial  $ 偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。比如函数f(x,y), 分别对x,y求偏导数，求得的梯度向量就是 $<br>( \\frac{ \\partial f }{ \\partial x },\\frac{ \\partial f }{ \\partial y })^T<br>$ ,简称grad f(x,y)或者 $▽f(x,y)$。对于在点$(x_0,y_0)$的具体梯度向量就是$( \\frac{ \\partial f }{ \\partial x_0 },\\frac{ \\partial f }{ \\partial y_0 })^T$.或者$▽f(x_0,y_0)$，如果是3个参数的向量梯度，就是 $( \\frac{ \\partial f }{ \\partial x },\\frac{ \\partial f }{ \\partial y },\\frac{ \\partial f }{ \\partial z })^T$,以此类推。</p>\n<p>那么这个梯度向量求出来有什么意义呢？他的意义从几何意义上讲，就是函数变化增加最快的地方。具体来说，对于函数f(x,y),在点$(x_0,y_0)$，沿着梯度向量的方向就是$( \\frac{ \\partial f }{ \\partial x_0 },\\frac{ \\partial f }{ \\partial y_0 })^T$的方向是f(x,y)增加最快的地方。或者说，沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向，也就是 $-( \\frac{ \\partial f }{ \\partial x_0 },\\frac{ \\partial f }{ \\partial y_0 })^T$的方向，梯度减少最快，也就是更加容易找到函数的最小值。</p>\n<p>例如：<br>函数 $f(x,y) = \\frac{1}{x^2+y^2} $ ，分别对x，y求偏导数得：</p>\n<script type=\"math/tex; mode=display\">\n \\frac{ \\partial f }{ \\partial x}=-\\frac{2x}{ (x^2+y^2)^2}</script><script type=\"math/tex; mode=display\">\n \\frac{ \\partial f }{ \\partial y}=-\\frac{2y}{ (x^2+y^2)^2}</script><p>所以</p>\n<script type=\"math/tex; mode=display\">\ngrad( \\frac{1}{x^2+y^2} ) = (-\\frac{2x}{ (x^2+y^2)^2} ,-\\frac{2y}{ (x^2+y^2)^2})</script><p>函数在某一点的梯度是这样一个向量，它的方向与取得最大方向导数的方向一致，而它的模为方向导数的最大值。 </p>\n<p>注意点：<br>1）梯度是一个向量<br>2）梯度的方向是最大方向导数的方向<br>3）梯度的值是最大方向导数的值</p>\n<h1 id=\"梯度下降与梯度上升\"><a href=\"#梯度下降与梯度上升\" class=\"headerlink\" title=\"梯度下降与梯度上升\"></a>梯度下降与梯度上升</h1><p>在机器学习算法中，在最小化损失函数时，可以通过梯度下降思想来求得最小化的损失函数和对应的参数值，反过来，如果要求最大化的损失函数，可以通过梯度上升思想来求取。</p>\n<h2 id=\"梯度下降\"><a href=\"#梯度下降\" class=\"headerlink\" title=\"梯度下降\"></a>梯度下降</h2><h3 id=\"关于梯度下降的几个概念\"><a href=\"#关于梯度下降的几个概念\" class=\"headerlink\" title=\"关于梯度下降的几个概念\"></a>关于梯度下降的几个概念</h3><p>1）步长（learning rate）：步长决定了在梯度下降迭代过程中，每一步沿梯度负方向前进的长度<br>2）特征（feature）：指的是样本中输入部门，比如样本（x0，y0），（x1，y1），则样本特征为x，样本输出为y<br>3）假设函数（hypothesis function）：在监督学习中，为了拟合输入样本，而使用的假设函数，记为$h_θ(x)$。比如对于样本$（x_i,y_i）(i=1,2,…n)$,可以采用拟合函数如下： $h_θ(x) = θ0+θ1_x$。<br>4）损失函数（loss function）：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小化，意味着拟合程度最好，对应的模型参数即为最优参数。在线性回归中，损失函数通常为样本输出和假设函数的差取平方。比如对于样本（xi,yi）(i=1,2,…n),采用线性回归，损失函数为：</p>\n<script type=\"math/tex; mode=display\">\n\\jmath (\\theta _0,\\theta _1)=\\sum_{i=0}^{m}( h_\\theta(x_i)-y_i )^2</script><p>其中$x_i$表示样本特征x的第i个元素，$y_i$表示样本输出y的第i个元素，$h_\\theta(x_i)$ 为假设函数。</p>\n<h3 id=\"梯度下降的代数方法描述\"><a href=\"#梯度下降的代数方法描述\" class=\"headerlink\" title=\"梯度下降的代数方法描述\"></a>梯度下降的代数方法描述</h3><ol>\n<li><p>先决条件：确定优化模型的假设函数和损失函数<br>这里假定线性回归的假设函数为$h_\\theta(x_1,x_2,…x_n)=\\theta_0+\\theta_1x_1+…+\\theta_nx_n$，其中 $\\theta _i(i=0,1,2…n)$ 为模型参数(公式中用$\\theta$代替)，$x_i(i=0,1,2…n)$为每个样本的n个特征值。</p>\n<p>则对应选定得损失函数为：</p>\n<script type=\"math/tex; mode=display\">\n\\jmath (\\theta _0,\\theta _1,...,,\\theta _n)=\\sum_{i=0}^{m}( h_\\theta(x_0,x_1,...,x_n)-y_i )^2</script></li>\n<li><p>算法相关参数的初始化<br>主要是初始化 $ \\theta _0,\\theta _1…,\\theta _n$，算法终止距离 $\\varepsilon $ 以及步长 $ \\alpha $。在没有任何先验知识的时候，我喜欢将所有的 $\\theta$ 初始化为0， 将步长初始化为1。在调优的时候再优化。</p>\n</li>\n<li><p>算法过程</p>\n</li>\n</ol>\n<ul>\n<li><p>1)：确定当前损失函数的梯度，对于$\\theta _i $，其梯度表达式为：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial }{\\partial \\theta _i}\\jmath (\\theta _1,\\theta _2,...,\\theta _n)</script></li>\n<li><p>2)：用步长乘以损失函数的梯度，得到当前位置的下降距离，即</p>\n<script type=\"math/tex; mode=display\">\n\\alpha \\frac{\\partial \\jmath (\\theta _1,\\theta _2,...,\\theta _n)}{\\partial \\theta _i}</script></li>\n<li><p>3)：确定是否所有的$\\theta _i$ ，梯度下降的距离都小于 $ \\varepsilon $，如果小于$ \\varepsilon $，则算法停止，当前所有的 $\\theta _i(i=1,2,3,…,n)$ 即为最终结果。否则执行下一步。</p>\n</li>\n<li><p>4)：更新所有的 $\\theta$，对于$\\theta _i $，其更新表达式如下。更新完毕后继续转入步骤1)。</p>\n<script type=\"math/tex; mode=display\">\n\\theta _i = \\theta _i - \\alpha \\frac{\\partial \\jmath (\\theta _1,\\theta _2,...,\\theta _n)}{\\partial \\theta _i}</script><h3 id=\"梯度下降的矩阵方式描述\"><a href=\"#梯度下降的矩阵方式描述\" class=\"headerlink\" title=\"梯度下降的矩阵方式描述\"></a>梯度下降的矩阵方式描述</h3><ol>\n<li>先决条件：确定优化模型的假设函数和损失函数<br>这里假定线性回归的假设函数为$h_\\theta(x_1,x_2,…x_n)=\\theta_0+\\theta_1x_1+…+\\theta_nx_n$，其中 $\\theta _i(i=0,1,2…n)$ 为模型参数，$x_i(i=0,1,2…n)$为每个样本的n个特征值。<br>假设函数对应的矩阵表示为：$ h_\\theta (x) = X \\theta $，假设函数 $h_\\theta(x)$ 为mx1的向量，$\\theta $ 为nx1的向量，里面有n个代数法的模型参数。X为mxn维的矩阵。m代表样本的个数，n代表样本的特征数。<br>则对应选定得损失函数为：<script type=\"math/tex; mode=display\">\n\\jmath (\\theta)=(X \\theta −Y)^T (X \\theta−Y)</script>其中YY是样本的输出向量，维度为m*1<br><br><br>2.算法相关参数初始化:<br>$\\theta$ 向量可以初始化为默认值，或者调优后的值。算法终止距离 $\\varepsilon $ ，步长 $\\alpha$ 和 “梯度下降的代数方法”描述中一致。<br><br><br>3.算法过程</li>\n</ol>\n</li>\n<li><p>1)：确定当前位置的损失函数的梯度，对于 $ \\theta $ 向量,其梯度表达式如下：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{ \\partial }{\\partial \\theta } \\jmath (\\theta)</script></li>\n<li>2)：用步长乘以损失函数的梯度，得到当前位置下降的距离，即 $\\alpha \\frac{ \\partial }{\\partial \\theta } \\jmath (\\theta)$ </li>\n<li>3)：确定 $\\theta$ 向量里面的每个值,梯度下降的距离都小于 $\\varepsilon$，如果小于 $\\varepsilon$ 则算法终止，当前 $\\theta$ 向量即为最终结果。否则进入步骤4)</li>\n<li>4)：更新 $\\theta$ 向量，其更新表达式如下。更新完毕后继续转入步骤1)<script type=\"math/tex; mode=display\">\n\\theta =\\theta - \\alpha \\frac{ \\partial }{\\partial \\theta } \\jmath (\\theta)</script></li>\n</ul>\n<h2 id=\"梯度上升\"><a href=\"#梯度上升\" class=\"headerlink\" title=\"梯度上升\"></a>梯度上升</h2><p>梯度上升和梯度下降的分析方式是一致的，只不过把 $ \\theta $ 的更新中 减号变为加号。</p>\n<h2 id=\"梯度下降的算法优化\"><a href=\"#梯度下降的算法优化\" class=\"headerlink\" title=\"梯度下降的算法优化\"></a>梯度下降的算法优化</h2><ol>\n<li><p>算法的步长选择。在前面的算法描述中，我提到取步长为1，但是实际上取值取决于数据样本，可以多取一些值，从大到小，分别运行算法，看看迭代效果，如果损失函数在变小，说明取值有效，否则要增大步长。前面说了。步长太大，会导致迭代过快，甚至有可能错过最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较为优的值。</p>\n</li>\n<li><p>算法参数的初始值选择。 初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；当然如果损失函数是凸函数则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。</p>\n</li>\n</ol>\n<p>3.归一化。由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据归一化，也就是对于每个特征x，求出它的均值 $\\bar{x}$ 和标准差std(x)，然后转化为：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{x - \\bar{x}}{std(x)}</script><p>这样特征的新期望为0，新方差为1，迭代次数可以大大加快。</p>\n<hr>\n<p><a href=\"http://blog.csdn.net/walilk/article/details/50978864\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/walilk/article/details/50978864</a></p>\n<p><a href=\"https://www.zhihu.com/question/24658302\" target=\"_blank\" rel=\"external\">https://www.zhihu.com/question/24658302</a></p>\n<p><a href=\"https://www.cnblogs.com/pinard/p/5970503.html\" target=\"_blank\" rel=\"external\">https://www.cnblogs.com/pinard/p/5970503.html</a></p>\n<p><a href=\"http://www.doc88.com/p-7844239247737.html\" target=\"_blank\" rel=\"external\">http://www.doc88.com/p-7844239247737.html</a></p>\n","site":{"data":{}},"excerpt":"<p>第一次看见随机梯度上升算法是看《机器学习实战》这本书，当时也是一知半解，只是大概知道和高等数学中的函数求导有一定的关系。下边我们就好好研究下随机梯度上升（下降）和梯度上升（下降）。<br>","more":"</p>\n<h1 id=\"高数中的导数\"><a href=\"#高数中的导数\" class=\"headerlink\" title=\"高数中的导数\"></a>高数中的导数</h1><p>设导数 y = f(x) 在 $ x_0 $的某个邻域内有定义，当自变量从 $ x_0 $ 变成</p>\n<script type=\"math/tex; mode=display\">\nx_{0} + \\Delta x</script><p>函数y=f(x)的增量</p>\n<script type=\"math/tex; mode=display\">\n\\Delta y = f(x_0 + \\Delta x) - f(x_0)</script><p>与自变量的增量 $ \\Delta x $ 之比：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{ \\Delta y }{ \\Delta x } = \\frac{ f(x_0 + \\Delta x)-f(x_0) }{ \\Delta x }</script><p>称为f(x)的平均变化率。<br>如 $ \\Delta x \\rightarrow 0 $ 平均变化率的极限</p>\n<script type=\"math/tex; mode=display\">\n\\lim_{\\Delta x \\rightarrow 0} \\frac{ \\Delta y }{ \\Delta x } = \\lim_{\\Delta x  \\rightarrow 0} \\frac{ f(x_0 + \\Delta x)-f(x_0) }{ \\Delta x }</script><p>存在，则称极限值为f(x)在$ x_0 $ 处的导数，并说f(x)在$ x_0 $ 处可导或有导数。当平均变化率极限不存在时，就说f(x)在 $ x_0 $ 处不可导或没有导数。</p>\n<p>关于导数的说明</p>\n<p>1）点导数是因变量在$ x_0 $ 处的变化率，它反映了因变量随自变量的变化而变化的快慢成都</p>\n<p>2）如果函数y = f(x)在开区间 I 内的每点都可导，就称f(x)在开区间 I 内可导</p>\n<p>3）对于任一 x 属于 I ，都对应着函数f(x)的一个导数，这个函数叫做原来函数f(x)的导函数</p>\n<p>4）导函数在x1 处 为 0，若 x&lt;1 时，f’(x) &gt; 0 ，这 f(x) 递增，若f’(x)&lt;0 ，f(x)递减</p>\n<p>5）f’(x0) 表示曲线y=f(x)在点 （x0,f($x_0$)）处的切线斜率</p>\n<h1 id=\"偏导数\"><a href=\"#偏导数\" class=\"headerlink\" title=\"偏导数\"></a>偏导数</h1><p>函数z=f(x,y)在点(x0,y0)的某一邻域内有定义，当y固定在y0而x在 $x_0$ 处有增量$ \\Delta x $ 时，相应的有函数增量</p>\n<script type=\"math/tex; mode=display\">\nf(x_0 + \\Delta x, y_0) - f(x_0,y_0)</script><p>如果</p>\n<script type=\"math/tex; mode=display\">\n\\lim_{\\Delta x\\rightarrow 0 } \\frac {f(x_0 + \\Delta x, y_0) - f(x_0,y_0)}{\\Delta x}</script><p>存在，则称z=f(x,y)在点($x_0$,$y_0$)处对x的偏导数，记为：$ f_x(x_0,y_0) $</p>\n<p>如果函数z=f(x,y)在区域D内任一点(x,y)处对x的偏导数都存在，那么这个偏导数就是x,y的函数，它就称为函数z=f(x,y)对自变量x的偏导数，记做</p>\n<script type=\"math/tex; mode=display\">\n\\frac{ \\partial z }{ \\partial x } , \\frac{ \\partial f }{ \\partial x } , z_x , f_x(x,y),</script><p>偏导数的概念可以推广到二元以上的函数，如 u = f(x,y,z)在x,y,z处</p>\n<script type=\"math/tex; mode=display\">\nf_x(x,y,z)=\\lim_{\\Delta x \\rightarrow 0} \\frac{f(x + \\Delta x,y,z) -f(x,y,z)}{\\Delta x}</script><script type=\"math/tex; mode=display\">\nf_y(x,y,z)=\\lim_{\\Delta y \\rightarrow 0} \\frac{f(x,y + \\Delta y,z) -f(x,y,z)}{\\Delta y}</script><script type=\"math/tex; mode=display\">\nf_z(x,y,z)=\\lim_{\\Delta z \\rightarrow 0} \\frac{f(x,y,z + \\Delta z) -f(x,y,z)}{\\Delta z}</script><p>可以看出导数与偏导数本质是一致的，都是自变量趋近于0时，函数值的变化与自变量的变化量比值的极限，直观的说，偏导数也就是函数在某一点沿坐标轴正方向的变化率。</p>\n<p>区别：<br>导数指的是一元函数中，函数y=f(x)某一点沿x轴正方向的的变化率；<br>偏导数指的是多元函数中，函数y=f(x,y,z)在某一点沿某一坐标轴正方向的变化率。</p>\n<p>偏导数的几何意义：<br>偏导数$ z = f_x(x_0,y_0)$表示的是曲面被 $ y=y_0 $ 所截得的曲线在点M处的切线$ M_0T_x $对x轴的斜率<br>偏导数$ z = f_y(x_0,y_0)$表示的是曲面被 $ x=x_0 $ 所截得的曲线在点M处的切线$ M_0T_y $对y轴的斜率</p>\n<p>例子：<br>求 $z = x^2 + 3 xy+y^2 $在点(1,2)处的偏导数。</p>\n<script type=\"math/tex; mode=display\">\n\\frac{ \\partial z}{\\partial x} = 2x +3y</script><script type=\"math/tex; mode=display\">\n\\frac{ \\partial z}{\\partial y} = 2y +3x</script><p>所以:<br>$z_x(x=1,y=2) = 8$<br>$z_y(x=1,y=2) = 7$</p>\n<h1 id=\"方向导数\"><a href=\"#方向导数\" class=\"headerlink\" title=\"方向导数\"></a>方向导数</h1><script type=\"math/tex; mode=display\">\n\\frac{ \\partial }{ \\partial l }  f(x_0,x_1,...,x_n) = \\lim_{\\rho \\rightarrow 0} \\frac{\\Delta y}{ \\Delta x } = \\lim_{\\rho \\rightarrow 0} \\frac{ f(x_0 + \\Delta x_0,...,x_j + \\Delta x_j,...,x_n + \\Delta x_n)-f(x_0,...,x_j,...,x_n)}{ \\rho }</script><script type=\"math/tex; mode=display\">\n\\rho = \\sqrt{ (\\Delta x_0)^{2} +...+(\\Delta x_j)^{2}+...+(\\Delta x_n)^{2}}</script><p>前边导数和偏导数的定义中，均是沿坐标轴正方向讨论函数的变化率。那么当讨论函数沿任意方向的变化率时，也就引出了方向导数的定义，即：某一点在某一趋近方向上的导数值。</p>\n<p>通俗的解释是： 我们不仅要知道函数在坐标轴正方向上的变化率（即偏导数），而且还要设法求得函数在其他特定方向上的变化率。而方向导数就是函数在其他特定方向上的变化率。 \n　</p>\n<h1 id=\"梯度\"><a href=\"#梯度\" class=\"headerlink\" title=\"梯度\"></a>梯度</h1><p>与方向导数有一定的关联，在微积分里面，对多元函数的参数求 $ \\partial  $ 偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。比如函数f(x,y), 分别对x,y求偏导数，求得的梯度向量就是 $<br>( \\frac{ \\partial f }{ \\partial x },\\frac{ \\partial f }{ \\partial y })^T<br>$ ,简称grad f(x,y)或者 $▽f(x,y)$。对于在点$(x_0,y_0)$的具体梯度向量就是$( \\frac{ \\partial f }{ \\partial x_0 },\\frac{ \\partial f }{ \\partial y_0 })^T$.或者$▽f(x_0,y_0)$，如果是3个参数的向量梯度，就是 $( \\frac{ \\partial f }{ \\partial x },\\frac{ \\partial f }{ \\partial y },\\frac{ \\partial f }{ \\partial z })^T$,以此类推。</p>\n<p>那么这个梯度向量求出来有什么意义呢？他的意义从几何意义上讲，就是函数变化增加最快的地方。具体来说，对于函数f(x,y),在点$(x_0,y_0)$，沿着梯度向量的方向就是$( \\frac{ \\partial f }{ \\partial x_0 },\\frac{ \\partial f }{ \\partial y_0 })^T$的方向是f(x,y)增加最快的地方。或者说，沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向，也就是 $-( \\frac{ \\partial f }{ \\partial x_0 },\\frac{ \\partial f }{ \\partial y_0 })^T$的方向，梯度减少最快，也就是更加容易找到函数的最小值。</p>\n<p>例如：<br>函数 $f(x,y) = \\frac{1}{x^2+y^2} $ ，分别对x，y求偏导数得：</p>\n<script type=\"math/tex; mode=display\">\n \\frac{ \\partial f }{ \\partial x}=-\\frac{2x}{ (x^2+y^2)^2}</script><script type=\"math/tex; mode=display\">\n \\frac{ \\partial f }{ \\partial y}=-\\frac{2y}{ (x^2+y^2)^2}</script><p>所以</p>\n<script type=\"math/tex; mode=display\">\ngrad( \\frac{1}{x^2+y^2} ) = (-\\frac{2x}{ (x^2+y^2)^2} ,-\\frac{2y}{ (x^2+y^2)^2})</script><p>函数在某一点的梯度是这样一个向量，它的方向与取得最大方向导数的方向一致，而它的模为方向导数的最大值。 </p>\n<p>注意点：<br>1）梯度是一个向量<br>2）梯度的方向是最大方向导数的方向<br>3）梯度的值是最大方向导数的值</p>\n<h1 id=\"梯度下降与梯度上升\"><a href=\"#梯度下降与梯度上升\" class=\"headerlink\" title=\"梯度下降与梯度上升\"></a>梯度下降与梯度上升</h1><p>在机器学习算法中，在最小化损失函数时，可以通过梯度下降思想来求得最小化的损失函数和对应的参数值，反过来，如果要求最大化的损失函数，可以通过梯度上升思想来求取。</p>\n<h2 id=\"梯度下降\"><a href=\"#梯度下降\" class=\"headerlink\" title=\"梯度下降\"></a>梯度下降</h2><h3 id=\"关于梯度下降的几个概念\"><a href=\"#关于梯度下降的几个概念\" class=\"headerlink\" title=\"关于梯度下降的几个概念\"></a>关于梯度下降的几个概念</h3><p>1）步长（learning rate）：步长决定了在梯度下降迭代过程中，每一步沿梯度负方向前进的长度<br>2）特征（feature）：指的是样本中输入部门，比如样本（x0，y0），（x1，y1），则样本特征为x，样本输出为y<br>3）假设函数（hypothesis function）：在监督学习中，为了拟合输入样本，而使用的假设函数，记为$h_θ(x)$。比如对于样本$（x_i,y_i）(i=1,2,…n)$,可以采用拟合函数如下： $h_θ(x) = θ0+θ1_x$。<br>4）损失函数（loss function）：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小化，意味着拟合程度最好，对应的模型参数即为最优参数。在线性回归中，损失函数通常为样本输出和假设函数的差取平方。比如对于样本（xi,yi）(i=1,2,…n),采用线性回归，损失函数为：</p>\n<script type=\"math/tex; mode=display\">\n\\jmath (\\theta _0,\\theta _1)=\\sum_{i=0}^{m}( h_\\theta(x_i)-y_i )^2</script><p>其中$x_i$表示样本特征x的第i个元素，$y_i$表示样本输出y的第i个元素，$h_\\theta(x_i)$ 为假设函数。</p>\n<h3 id=\"梯度下降的代数方法描述\"><a href=\"#梯度下降的代数方法描述\" class=\"headerlink\" title=\"梯度下降的代数方法描述\"></a>梯度下降的代数方法描述</h3><ol>\n<li><p>先决条件：确定优化模型的假设函数和损失函数<br>这里假定线性回归的假设函数为$h_\\theta(x_1,x_2,…x_n)=\\theta_0+\\theta_1x_1+…+\\theta_nx_n$，其中 $\\theta _i(i=0,1,2…n)$ 为模型参数(公式中用$\\theta$代替)，$x_i(i=0,1,2…n)$为每个样本的n个特征值。</p>\n<p>则对应选定得损失函数为：</p>\n<script type=\"math/tex; mode=display\">\n\\jmath (\\theta _0,\\theta _1,...,,\\theta _n)=\\sum_{i=0}^{m}( h_\\theta(x_0,x_1,...,x_n)-y_i )^2</script></li>\n<li><p>算法相关参数的初始化<br>主要是初始化 $ \\theta _0,\\theta _1…,\\theta _n$，算法终止距离 $\\varepsilon $ 以及步长 $ \\alpha $。在没有任何先验知识的时候，我喜欢将所有的 $\\theta$ 初始化为0， 将步长初始化为1。在调优的时候再优化。</p>\n</li>\n<li><p>算法过程</p>\n</li>\n</ol>\n<ul>\n<li><p>1)：确定当前损失函数的梯度，对于$\\theta _i $，其梯度表达式为：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial }{\\partial \\theta _i}\\jmath (\\theta _1,\\theta _2,...,\\theta _n)</script></li>\n<li><p>2)：用步长乘以损失函数的梯度，得到当前位置的下降距离，即</p>\n<script type=\"math/tex; mode=display\">\n\\alpha \\frac{\\partial \\jmath (\\theta _1,\\theta _2,...,\\theta _n)}{\\partial \\theta _i}</script></li>\n<li><p>3)：确定是否所有的$\\theta _i$ ，梯度下降的距离都小于 $ \\varepsilon $，如果小于$ \\varepsilon $，则算法停止，当前所有的 $\\theta _i(i=1,2,3,…,n)$ 即为最终结果。否则执行下一步。</p>\n</li>\n<li><p>4)：更新所有的 $\\theta$，对于$\\theta _i $，其更新表达式如下。更新完毕后继续转入步骤1)。</p>\n<script type=\"math/tex; mode=display\">\n\\theta _i = \\theta _i - \\alpha \\frac{\\partial \\jmath (\\theta _1,\\theta _2,...,\\theta _n)}{\\partial \\theta _i}</script><h3 id=\"梯度下降的矩阵方式描述\"><a href=\"#梯度下降的矩阵方式描述\" class=\"headerlink\" title=\"梯度下降的矩阵方式描述\"></a>梯度下降的矩阵方式描述</h3><ol>\n<li>先决条件：确定优化模型的假设函数和损失函数<br>这里假定线性回归的假设函数为$h_\\theta(x_1,x_2,…x_n)=\\theta_0+\\theta_1x_1+…+\\theta_nx_n$，其中 $\\theta _i(i=0,1,2…n)$ 为模型参数，$x_i(i=0,1,2…n)$为每个样本的n个特征值。<br>假设函数对应的矩阵表示为：$ h_\\theta (x) = X \\theta $，假设函数 $h_\\theta(x)$ 为mx1的向量，$\\theta $ 为nx1的向量，里面有n个代数法的模型参数。X为mxn维的矩阵。m代表样本的个数，n代表样本的特征数。<br>则对应选定得损失函数为：<script type=\"math/tex; mode=display\">\n\\jmath (\\theta)=(X \\theta −Y)^T (X \\theta−Y)</script>其中YY是样本的输出向量，维度为m*1<br><br><br>2.算法相关参数初始化:<br>$\\theta$ 向量可以初始化为默认值，或者调优后的值。算法终止距离 $\\varepsilon $ ，步长 $\\alpha$ 和 “梯度下降的代数方法”描述中一致。<br><br><br>3.算法过程</li>\n</ol>\n</li>\n<li><p>1)：确定当前位置的损失函数的梯度，对于 $ \\theta $ 向量,其梯度表达式如下：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{ \\partial }{\\partial \\theta } \\jmath (\\theta)</script></li>\n<li>2)：用步长乘以损失函数的梯度，得到当前位置下降的距离，即 $\\alpha \\frac{ \\partial }{\\partial \\theta } \\jmath (\\theta)$ </li>\n<li>3)：确定 $\\theta$ 向量里面的每个值,梯度下降的距离都小于 $\\varepsilon$，如果小于 $\\varepsilon$ 则算法终止，当前 $\\theta$ 向量即为最终结果。否则进入步骤4)</li>\n<li>4)：更新 $\\theta$ 向量，其更新表达式如下。更新完毕后继续转入步骤1)<script type=\"math/tex; mode=display\">\n\\theta =\\theta - \\alpha \\frac{ \\partial }{\\partial \\theta } \\jmath (\\theta)</script></li>\n</ul>\n<h2 id=\"梯度上升\"><a href=\"#梯度上升\" class=\"headerlink\" title=\"梯度上升\"></a>梯度上升</h2><p>梯度上升和梯度下降的分析方式是一致的，只不过把 $ \\theta $ 的更新中 减号变为加号。</p>\n<h2 id=\"梯度下降的算法优化\"><a href=\"#梯度下降的算法优化\" class=\"headerlink\" title=\"梯度下降的算法优化\"></a>梯度下降的算法优化</h2><ol>\n<li><p>算法的步长选择。在前面的算法描述中，我提到取步长为1，但是实际上取值取决于数据样本，可以多取一些值，从大到小，分别运行算法，看看迭代效果，如果损失函数在变小，说明取值有效，否则要增大步长。前面说了。步长太大，会导致迭代过快，甚至有可能错过最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较为优的值。</p>\n</li>\n<li><p>算法参数的初始值选择。 初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；当然如果损失函数是凸函数则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。</p>\n</li>\n</ol>\n<p>3.归一化。由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据归一化，也就是对于每个特征x，求出它的均值 $\\bar{x}$ 和标准差std(x)，然后转化为：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{x - \\bar{x}}{std(x)}</script><p>这样特征的新期望为0，新方差为1，迭代次数可以大大加快。</p>\n<hr>\n<p><a href=\"http://blog.csdn.net/walilk/article/details/50978864\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/walilk/article/details/50978864</a></p>\n<p><a href=\"https://www.zhihu.com/question/24658302\" target=\"_blank\" rel=\"external\">https://www.zhihu.com/question/24658302</a></p>\n<p><a href=\"https://www.cnblogs.com/pinard/p/5970503.html\" target=\"_blank\" rel=\"external\">https://www.cnblogs.com/pinard/p/5970503.html</a></p>\n<p><a href=\"http://www.doc88.com/p-7844239247737.html\" target=\"_blank\" rel=\"external\">http://www.doc88.com/p-7844239247737.html</a></p>"},{"title":"Hexo-Yilia加入相册功能","date":"2017-12-14T09:55:29.000Z","_content":"参考：[点击查看](http://maker997.com/2017/07/01/hexo-Yilia-%E4%B8%BB%E9%A2%98%E5%A6%82%E4%BD%95%E6%B7%BB%E5%8A%A0%E7%9B%B8%E5%86%8C%E5%8A%9F%E8%83%BD)\n\n但是其中有一些小问题，自己便重新整理了一下（本文适用于使用github存放照片）\n\n<!--More-->\n# 主页新建相册链接\n\n主题_config.json文件的menu 中加入 相册和对应的链接\n```\nthemes/yilia/_config.json\n\nmenu:\n  主页: /\n  ... ...\n  相册: /photos\n```\n\n# 新建目录并拷贝相应文件\n使用的是litten 大神的博客 photos文件夹，对应的路径为：\nhttps://github.com/litten/BlogBackup/tree/master/source/photos\n\n自己的项目根目录下的source文件夹下新建photos文件夹，将下载的几个文件放在该文件夹中，或者不用新建，直接将下载的photos文件夹放在source目录下。\n\n# 文件修改\n\n 1. 修改 ins.js 文件的 render()函数\n 这个函数是用来渲染数据的\n修改图片的路径地址.minSrc 小图的路径. src 大图的路径.修改为自己的图片路径(github的路径)\n例如我的为：\n```\nvar minSrc = 'https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/min_photos/' + data.link[i] + '.min.jpg';\nvar src = 'https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/photos/' + data.link[i];\n\n```\n# 生成json\n1：下载相应python工具文件\n\n- tools.py\n- ImageProcess.py\n\n下载地址：https://github.com/Thinkgamer/GitBlog\n\n2：新建photos和min_photos文件夹\n在项目根目录下创建，用来存放照片和压缩后的照片\n```\nmkdir photos\nmkdir min_photos\n```\n3：py文件和文件夹都放在项目根目录下\n\n4：生成json\n执行\n```\npython tools.py\n```\n如果提示：\n```\nTraceback (most recent call last):\n  File \"tools.py\", line 13, in <module>\n    from PIL import Image\nImportError: No module named PIL\n```\n说明你没有安装pillow，执行以下命令安装即可\n```\npip install pillow\n```\n\n如果报错：\n```\nValueError: time data 'DSC' does not match format '%Y-%m-%d'\n```\n说明你照片的命名方式不合格，这里必须命名为以下这样的格式（当然时间是随意的）\n```\n2016-10-12_xxx.jpg/png\n```\nok，至此会在min_photos文件夹下生成同名的文件，但是大小会小很多\n\n# 本地预览和部署\n## 本地预览\n项目根目录下执行\n```\nhexo s\n```\n浏览器4000端口访问，按照上边的方式进行配置，正常情况下你是看不到图片的，通过调试可以发现图片的url中后缀变成了 xxx.jpg.jpg，所以我们要去掉一个jpg\n\n改正方法\nins.js/render 函数\n```\nvar minSrc = 'https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/min_photos/' + data.link[i] + '.min.jpg';\n\n换成\n\nvar minSrc = 'https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/min_photos/' + data.link[i];\n\n注释掉该行：\nsrc += '.jpg'; \n```\n\n到这里没完，路径都对了，但是在浏览器中还是不能看到图片，调试发现，下载大神的photos文件夹的ins.js中有一行代码，饮用了一张图片，默认情况下，在你的项目中，这张图片是不存在的，改正办法就是在对应目录下放一张图片，并修改相应的名字\n\n```\nsrc=\"/assets/img/empty.png\n```\n\nok，至此刷新浏览器是可以看到图片的，如果还看不到，应该就是浏览器缓存问题了，如果还有问题，可以加我微信进行沟通：gyt13342445911","source":"_posts/随手记/Hexo-Yilia加入相册功能.md","raw":"---\ntitle: Hexo-Yilia加入相册功能\ndate: 2017-12-14 17:55:29\ntags: [hexo]\ncategories: 随手记\n---\n参考：[点击查看](http://maker997.com/2017/07/01/hexo-Yilia-%E4%B8%BB%E9%A2%98%E5%A6%82%E4%BD%95%E6%B7%BB%E5%8A%A0%E7%9B%B8%E5%86%8C%E5%8A%9F%E8%83%BD)\n\n但是其中有一些小问题，自己便重新整理了一下（本文适用于使用github存放照片）\n\n<!--More-->\n# 主页新建相册链接\n\n主题_config.json文件的menu 中加入 相册和对应的链接\n```\nthemes/yilia/_config.json\n\nmenu:\n  主页: /\n  ... ...\n  相册: /photos\n```\n\n# 新建目录并拷贝相应文件\n使用的是litten 大神的博客 photos文件夹，对应的路径为：\nhttps://github.com/litten/BlogBackup/tree/master/source/photos\n\n自己的项目根目录下的source文件夹下新建photos文件夹，将下载的几个文件放在该文件夹中，或者不用新建，直接将下载的photos文件夹放在source目录下。\n\n# 文件修改\n\n 1. 修改 ins.js 文件的 render()函数\n 这个函数是用来渲染数据的\n修改图片的路径地址.minSrc 小图的路径. src 大图的路径.修改为自己的图片路径(github的路径)\n例如我的为：\n```\nvar minSrc = 'https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/min_photos/' + data.link[i] + '.min.jpg';\nvar src = 'https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/photos/' + data.link[i];\n\n```\n# 生成json\n1：下载相应python工具文件\n\n- tools.py\n- ImageProcess.py\n\n下载地址：https://github.com/Thinkgamer/GitBlog\n\n2：新建photos和min_photos文件夹\n在项目根目录下创建，用来存放照片和压缩后的照片\n```\nmkdir photos\nmkdir min_photos\n```\n3：py文件和文件夹都放在项目根目录下\n\n4：生成json\n执行\n```\npython tools.py\n```\n如果提示：\n```\nTraceback (most recent call last):\n  File \"tools.py\", line 13, in <module>\n    from PIL import Image\nImportError: No module named PIL\n```\n说明你没有安装pillow，执行以下命令安装即可\n```\npip install pillow\n```\n\n如果报错：\n```\nValueError: time data 'DSC' does not match format '%Y-%m-%d'\n```\n说明你照片的命名方式不合格，这里必须命名为以下这样的格式（当然时间是随意的）\n```\n2016-10-12_xxx.jpg/png\n```\nok，至此会在min_photos文件夹下生成同名的文件，但是大小会小很多\n\n# 本地预览和部署\n## 本地预览\n项目根目录下执行\n```\nhexo s\n```\n浏览器4000端口访问，按照上边的方式进行配置，正常情况下你是看不到图片的，通过调试可以发现图片的url中后缀变成了 xxx.jpg.jpg，所以我们要去掉一个jpg\n\n改正方法\nins.js/render 函数\n```\nvar minSrc = 'https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/min_photos/' + data.link[i] + '.min.jpg';\n\n换成\n\nvar minSrc = 'https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/min_photos/' + data.link[i];\n\n注释掉该行：\nsrc += '.jpg'; \n```\n\n到这里没完，路径都对了，但是在浏览器中还是不能看到图片，调试发现，下载大神的photos文件夹的ins.js中有一行代码，饮用了一张图片，默认情况下，在你的项目中，这张图片是不存在的，改正办法就是在对应目录下放一张图片，并修改相应的名字\n\n```\nsrc=\"/assets/img/empty.png\n```\n\nok，至此刷新浏览器是可以看到图片的，如果还看不到，应该就是浏览器缓存问题了，如果还有问题，可以加我微信进行沟通：gyt13342445911","slug":"随手记/Hexo-Yilia加入相册功能","published":1,"updated":"2017-12-19T02:54:56.791Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjbd1r26r001bkxuwkcn3b7fq","content":"<p>参考：<a href=\"http://maker997.com/2017/07/01/hexo-Yilia-%E4%B8%BB%E9%A2%98%E5%A6%82%E4%BD%95%E6%B7%BB%E5%8A%A0%E7%9B%B8%E5%86%8C%E5%8A%9F%E8%83%BD\" target=\"_blank\" rel=\"external\">点击查看</a></p>\n<p>但是其中有一些小问题，自己便重新整理了一下（本文适用于使用github存放照片）</p>\n<a id=\"more\"></a>\n<h1 id=\"主页新建相册链接\"><a href=\"#主页新建相册链接\" class=\"headerlink\" title=\"主页新建相册链接\"></a>主页新建相册链接</h1><p>主题_config.json文件的menu 中加入 相册和对应的链接<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">themes/yilia/_config.json</span><br><span class=\"line\"></span><br><span class=\"line\">menu:</span><br><span class=\"line\">  主页: /</span><br><span class=\"line\">  ... ...</span><br><span class=\"line\">  相册: /photos</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"新建目录并拷贝相应文件\"><a href=\"#新建目录并拷贝相应文件\" class=\"headerlink\" title=\"新建目录并拷贝相应文件\"></a>新建目录并拷贝相应文件</h1><p>使用的是litten 大神的博客 photos文件夹，对应的路径为：<br><a href=\"https://github.com/litten/BlogBackup/tree/master/source/photos\" target=\"_blank\" rel=\"external\">https://github.com/litten/BlogBackup/tree/master/source/photos</a></p>\n<p>自己的项目根目录下的source文件夹下新建photos文件夹，将下载的几个文件放在该文件夹中，或者不用新建，直接将下载的photos文件夹放在source目录下。</p>\n<h1 id=\"文件修改\"><a href=\"#文件修改\" class=\"headerlink\" title=\"文件修改\"></a>文件修改</h1><ol>\n<li>修改 ins.js 文件的 render()函数<br>这个函数是用来渲染数据的<br>修改图片的路径地址.minSrc 小图的路径. src 大图的路径.修改为自己的图片路径(github的路径)<br>例如我的为：<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">var minSrc = &apos;https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/min_photos/&apos; + data.link[i] + &apos;.min.jpg&apos;;</span><br><span class=\"line\">var src = &apos;https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/photos/&apos; + data.link[i];</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<h1 id=\"生成json\"><a href=\"#生成json\" class=\"headerlink\" title=\"生成json\"></a>生成json</h1><p>1：下载相应python工具文件</p>\n<ul>\n<li>tools.py</li>\n<li>ImageProcess.py</li>\n</ul>\n<p>下载地址：<a href=\"https://github.com/Thinkgamer/GitBlog\" target=\"_blank\" rel=\"external\">https://github.com/Thinkgamer/GitBlog</a></p>\n<p>2：新建photos和min_photos文件夹<br>在项目根目录下创建，用来存放照片和压缩后的照片<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir photos</span><br><span class=\"line\">mkdir min_photos</span><br></pre></td></tr></table></figure></p>\n<p>3：py文件和文件夹都放在项目根目录下</p>\n<p>4：生成json<br>执行<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python tools.py</span><br></pre></td></tr></table></figure></p>\n<p>如果提示：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Traceback (most recent call last):</span><br><span class=\"line\">  File &quot;tools.py&quot;, line 13, in &lt;module&gt;</span><br><span class=\"line\">    from PIL import Image</span><br><span class=\"line\">ImportError: No module named PIL</span><br></pre></td></tr></table></figure></p>\n<p>说明你没有安装pillow，执行以下命令安装即可<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install pillow</span><br></pre></td></tr></table></figure></p>\n<p>如果报错：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ValueError: time data &apos;DSC&apos; does not match format &apos;%Y-%m-%d&apos;</span><br></pre></td></tr></table></figure></p>\n<p>说明你照片的命名方式不合格，这里必须命名为以下这样的格式（当然时间是随意的）<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">2016-10-12_xxx.jpg/png</span><br></pre></td></tr></table></figure></p>\n<p>ok，至此会在min_photos文件夹下生成同名的文件，但是大小会小很多</p>\n<h1 id=\"本地预览和部署\"><a href=\"#本地预览和部署\" class=\"headerlink\" title=\"本地预览和部署\"></a>本地预览和部署</h1><h2 id=\"本地预览\"><a href=\"#本地预览\" class=\"headerlink\" title=\"本地预览\"></a>本地预览</h2><p>项目根目录下执行<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo s</span><br></pre></td></tr></table></figure></p>\n<p>浏览器4000端口访问，按照上边的方式进行配置，正常情况下你是看不到图片的，通过调试可以发现图片的url中后缀变成了 xxx.jpg.jpg，所以我们要去掉一个jpg</p>\n<p>改正方法<br>ins.js/render 函数<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">var minSrc = &apos;https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/min_photos/&apos; + data.link[i] + &apos;.min.jpg&apos;;</span><br><span class=\"line\"></span><br><span class=\"line\">换成</span><br><span class=\"line\"></span><br><span class=\"line\">var minSrc = &apos;https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/min_photos/&apos; + data.link[i];</span><br><span class=\"line\"></span><br><span class=\"line\">注释掉该行：</span><br><span class=\"line\">src += &apos;.jpg&apos;;</span><br></pre></td></tr></table></figure></p>\n<p>到这里没完，路径都对了，但是在浏览器中还是不能看到图片，调试发现，下载大神的photos文件夹的ins.js中有一行代码，饮用了一张图片，默认情况下，在你的项目中，这张图片是不存在的，改正办法就是在对应目录下放一张图片，并修改相应的名字</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">src=&quot;/assets/img/empty.png</span><br></pre></td></tr></table></figure>\n<p>ok，至此刷新浏览器是可以看到图片的，如果还看不到，应该就是浏览器缓存问题了，如果还有问题，可以加我微信进行沟通：gyt13342445911</p>\n","site":{"data":{}},"excerpt":"<p>参考：<a href=\"http://maker997.com/2017/07/01/hexo-Yilia-%E4%B8%BB%E9%A2%98%E5%A6%82%E4%BD%95%E6%B7%BB%E5%8A%A0%E7%9B%B8%E5%86%8C%E5%8A%9F%E8%83%BD\" target=\"_blank\" rel=\"external\">点击查看</a></p>\n<p>但是其中有一些小问题，自己便重新整理了一下（本文适用于使用github存放照片）</p>","more":"<h1 id=\"主页新建相册链接\"><a href=\"#主页新建相册链接\" class=\"headerlink\" title=\"主页新建相册链接\"></a>主页新建相册链接</h1><p>主题_config.json文件的menu 中加入 相册和对应的链接<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">themes/yilia/_config.json</span><br><span class=\"line\"></span><br><span class=\"line\">menu:</span><br><span class=\"line\">  主页: /</span><br><span class=\"line\">  ... ...</span><br><span class=\"line\">  相册: /photos</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"新建目录并拷贝相应文件\"><a href=\"#新建目录并拷贝相应文件\" class=\"headerlink\" title=\"新建目录并拷贝相应文件\"></a>新建目录并拷贝相应文件</h1><p>使用的是litten 大神的博客 photos文件夹，对应的路径为：<br><a href=\"https://github.com/litten/BlogBackup/tree/master/source/photos\" target=\"_blank\" rel=\"external\">https://github.com/litten/BlogBackup/tree/master/source/photos</a></p>\n<p>自己的项目根目录下的source文件夹下新建photos文件夹，将下载的几个文件放在该文件夹中，或者不用新建，直接将下载的photos文件夹放在source目录下。</p>\n<h1 id=\"文件修改\"><a href=\"#文件修改\" class=\"headerlink\" title=\"文件修改\"></a>文件修改</h1><ol>\n<li>修改 ins.js 文件的 render()函数<br>这个函数是用来渲染数据的<br>修改图片的路径地址.minSrc 小图的路径. src 大图的路径.修改为自己的图片路径(github的路径)<br>例如我的为：<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">var minSrc = &apos;https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/min_photos/&apos; + data.link[i] + &apos;.min.jpg&apos;;</span><br><span class=\"line\">var src = &apos;https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/photos/&apos; + data.link[i];</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<h1 id=\"生成json\"><a href=\"#生成json\" class=\"headerlink\" title=\"生成json\"></a>生成json</h1><p>1：下载相应python工具文件</p>\n<ul>\n<li>tools.py</li>\n<li>ImageProcess.py</li>\n</ul>\n<p>下载地址：<a href=\"https://github.com/Thinkgamer/GitBlog\" target=\"_blank\" rel=\"external\">https://github.com/Thinkgamer/GitBlog</a></p>\n<p>2：新建photos和min_photos文件夹<br>在项目根目录下创建，用来存放照片和压缩后的照片<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir photos</span><br><span class=\"line\">mkdir min_photos</span><br></pre></td></tr></table></figure></p>\n<p>3：py文件和文件夹都放在项目根目录下</p>\n<p>4：生成json<br>执行<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python tools.py</span><br></pre></td></tr></table></figure></p>\n<p>如果提示：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Traceback (most recent call last):</span><br><span class=\"line\">  File &quot;tools.py&quot;, line 13, in &lt;module&gt;</span><br><span class=\"line\">    from PIL import Image</span><br><span class=\"line\">ImportError: No module named PIL</span><br></pre></td></tr></table></figure></p>\n<p>说明你没有安装pillow，执行以下命令安装即可<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install pillow</span><br></pre></td></tr></table></figure></p>\n<p>如果报错：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ValueError: time data &apos;DSC&apos; does not match format &apos;%Y-%m-%d&apos;</span><br></pre></td></tr></table></figure></p>\n<p>说明你照片的命名方式不合格，这里必须命名为以下这样的格式（当然时间是随意的）<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">2016-10-12_xxx.jpg/png</span><br></pre></td></tr></table></figure></p>\n<p>ok，至此会在min_photos文件夹下生成同名的文件，但是大小会小很多</p>\n<h1 id=\"本地预览和部署\"><a href=\"#本地预览和部署\" class=\"headerlink\" title=\"本地预览和部署\"></a>本地预览和部署</h1><h2 id=\"本地预览\"><a href=\"#本地预览\" class=\"headerlink\" title=\"本地预览\"></a>本地预览</h2><p>项目根目录下执行<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo s</span><br></pre></td></tr></table></figure></p>\n<p>浏览器4000端口访问，按照上边的方式进行配置，正常情况下你是看不到图片的，通过调试可以发现图片的url中后缀变成了 xxx.jpg.jpg，所以我们要去掉一个jpg</p>\n<p>改正方法<br>ins.js/render 函数<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">var minSrc = &apos;https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/min_photos/&apos; + data.link[i] + &apos;.min.jpg&apos;;</span><br><span class=\"line\"></span><br><span class=\"line\">换成</span><br><span class=\"line\"></span><br><span class=\"line\">var minSrc = &apos;https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/min_photos/&apos; + data.link[i];</span><br><span class=\"line\"></span><br><span class=\"line\">注释掉该行：</span><br><span class=\"line\">src += &apos;.jpg&apos;;</span><br></pre></td></tr></table></figure></p>\n<p>到这里没完，路径都对了，但是在浏览器中还是不能看到图片，调试发现，下载大神的photos文件夹的ins.js中有一行代码，饮用了一张图片，默认情况下，在你的项目中，这张图片是不存在的，改正办法就是在对应目录下放一张图片，并修改相应的名字</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">src=&quot;/assets/img/empty.png</span><br></pre></td></tr></table></figure>\n<p>ok，至此刷新浏览器是可以看到图片的，如果还看不到，应该就是浏览器缓存问题了，如果还有问题，可以加我微信进行沟通：gyt13342445911</p>"},{"title":"一切的闹闹哄哄，只是他在水帘洞躲避风沙那晚做的一个梦","date":"2017-04-15T17:03:56.000Z","_content":"\n>送同学走之后，我在路边默默的站了有五分钟，突然觉得我无处可去，有一种深入骨髓的悲哀和无奈，然后我就想起了一个命题，“如今的你，何去何从！”我不知道为什么会突然想到这样一个命题，或许是我们每个人都是至尊宝吧。其实每个人对《大话》的理解都是有所不同的，同样的人在不同的时期认识也会有偏差，就好比我第一次看的时候，笑得腹背抽筋，呲牙咧嘴，第二次看的时候，笑得少了，想的多了，过后便什么感觉也没有了，第三次看得时候，忽然觉得不知道是该哭还是该笑，笑得时候太牵强，哭得时候太尴尬。第四次便是这一次，看完之后觉得有一种无可奈何的悲哀。\n\n<!--More-->\n![一切的闹闹哄哄，只是他在水帘洞躲避风沙那晚做的一个梦。](http://img.blog.csdn.net/20170415205051154?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n-----\n## **对《大话》的解读**\n\n有人说这是一部烂片，也有人说这是一部经典，有人说这是对西游文化的过度消费，有人说这是一代人的反思与成长，有人说阴差阳错的蝴蝶效应，让一个年代的人支撑起了这部超越中国电影水平50年的无厘头式作品，也有人说这是一部笑中带泪，总有一天你走在路上也会像一条狗的悲剧。\n\n一千个人眼中有一千个哈姆雷特，对于一部影视作品，我们无法去评论任何一个人的理解是对还是错，因为经历不同，就像你不能把你的情感强加到别人身上。喜欢一部作品，在不知不觉间你会忽略他的缺点，讨厌一个事物，在不知不觉间就会放大他的短处。我们也不能用我们现代的思维和观念去评判20世纪港台电影的文化和情感。\n\n至尊宝的路线是，有人给他点了三颗痣，拔出来紫青宝剑，开启了与紫霞的交涉，娶亲，戴上紧箍咒，大战牛魔王，西天取经。其实至尊宝这条路线，何尝不是我们的漫漫人生路呢，至尊宝用月光宝盒来寻找500年前的白晶晶，却阴差阳错遇到了紫霞仙子，最后在对白晶晶失而复得之后又得知紫霞在他内心留下的一滴眼泪，最终大彻大悟，明白了舍生取义，参透了生亦何哀，死亦何苦。我们的人生不也一样，总会有各种各样的未知因素影响我们的选择，在努力面对眼前一切之后，最终回头感叹年华，围炉小饮，本来无一物，何处惹尘埃。\n\n对于紫霞和至尊宝的爱情，没有这种体验，是不会懂的，越是深情的人越容易受伤，她放下尊严来爱他，终于他后悔了。在至尊宝的梦里，紫霞仙子说：“飞蛾明明知道前面是火堆，却还义无反顾的扑进去。”她笑一下，接着说：“飞蛾就是这么傻！”而同样的至尊宝，“有一个名字叫紫霞的，你叫七百八十四次“，时光真是个好东西，所谓雕刻时光，是说生活像把钝刀，锉平我们的触觉，而电影则是解毒的重药，它让人突然领悟到，我们的内心要比自己想像的敏感干净得多！你等的那么辛苦，他却陌生到让你心疼。以至于在人生的不同阶段去欣赏《大话》，似乎都能从中找到某些影子，原先是笑，后来是苦，到最后便是沉默了。城墙上的一吻，至尊宝变成了旁观者，他借用夕阳武士的身份和转世的紫霞完成了他告别的深情，于是他的背影，他的离开，寂寞成了“好像一条狗”。\n\n------\n## **他好像一条狗呀！**\n\n-“那个人的样子好怪啊”\n-“我也看到了，他好像条狗啊”\n是呀，英雄的离开，留下的永远只是背影，只不过在至尊宝这里加上了悲剧色彩，加上了人艰不拆的辛酸泪，以至于多少年后才明白了至尊宝转身离开有多难。至尊宝的结局是一个男人的悲怆与无奈。\n\n至尊宝用了月光宝盒来寻找500年前的白晶晶，同时遇到了在她看来，他是她的命中注定的紫霞仙子，直到后来牛魔王的出现，夺走了紫霞，夺走了白晶晶，夺走了至尊宝往日的快乐，他明白他要夺回这一切，可是面对戏剧般的月光宝盒，至尊宝得到的更多是无力和苍白，面对这些无情的现实，幻想一次又一次地破灭。直到最后的关头，至尊宝终于醒悟，靠月光宝盒不行，至尊宝更是没有那个本事，只有成为孙悟空，只有戴上那个金刚圈，他才有能力同牛魔王一较高下。\n\n这是一个极大的讽刺。你想要得到吗？好，那么你必须先放弃至尊宝的身份，你必须做出选择，必须忍受无尽的痛苦，他想要化解时间无尽的仇恨，就必须放弃自己的感情，不是不爱，而是大彻大悟之后的大爱，他必需化身为孙悟空，帮助唐三藏取得经书，化解这世间的恨。\n\n那么至尊宝的放弃是自觉自愿的醒悟吗？不，他并不愿意，但是他必须拯救紫霞，必须化解人间的恨，他别无选择，他必须戴上紧箍咒。虽然成为了孙悟空，成了大英雄，但他对自己的生存状态极度不满。所以在最后，孙悟空将他心中残存的至尊宝的影子幻化作一位夕阳武士，在对现实世界彻底失望后，只能构造一个虚幻的想象来了却这桩心愿，并借武士的口中表达了对自己生存状态的不满，活得好象是一条狗一样。唉，一个男人的悲怆和无奈。\n\n------\n## **那句意中人，满足了多少人的少女心**\n\n“我知道有一天，他会在一个万众瞩目的情况下出现，身披金甲圣衣，脚踏七色云彩来娶我”\n“我的意中人是个盖世英雄，有一天他会踏着云彩来娶我”\n这两句分别是紫霞在牛魔王娶她前的晚上和死前对孙悟空说的，多么经典的台词，以至于现在多少人还幻想着自己的意中人。\n\n进入至尊宝内心的只有两个女人，一个是白晶晶，一个是紫霞，白晶晶问的是“他最喜欢的人是不是我”，紫霞问的是“他跟他的娘子是不是很恩爱呀？”，白晶晶的爱是一种索求的爱，而紫霞的爱则是无怨无悔的。\n\n所以最终至尊宝回来了，在化身为孙悟空之后，身披金甲圣衣，脚踏七色云彩而来，他实现了紫霞的梦想，只不过加了一层掩饰与牵强。\n\n从现如今这个角度反思紫霞的意中人，我是不太赞同的，童话毕竟是童话，正是这个经典的对白，让多少人活在自己的想象中，我们都渴望对方是个“意中人”的形象，可是我们却忽略乐一个”等价“的观念，你凭什么拥有你的”意中人“，你配得上吗？这不仅让我想起了另外一个命题：“不要去羡慕那些散发光芒的成功者，因为你不知道他背后付出的努力和艰辛”，这其实是一个道理，如果你仅仅是停留在幻想和计划的层面，那么你永远得不到你的”意中人“。\n\n-----\n## **我猜中了开头，可是我猜不着这结局**\n紫霞说猜得到开始，却猜不透这结局。大约直到最后，她也没能明白、没能理解至尊宝的苦心。又或者说，是至尊宝从来也未能真正了解她的心意。我曾经以为，死去的紫霞是最可怜的角色。可是，至尊宝又何尝不可怜呢，他甚至，连伤心的权利也没有了。在紫霞死去的一瞬间，他的心也已经跟着死去了。在他余下的人生里，再也不会有欢笑、快乐，再也不会有那样一个可以在他心里流下眼泪的女孩子。就算取回西经又能如何，心爱的人再也会不来了。就算成佛又能如何，没有了你，整个世界对我来说都毫无意义。\n\n----\n## **《大话》把遗憾和难题抛给了时间**\n\n又一次的时空穿梭后，面对城头男女，孙悟空附身夕阳武士，给出无数人热泪纵横，内心中期盼的最后答案。 \n没有失去过，也永远不能明白得到的快乐。 \n附身后的孙悟空发自内心肺腑地给了女子一记深深长吻，这一吻穿越地老天荒，不再相信自欺欺人的一万年，他那般语气坚决地说出了那三个字。 \n先前拒不让步的夕阳武士，拥抱着爱人幸福陶醉。 \n转身远去的孙悟空了却尘缘心事，消失在大漠黄沙尽头。 \n只是每次在紫霞被刺中或者孙悟空松手的瞬间，还是会心潮如水甚至潸然泪下。 　 \n十年大话，一群人围坐着观看《大话西游》的狂热时代过去了，心底保存的泪水也慢慢尘封直至故事终结。毫无拘束的开怀大笑渐渐沦为一个人的狂欢，难加掩饰的心底苍凉逐渐成了人生重担\n\n---\n## **如今的你，何去何从**\n\n“如今的你，何去何从？”\n“对呀，何去何从”\n\n真的羡慕至尊宝最初为了营救白晶晶，借用外力，使用月光宝盒穿越回500年前，为了解救紫霞，戴上紧箍咒。\n\n而你呢？没有目标，你便是一个游荡的灵魂。\n\n---\n## **加长版加了什么**\n1：紫霞刚出现时在沙漠和雪蛤精，孔雀王的对话以及 他们的拔剑抢婚，与影片中紫霞和至尊宝在 牛魔王婚礼上遇见时雪蛤精，孔雀王和反对结婚作了呼应。\n\n2：约好二更相见，原版是牛夫人出现 ，然后是至尊宝直接被猪八戒和沙师弟拉去救师父，加长版中先是牛夫人出现，然后牛魔王，然后至尊宝\n\n3：牛魔王婚礼时猪八戒和沙僧在小妖堆里跟他们“打成一片”\n\n4：至尊宝被青霞揍晕第二次之后，早上跟紫霞说的那通话“你要让我拿点信物给他看, 你有什么项链啊,首饰啊,金银珠宝啊,月光宝盒啊什么的……”原版的这段声音不是石班瑜所配。新版中，这段声音是重新配音，换上了石班瑜的声音。\n\n5：2K画面的修复\n\n---\n<h2>一切的闹闹哄哄，只是他在水帘洞躲避风沙那晚做的一个梦","source":"_posts/随手记/一切的闹闹哄哄，只是他在水帘洞躲避风沙那晚做的一个梦.md","raw":"---\ntitle: 一切的闹闹哄哄，只是他在水帘洞躲避风沙那晚做的一个梦\ndate: 2017-04-16 01:03:56\ntags: [随手记]\ncategories: 随手记\n---\n\n>送同学走之后，我在路边默默的站了有五分钟，突然觉得我无处可去，有一种深入骨髓的悲哀和无奈，然后我就想起了一个命题，“如今的你，何去何从！”我不知道为什么会突然想到这样一个命题，或许是我们每个人都是至尊宝吧。其实每个人对《大话》的理解都是有所不同的，同样的人在不同的时期认识也会有偏差，就好比我第一次看的时候，笑得腹背抽筋，呲牙咧嘴，第二次看的时候，笑得少了，想的多了，过后便什么感觉也没有了，第三次看得时候，忽然觉得不知道是该哭还是该笑，笑得时候太牵强，哭得时候太尴尬。第四次便是这一次，看完之后觉得有一种无可奈何的悲哀。\n\n<!--More-->\n![一切的闹闹哄哄，只是他在水帘洞躲避风沙那晚做的一个梦。](http://img.blog.csdn.net/20170415205051154?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n-----\n## **对《大话》的解读**\n\n有人说这是一部烂片，也有人说这是一部经典，有人说这是对西游文化的过度消费，有人说这是一代人的反思与成长，有人说阴差阳错的蝴蝶效应，让一个年代的人支撑起了这部超越中国电影水平50年的无厘头式作品，也有人说这是一部笑中带泪，总有一天你走在路上也会像一条狗的悲剧。\n\n一千个人眼中有一千个哈姆雷特，对于一部影视作品，我们无法去评论任何一个人的理解是对还是错，因为经历不同，就像你不能把你的情感强加到别人身上。喜欢一部作品，在不知不觉间你会忽略他的缺点，讨厌一个事物，在不知不觉间就会放大他的短处。我们也不能用我们现代的思维和观念去评判20世纪港台电影的文化和情感。\n\n至尊宝的路线是，有人给他点了三颗痣，拔出来紫青宝剑，开启了与紫霞的交涉，娶亲，戴上紧箍咒，大战牛魔王，西天取经。其实至尊宝这条路线，何尝不是我们的漫漫人生路呢，至尊宝用月光宝盒来寻找500年前的白晶晶，却阴差阳错遇到了紫霞仙子，最后在对白晶晶失而复得之后又得知紫霞在他内心留下的一滴眼泪，最终大彻大悟，明白了舍生取义，参透了生亦何哀，死亦何苦。我们的人生不也一样，总会有各种各样的未知因素影响我们的选择，在努力面对眼前一切之后，最终回头感叹年华，围炉小饮，本来无一物，何处惹尘埃。\n\n对于紫霞和至尊宝的爱情，没有这种体验，是不会懂的，越是深情的人越容易受伤，她放下尊严来爱他，终于他后悔了。在至尊宝的梦里，紫霞仙子说：“飞蛾明明知道前面是火堆，却还义无反顾的扑进去。”她笑一下，接着说：“飞蛾就是这么傻！”而同样的至尊宝，“有一个名字叫紫霞的，你叫七百八十四次“，时光真是个好东西，所谓雕刻时光，是说生活像把钝刀，锉平我们的触觉，而电影则是解毒的重药，它让人突然领悟到，我们的内心要比自己想像的敏感干净得多！你等的那么辛苦，他却陌生到让你心疼。以至于在人生的不同阶段去欣赏《大话》，似乎都能从中找到某些影子，原先是笑，后来是苦，到最后便是沉默了。城墙上的一吻，至尊宝变成了旁观者，他借用夕阳武士的身份和转世的紫霞完成了他告别的深情，于是他的背影，他的离开，寂寞成了“好像一条狗”。\n\n------\n## **他好像一条狗呀！**\n\n-“那个人的样子好怪啊”\n-“我也看到了，他好像条狗啊”\n是呀，英雄的离开，留下的永远只是背影，只不过在至尊宝这里加上了悲剧色彩，加上了人艰不拆的辛酸泪，以至于多少年后才明白了至尊宝转身离开有多难。至尊宝的结局是一个男人的悲怆与无奈。\n\n至尊宝用了月光宝盒来寻找500年前的白晶晶，同时遇到了在她看来，他是她的命中注定的紫霞仙子，直到后来牛魔王的出现，夺走了紫霞，夺走了白晶晶，夺走了至尊宝往日的快乐，他明白他要夺回这一切，可是面对戏剧般的月光宝盒，至尊宝得到的更多是无力和苍白，面对这些无情的现实，幻想一次又一次地破灭。直到最后的关头，至尊宝终于醒悟，靠月光宝盒不行，至尊宝更是没有那个本事，只有成为孙悟空，只有戴上那个金刚圈，他才有能力同牛魔王一较高下。\n\n这是一个极大的讽刺。你想要得到吗？好，那么你必须先放弃至尊宝的身份，你必须做出选择，必须忍受无尽的痛苦，他想要化解时间无尽的仇恨，就必须放弃自己的感情，不是不爱，而是大彻大悟之后的大爱，他必需化身为孙悟空，帮助唐三藏取得经书，化解这世间的恨。\n\n那么至尊宝的放弃是自觉自愿的醒悟吗？不，他并不愿意，但是他必须拯救紫霞，必须化解人间的恨，他别无选择，他必须戴上紧箍咒。虽然成为了孙悟空，成了大英雄，但他对自己的生存状态极度不满。所以在最后，孙悟空将他心中残存的至尊宝的影子幻化作一位夕阳武士，在对现实世界彻底失望后，只能构造一个虚幻的想象来了却这桩心愿，并借武士的口中表达了对自己生存状态的不满，活得好象是一条狗一样。唉，一个男人的悲怆和无奈。\n\n------\n## **那句意中人，满足了多少人的少女心**\n\n“我知道有一天，他会在一个万众瞩目的情况下出现，身披金甲圣衣，脚踏七色云彩来娶我”\n“我的意中人是个盖世英雄，有一天他会踏着云彩来娶我”\n这两句分别是紫霞在牛魔王娶她前的晚上和死前对孙悟空说的，多么经典的台词，以至于现在多少人还幻想着自己的意中人。\n\n进入至尊宝内心的只有两个女人，一个是白晶晶，一个是紫霞，白晶晶问的是“他最喜欢的人是不是我”，紫霞问的是“他跟他的娘子是不是很恩爱呀？”，白晶晶的爱是一种索求的爱，而紫霞的爱则是无怨无悔的。\n\n所以最终至尊宝回来了，在化身为孙悟空之后，身披金甲圣衣，脚踏七色云彩而来，他实现了紫霞的梦想，只不过加了一层掩饰与牵强。\n\n从现如今这个角度反思紫霞的意中人，我是不太赞同的，童话毕竟是童话，正是这个经典的对白，让多少人活在自己的想象中，我们都渴望对方是个“意中人”的形象，可是我们却忽略乐一个”等价“的观念，你凭什么拥有你的”意中人“，你配得上吗？这不仅让我想起了另外一个命题：“不要去羡慕那些散发光芒的成功者，因为你不知道他背后付出的努力和艰辛”，这其实是一个道理，如果你仅仅是停留在幻想和计划的层面，那么你永远得不到你的”意中人“。\n\n-----\n## **我猜中了开头，可是我猜不着这结局**\n紫霞说猜得到开始，却猜不透这结局。大约直到最后，她也没能明白、没能理解至尊宝的苦心。又或者说，是至尊宝从来也未能真正了解她的心意。我曾经以为，死去的紫霞是最可怜的角色。可是，至尊宝又何尝不可怜呢，他甚至，连伤心的权利也没有了。在紫霞死去的一瞬间，他的心也已经跟着死去了。在他余下的人生里，再也不会有欢笑、快乐，再也不会有那样一个可以在他心里流下眼泪的女孩子。就算取回西经又能如何，心爱的人再也会不来了。就算成佛又能如何，没有了你，整个世界对我来说都毫无意义。\n\n----\n## **《大话》把遗憾和难题抛给了时间**\n\n又一次的时空穿梭后，面对城头男女，孙悟空附身夕阳武士，给出无数人热泪纵横，内心中期盼的最后答案。 \n没有失去过，也永远不能明白得到的快乐。 \n附身后的孙悟空发自内心肺腑地给了女子一记深深长吻，这一吻穿越地老天荒，不再相信自欺欺人的一万年，他那般语气坚决地说出了那三个字。 \n先前拒不让步的夕阳武士，拥抱着爱人幸福陶醉。 \n转身远去的孙悟空了却尘缘心事，消失在大漠黄沙尽头。 \n只是每次在紫霞被刺中或者孙悟空松手的瞬间，还是会心潮如水甚至潸然泪下。 　 \n十年大话，一群人围坐着观看《大话西游》的狂热时代过去了，心底保存的泪水也慢慢尘封直至故事终结。毫无拘束的开怀大笑渐渐沦为一个人的狂欢，难加掩饰的心底苍凉逐渐成了人生重担\n\n---\n## **如今的你，何去何从**\n\n“如今的你，何去何从？”\n“对呀，何去何从”\n\n真的羡慕至尊宝最初为了营救白晶晶，借用外力，使用月光宝盒穿越回500年前，为了解救紫霞，戴上紧箍咒。\n\n而你呢？没有目标，你便是一个游荡的灵魂。\n\n---\n## **加长版加了什么**\n1：紫霞刚出现时在沙漠和雪蛤精，孔雀王的对话以及 他们的拔剑抢婚，与影片中紫霞和至尊宝在 牛魔王婚礼上遇见时雪蛤精，孔雀王和反对结婚作了呼应。\n\n2：约好二更相见，原版是牛夫人出现 ，然后是至尊宝直接被猪八戒和沙师弟拉去救师父，加长版中先是牛夫人出现，然后牛魔王，然后至尊宝\n\n3：牛魔王婚礼时猪八戒和沙僧在小妖堆里跟他们“打成一片”\n\n4：至尊宝被青霞揍晕第二次之后，早上跟紫霞说的那通话“你要让我拿点信物给他看, 你有什么项链啊,首饰啊,金银珠宝啊,月光宝盒啊什么的……”原版的这段声音不是石班瑜所配。新版中，这段声音是重新配音，换上了石班瑜的声音。\n\n5：2K画面的修复\n\n---\n<h2>一切的闹闹哄哄，只是他在水帘洞躲避风沙那晚做的一个梦","slug":"随手记/一切的闹闹哄哄，只是他在水帘洞躲避风沙那晚做的一个梦","published":1,"updated":"2017-12-19T02:54:56.791Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjbd1r26s001fkxuwvfmohrrx","content":"<blockquote>\n<p>送同学走之后，我在路边默默的站了有五分钟，突然觉得我无处可去，有一种深入骨髓的悲哀和无奈，然后我就想起了一个命题，“如今的你，何去何从！”我不知道为什么会突然想到这样一个命题，或许是我们每个人都是至尊宝吧。其实每个人对《大话》的理解都是有所不同的，同样的人在不同的时期认识也会有偏差，就好比我第一次看的时候，笑得腹背抽筋，呲牙咧嘴，第二次看的时候，笑得少了，想的多了，过后便什么感觉也没有了，第三次看得时候，忽然觉得不知道是该哭还是该笑，笑得时候太牵强，哭得时候太尴尬。第四次便是这一次，看完之后觉得有一种无可奈何的悲哀。</p>\n</blockquote>\n<a id=\"more\"></a>\n<p><img src=\"http://img.blog.csdn.net/20170415205051154?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"一切的闹闹哄哄，只是他在水帘洞躲避风沙那晚做的一个梦。\"></p>\n<hr>\n<h2 id=\"对《大话》的解读\"><a href=\"#对《大话》的解读\" class=\"headerlink\" title=\"对《大话》的解读\"></a><strong>对《大话》的解读</strong></h2><p>有人说这是一部烂片，也有人说这是一部经典，有人说这是对西游文化的过度消费，有人说这是一代人的反思与成长，有人说阴差阳错的蝴蝶效应，让一个年代的人支撑起了这部超越中国电影水平50年的无厘头式作品，也有人说这是一部笑中带泪，总有一天你走在路上也会像一条狗的悲剧。</p>\n<p>一千个人眼中有一千个哈姆雷特，对于一部影视作品，我们无法去评论任何一个人的理解是对还是错，因为经历不同，就像你不能把你的情感强加到别人身上。喜欢一部作品，在不知不觉间你会忽略他的缺点，讨厌一个事物，在不知不觉间就会放大他的短处。我们也不能用我们现代的思维和观念去评判20世纪港台电影的文化和情感。</p>\n<p>至尊宝的路线是，有人给他点了三颗痣，拔出来紫青宝剑，开启了与紫霞的交涉，娶亲，戴上紧箍咒，大战牛魔王，西天取经。其实至尊宝这条路线，何尝不是我们的漫漫人生路呢，至尊宝用月光宝盒来寻找500年前的白晶晶，却阴差阳错遇到了紫霞仙子，最后在对白晶晶失而复得之后又得知紫霞在他内心留下的一滴眼泪，最终大彻大悟，明白了舍生取义，参透了生亦何哀，死亦何苦。我们的人生不也一样，总会有各种各样的未知因素影响我们的选择，在努力面对眼前一切之后，最终回头感叹年华，围炉小饮，本来无一物，何处惹尘埃。</p>\n<p>对于紫霞和至尊宝的爱情，没有这种体验，是不会懂的，越是深情的人越容易受伤，她放下尊严来爱他，终于他后悔了。在至尊宝的梦里，紫霞仙子说：“飞蛾明明知道前面是火堆，却还义无反顾的扑进去。”她笑一下，接着说：“飞蛾就是这么傻！”而同样的至尊宝，“有一个名字叫紫霞的，你叫七百八十四次“，时光真是个好东西，所谓雕刻时光，是说生活像把钝刀，锉平我们的触觉，而电影则是解毒的重药，它让人突然领悟到，我们的内心要比自己想像的敏感干净得多！你等的那么辛苦，他却陌生到让你心疼。以至于在人生的不同阶段去欣赏《大话》，似乎都能从中找到某些影子，原先是笑，后来是苦，到最后便是沉默了。城墙上的一吻，至尊宝变成了旁观者，他借用夕阳武士的身份和转世的紫霞完成了他告别的深情，于是他的背影，他的离开，寂寞成了“好像一条狗”。</p>\n<hr>\n<h2 id=\"他好像一条狗呀！\"><a href=\"#他好像一条狗呀！\" class=\"headerlink\" title=\"他好像一条狗呀！\"></a><strong>他好像一条狗呀！</strong></h2><p>-“那个人的样子好怪啊”<br>-“我也看到了，他好像条狗啊”<br>是呀，英雄的离开，留下的永远只是背影，只不过在至尊宝这里加上了悲剧色彩，加上了人艰不拆的辛酸泪，以至于多少年后才明白了至尊宝转身离开有多难。至尊宝的结局是一个男人的悲怆与无奈。</p>\n<p>至尊宝用了月光宝盒来寻找500年前的白晶晶，同时遇到了在她看来，他是她的命中注定的紫霞仙子，直到后来牛魔王的出现，夺走了紫霞，夺走了白晶晶，夺走了至尊宝往日的快乐，他明白他要夺回这一切，可是面对戏剧般的月光宝盒，至尊宝得到的更多是无力和苍白，面对这些无情的现实，幻想一次又一次地破灭。直到最后的关头，至尊宝终于醒悟，靠月光宝盒不行，至尊宝更是没有那个本事，只有成为孙悟空，只有戴上那个金刚圈，他才有能力同牛魔王一较高下。</p>\n<p>这是一个极大的讽刺。你想要得到吗？好，那么你必须先放弃至尊宝的身份，你必须做出选择，必须忍受无尽的痛苦，他想要化解时间无尽的仇恨，就必须放弃自己的感情，不是不爱，而是大彻大悟之后的大爱，他必需化身为孙悟空，帮助唐三藏取得经书，化解这世间的恨。</p>\n<p>那么至尊宝的放弃是自觉自愿的醒悟吗？不，他并不愿意，但是他必须拯救紫霞，必须化解人间的恨，他别无选择，他必须戴上紧箍咒。虽然成为了孙悟空，成了大英雄，但他对自己的生存状态极度不满。所以在最后，孙悟空将他心中残存的至尊宝的影子幻化作一位夕阳武士，在对现实世界彻底失望后，只能构造一个虚幻的想象来了却这桩心愿，并借武士的口中表达了对自己生存状态的不满，活得好象是一条狗一样。唉，一个男人的悲怆和无奈。</p>\n<hr>\n<h2 id=\"那句意中人，满足了多少人的少女心\"><a href=\"#那句意中人，满足了多少人的少女心\" class=\"headerlink\" title=\"那句意中人，满足了多少人的少女心\"></a><strong>那句意中人，满足了多少人的少女心</strong></h2><p>“我知道有一天，他会在一个万众瞩目的情况下出现，身披金甲圣衣，脚踏七色云彩来娶我”<br>“我的意中人是个盖世英雄，有一天他会踏着云彩来娶我”<br>这两句分别是紫霞在牛魔王娶她前的晚上和死前对孙悟空说的，多么经典的台词，以至于现在多少人还幻想着自己的意中人。</p>\n<p>进入至尊宝内心的只有两个女人，一个是白晶晶，一个是紫霞，白晶晶问的是“他最喜欢的人是不是我”，紫霞问的是“他跟他的娘子是不是很恩爱呀？”，白晶晶的爱是一种索求的爱，而紫霞的爱则是无怨无悔的。</p>\n<p>所以最终至尊宝回来了，在化身为孙悟空之后，身披金甲圣衣，脚踏七色云彩而来，他实现了紫霞的梦想，只不过加了一层掩饰与牵强。</p>\n<p>从现如今这个角度反思紫霞的意中人，我是不太赞同的，童话毕竟是童话，正是这个经典的对白，让多少人活在自己的想象中，我们都渴望对方是个“意中人”的形象，可是我们却忽略乐一个”等价“的观念，你凭什么拥有你的”意中人“，你配得上吗？这不仅让我想起了另外一个命题：“不要去羡慕那些散发光芒的成功者，因为你不知道他背后付出的努力和艰辛”，这其实是一个道理，如果你仅仅是停留在幻想和计划的层面，那么你永远得不到你的”意中人“。</p>\n<hr>\n<h2 id=\"我猜中了开头，可是我猜不着这结局\"><a href=\"#我猜中了开头，可是我猜不着这结局\" class=\"headerlink\" title=\"我猜中了开头，可是我猜不着这结局\"></a><strong>我猜中了开头，可是我猜不着这结局</strong></h2><p>紫霞说猜得到开始，却猜不透这结局。大约直到最后，她也没能明白、没能理解至尊宝的苦心。又或者说，是至尊宝从来也未能真正了解她的心意。我曾经以为，死去的紫霞是最可怜的角色。可是，至尊宝又何尝不可怜呢，他甚至，连伤心的权利也没有了。在紫霞死去的一瞬间，他的心也已经跟着死去了。在他余下的人生里，再也不会有欢笑、快乐，再也不会有那样一个可以在他心里流下眼泪的女孩子。就算取回西经又能如何，心爱的人再也会不来了。就算成佛又能如何，没有了你，整个世界对我来说都毫无意义。</p>\n<hr>\n<h2 id=\"《大话》把遗憾和难题抛给了时间\"><a href=\"#《大话》把遗憾和难题抛给了时间\" class=\"headerlink\" title=\"《大话》把遗憾和难题抛给了时间\"></a><strong>《大话》把遗憾和难题抛给了时间</strong></h2><p>又一次的时空穿梭后，面对城头男女，孙悟空附身夕阳武士，给出无数人热泪纵横，内心中期盼的最后答案。<br>没有失去过，也永远不能明白得到的快乐。<br>附身后的孙悟空发自内心肺腑地给了女子一记深深长吻，这一吻穿越地老天荒，不再相信自欺欺人的一万年，他那般语气坚决地说出了那三个字。<br>先前拒不让步的夕阳武士，拥抱着爱人幸福陶醉。<br>转身远去的孙悟空了却尘缘心事，消失在大漠黄沙尽头。<br>只是每次在紫霞被刺中或者孙悟空松手的瞬间，还是会心潮如水甚至潸然泪下。 　<br>十年大话，一群人围坐着观看《大话西游》的狂热时代过去了，心底保存的泪水也慢慢尘封直至故事终结。毫无拘束的开怀大笑渐渐沦为一个人的狂欢，难加掩饰的心底苍凉逐渐成了人生重担</p>\n<hr>\n<h2 id=\"如今的你，何去何从\"><a href=\"#如今的你，何去何从\" class=\"headerlink\" title=\"如今的你，何去何从\"></a><strong>如今的你，何去何从</strong></h2><p>“如今的你，何去何从？”<br>“对呀，何去何从”</p>\n<p>真的羡慕至尊宝最初为了营救白晶晶，借用外力，使用月光宝盒穿越回500年前，为了解救紫霞，戴上紧箍咒。</p>\n<p>而你呢？没有目标，你便是一个游荡的灵魂。</p>\n<hr>\n<h2 id=\"加长版加了什么\"><a href=\"#加长版加了什么\" class=\"headerlink\" title=\"加长版加了什么\"></a><strong>加长版加了什么</strong></h2><p>1：紫霞刚出现时在沙漠和雪蛤精，孔雀王的对话以及 他们的拔剑抢婚，与影片中紫霞和至尊宝在 牛魔王婚礼上遇见时雪蛤精，孔雀王和反对结婚作了呼应。</p>\n<p>2：约好二更相见，原版是牛夫人出现 ，然后是至尊宝直接被猪八戒和沙师弟拉去救师父，加长版中先是牛夫人出现，然后牛魔王，然后至尊宝</p>\n<p>3：牛魔王婚礼时猪八戒和沙僧在小妖堆里跟他们“打成一片”</p>\n<p>4：至尊宝被青霞揍晕第二次之后，早上跟紫霞说的那通话“你要让我拿点信物给他看, 你有什么项链啊,首饰啊,金银珠宝啊,月光宝盒啊什么的……”原版的这段声音不是石班瑜所配。新版中，这段声音是重新配音，换上了石班瑜的声音。</p>\n<p>5：2K画面的修复</p>\n<hr>\n<p></p><h2>一切的闹闹哄哄，只是他在水帘洞躲避风沙那晚做的一个梦<p></p>\n</h2>","site":{"data":{}},"excerpt":"<blockquote>\n<p>送同学走之后，我在路边默默的站了有五分钟，突然觉得我无处可去，有一种深入骨髓的悲哀和无奈，然后我就想起了一个命题，“如今的你，何去何从！”我不知道为什么会突然想到这样一个命题，或许是我们每个人都是至尊宝吧。其实每个人对《大话》的理解都是有所不同的，同样的人在不同的时期认识也会有偏差，就好比我第一次看的时候，笑得腹背抽筋，呲牙咧嘴，第二次看的时候，笑得少了，想的多了，过后便什么感觉也没有了，第三次看得时候，忽然觉得不知道是该哭还是该笑，笑得时候太牵强，哭得时候太尴尬。第四次便是这一次，看完之后觉得有一种无可奈何的悲哀。</p>\n</blockquote>","more":"<p><img src=\"http://img.blog.csdn.net/20170415205051154?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"一切的闹闹哄哄，只是他在水帘洞躲避风沙那晚做的一个梦。\"></p>\n<hr>\n<h2 id=\"对《大话》的解读\"><a href=\"#对《大话》的解读\" class=\"headerlink\" title=\"对《大话》的解读\"></a><strong>对《大话》的解读</strong></h2><p>有人说这是一部烂片，也有人说这是一部经典，有人说这是对西游文化的过度消费，有人说这是一代人的反思与成长，有人说阴差阳错的蝴蝶效应，让一个年代的人支撑起了这部超越中国电影水平50年的无厘头式作品，也有人说这是一部笑中带泪，总有一天你走在路上也会像一条狗的悲剧。</p>\n<p>一千个人眼中有一千个哈姆雷特，对于一部影视作品，我们无法去评论任何一个人的理解是对还是错，因为经历不同，就像你不能把你的情感强加到别人身上。喜欢一部作品，在不知不觉间你会忽略他的缺点，讨厌一个事物，在不知不觉间就会放大他的短处。我们也不能用我们现代的思维和观念去评判20世纪港台电影的文化和情感。</p>\n<p>至尊宝的路线是，有人给他点了三颗痣，拔出来紫青宝剑，开启了与紫霞的交涉，娶亲，戴上紧箍咒，大战牛魔王，西天取经。其实至尊宝这条路线，何尝不是我们的漫漫人生路呢，至尊宝用月光宝盒来寻找500年前的白晶晶，却阴差阳错遇到了紫霞仙子，最后在对白晶晶失而复得之后又得知紫霞在他内心留下的一滴眼泪，最终大彻大悟，明白了舍生取义，参透了生亦何哀，死亦何苦。我们的人生不也一样，总会有各种各样的未知因素影响我们的选择，在努力面对眼前一切之后，最终回头感叹年华，围炉小饮，本来无一物，何处惹尘埃。</p>\n<p>对于紫霞和至尊宝的爱情，没有这种体验，是不会懂的，越是深情的人越容易受伤，她放下尊严来爱他，终于他后悔了。在至尊宝的梦里，紫霞仙子说：“飞蛾明明知道前面是火堆，却还义无反顾的扑进去。”她笑一下，接着说：“飞蛾就是这么傻！”而同样的至尊宝，“有一个名字叫紫霞的，你叫七百八十四次“，时光真是个好东西，所谓雕刻时光，是说生活像把钝刀，锉平我们的触觉，而电影则是解毒的重药，它让人突然领悟到，我们的内心要比自己想像的敏感干净得多！你等的那么辛苦，他却陌生到让你心疼。以至于在人生的不同阶段去欣赏《大话》，似乎都能从中找到某些影子，原先是笑，后来是苦，到最后便是沉默了。城墙上的一吻，至尊宝变成了旁观者，他借用夕阳武士的身份和转世的紫霞完成了他告别的深情，于是他的背影，他的离开，寂寞成了“好像一条狗”。</p>\n<hr>\n<h2 id=\"他好像一条狗呀！\"><a href=\"#他好像一条狗呀！\" class=\"headerlink\" title=\"他好像一条狗呀！\"></a><strong>他好像一条狗呀！</strong></h2><p>-“那个人的样子好怪啊”<br>-“我也看到了，他好像条狗啊”<br>是呀，英雄的离开，留下的永远只是背影，只不过在至尊宝这里加上了悲剧色彩，加上了人艰不拆的辛酸泪，以至于多少年后才明白了至尊宝转身离开有多难。至尊宝的结局是一个男人的悲怆与无奈。</p>\n<p>至尊宝用了月光宝盒来寻找500年前的白晶晶，同时遇到了在她看来，他是她的命中注定的紫霞仙子，直到后来牛魔王的出现，夺走了紫霞，夺走了白晶晶，夺走了至尊宝往日的快乐，他明白他要夺回这一切，可是面对戏剧般的月光宝盒，至尊宝得到的更多是无力和苍白，面对这些无情的现实，幻想一次又一次地破灭。直到最后的关头，至尊宝终于醒悟，靠月光宝盒不行，至尊宝更是没有那个本事，只有成为孙悟空，只有戴上那个金刚圈，他才有能力同牛魔王一较高下。</p>\n<p>这是一个极大的讽刺。你想要得到吗？好，那么你必须先放弃至尊宝的身份，你必须做出选择，必须忍受无尽的痛苦，他想要化解时间无尽的仇恨，就必须放弃自己的感情，不是不爱，而是大彻大悟之后的大爱，他必需化身为孙悟空，帮助唐三藏取得经书，化解这世间的恨。</p>\n<p>那么至尊宝的放弃是自觉自愿的醒悟吗？不，他并不愿意，但是他必须拯救紫霞，必须化解人间的恨，他别无选择，他必须戴上紧箍咒。虽然成为了孙悟空，成了大英雄，但他对自己的生存状态极度不满。所以在最后，孙悟空将他心中残存的至尊宝的影子幻化作一位夕阳武士，在对现实世界彻底失望后，只能构造一个虚幻的想象来了却这桩心愿，并借武士的口中表达了对自己生存状态的不满，活得好象是一条狗一样。唉，一个男人的悲怆和无奈。</p>\n<hr>\n<h2 id=\"那句意中人，满足了多少人的少女心\"><a href=\"#那句意中人，满足了多少人的少女心\" class=\"headerlink\" title=\"那句意中人，满足了多少人的少女心\"></a><strong>那句意中人，满足了多少人的少女心</strong></h2><p>“我知道有一天，他会在一个万众瞩目的情况下出现，身披金甲圣衣，脚踏七色云彩来娶我”<br>“我的意中人是个盖世英雄，有一天他会踏着云彩来娶我”<br>这两句分别是紫霞在牛魔王娶她前的晚上和死前对孙悟空说的，多么经典的台词，以至于现在多少人还幻想着自己的意中人。</p>\n<p>进入至尊宝内心的只有两个女人，一个是白晶晶，一个是紫霞，白晶晶问的是“他最喜欢的人是不是我”，紫霞问的是“他跟他的娘子是不是很恩爱呀？”，白晶晶的爱是一种索求的爱，而紫霞的爱则是无怨无悔的。</p>\n<p>所以最终至尊宝回来了，在化身为孙悟空之后，身披金甲圣衣，脚踏七色云彩而来，他实现了紫霞的梦想，只不过加了一层掩饰与牵强。</p>\n<p>从现如今这个角度反思紫霞的意中人，我是不太赞同的，童话毕竟是童话，正是这个经典的对白，让多少人活在自己的想象中，我们都渴望对方是个“意中人”的形象，可是我们却忽略乐一个”等价“的观念，你凭什么拥有你的”意中人“，你配得上吗？这不仅让我想起了另外一个命题：“不要去羡慕那些散发光芒的成功者，因为你不知道他背后付出的努力和艰辛”，这其实是一个道理，如果你仅仅是停留在幻想和计划的层面，那么你永远得不到你的”意中人“。</p>\n<hr>\n<h2 id=\"我猜中了开头，可是我猜不着这结局\"><a href=\"#我猜中了开头，可是我猜不着这结局\" class=\"headerlink\" title=\"我猜中了开头，可是我猜不着这结局\"></a><strong>我猜中了开头，可是我猜不着这结局</strong></h2><p>紫霞说猜得到开始，却猜不透这结局。大约直到最后，她也没能明白、没能理解至尊宝的苦心。又或者说，是至尊宝从来也未能真正了解她的心意。我曾经以为，死去的紫霞是最可怜的角色。可是，至尊宝又何尝不可怜呢，他甚至，连伤心的权利也没有了。在紫霞死去的一瞬间，他的心也已经跟着死去了。在他余下的人生里，再也不会有欢笑、快乐，再也不会有那样一个可以在他心里流下眼泪的女孩子。就算取回西经又能如何，心爱的人再也会不来了。就算成佛又能如何，没有了你，整个世界对我来说都毫无意义。</p>\n<hr>\n<h2 id=\"《大话》把遗憾和难题抛给了时间\"><a href=\"#《大话》把遗憾和难题抛给了时间\" class=\"headerlink\" title=\"《大话》把遗憾和难题抛给了时间\"></a><strong>《大话》把遗憾和难题抛给了时间</strong></h2><p>又一次的时空穿梭后，面对城头男女，孙悟空附身夕阳武士，给出无数人热泪纵横，内心中期盼的最后答案。<br>没有失去过，也永远不能明白得到的快乐。<br>附身后的孙悟空发自内心肺腑地给了女子一记深深长吻，这一吻穿越地老天荒，不再相信自欺欺人的一万年，他那般语气坚决地说出了那三个字。<br>先前拒不让步的夕阳武士，拥抱着爱人幸福陶醉。<br>转身远去的孙悟空了却尘缘心事，消失在大漠黄沙尽头。<br>只是每次在紫霞被刺中或者孙悟空松手的瞬间，还是会心潮如水甚至潸然泪下。 　<br>十年大话，一群人围坐着观看《大话西游》的狂热时代过去了，心底保存的泪水也慢慢尘封直至故事终结。毫无拘束的开怀大笑渐渐沦为一个人的狂欢，难加掩饰的心底苍凉逐渐成了人生重担</p>\n<hr>\n<h2 id=\"如今的你，何去何从\"><a href=\"#如今的你，何去何从\" class=\"headerlink\" title=\"如今的你，何去何从\"></a><strong>如今的你，何去何从</strong></h2><p>“如今的你，何去何从？”<br>“对呀，何去何从”</p>\n<p>真的羡慕至尊宝最初为了营救白晶晶，借用外力，使用月光宝盒穿越回500年前，为了解救紫霞，戴上紧箍咒。</p>\n<p>而你呢？没有目标，你便是一个游荡的灵魂。</p>\n<hr>\n<h2 id=\"加长版加了什么\"><a href=\"#加长版加了什么\" class=\"headerlink\" title=\"加长版加了什么\"></a><strong>加长版加了什么</strong></h2><p>1：紫霞刚出现时在沙漠和雪蛤精，孔雀王的对话以及 他们的拔剑抢婚，与影片中紫霞和至尊宝在 牛魔王婚礼上遇见时雪蛤精，孔雀王和反对结婚作了呼应。</p>\n<p>2：约好二更相见，原版是牛夫人出现 ，然后是至尊宝直接被猪八戒和沙师弟拉去救师父，加长版中先是牛夫人出现，然后牛魔王，然后至尊宝</p>\n<p>3：牛魔王婚礼时猪八戒和沙僧在小妖堆里跟他们“打成一片”</p>\n<p>4：至尊宝被青霞揍晕第二次之后，早上跟紫霞说的那通话“你要让我拿点信物给他看, 你有什么项链啊,首饰啊,金银珠宝啊,月光宝盒啊什么的……”原版的这段声音不是石班瑜所配。新版中，这段声音是重新配音，换上了石班瑜的声音。</p>\n<p>5：2K画面的修复</p>\n<hr>\n<p></p><h2>一切的闹闹哄哄，只是他在水帘洞躲避风沙那晚做的一个梦<p></p>\n</h2>"},{"title":"别了青春与流年，遇见下一个自己","date":"2016-12-20T16:35:00.000Z","_content":"\n<font size=3>如果说岁月是年轮，我们便是推行者，如果说成长是一场华丽的蜕变，我们便是领舞者。一路走来，太多不易，告别青春的年少轻狂，我们成了岁月里被磨平的棱角，静静的守在属于自己的一亩三分地。</font>\n\n\n<!--More-->\n\n## **2016-时间是长了脚的妖怪，跑的飞快**\n\n&nbsp;&nbsp;&nbsp;&nbsp;四年时光，匆匆而过，沈阳占据了我23岁之前的太多第一次，第一次一个人一包行李，第一次21个小时的硬座，第一次坐地铁，第一次谈恋爱，第一次分手，第一次旅行，第一次坐摩天轮，第一次吃棉花糖，第一次看电影，第一次接触电脑，第一次......\n&nbsp;&nbsp;&nbsp;&nbsp;时间是长了脚的妖怪，跑的飞快，只是好像后来我们都离开，各自生活在喧嚣未来，当时的遗憾在回忆肆虐的某些时段，重新打开，又好象我们同时都在。\n&nbsp;&nbsp;&nbsp;&nbsp;有人说，谈过恋爱，分过手，挂过科，拿过奖学金，当过学生干部的大学才是完美的，那么我想我还是比较幸运的，回顾我的大学生活，除了两件我极力想做的事情没有完成之外，经历了太多，谈过恋爱，分过手，当过学生干部，拿过奖学金，参加过各种志愿活动，也做过校园代理，被坑过，被骗过，也干过兼职，做过外包，送过外卖，发过传单，在这整个过程中，认识了不少人，见过不少事，看明白了不少的社会道理，看清了多少人的虚情假意，感谢那一路让我经历成长的人。  \n&nbsp;&nbsp;&nbsp;&nbsp;大一是我大学生活里最快乐的一年，那时的我们很单纯。只是后来，大家都变了。\n\n![这里写图片描述](http://img.blog.csdn.net/20161220223409601)\n\n-----\n\n## **2016-剑未配好，已出江湖，来一场说走就走的北漂**\n&nbsp;&nbsp;&nbsp;&nbsp;该到来的还是会到来，虽然对于工作我是做好了准备，但是还是有点措手不及。\n\n- 七月，别了流年\n&nbsp;&nbsp;&nbsp;&nbsp;那是七月，我的心情迫切的像火辣辣的太阳，拉着行李，从大学的门前离开，没有回头，虽然这里有我牵挂的人，有我念着的事，但我还是把更多的希望寄托在充满魔性的首都，因为我相信这里是梦会是开始的地方，于是在朋友的帮助下，我开启了我的北漂生活。\n\n- 广联达\n&nbsp;&nbsp;&nbsp;&nbsp;我来北京的第一家公司是广联达，建筑行业国内算是龙头老大了，虽然在互联网行业不是太牛逼，但对于一个初出茅庐的我还是够我学习和经历了，而且凑巧的是公司是我一个八几年的校友创立的，只是这和我没有半毛钱关系，在那的三个月里，我连个人影都没有见过。\n&nbsp;&nbsp;&nbsp;&nbsp;后来的后来我选择了离开，不是公司不好，不是带我的师父不优秀，不是同事不牛逼，只是我感觉那里不适合我。\n&nbsp;&nbsp;&nbsp;&nbsp;我的师父是项目组组长吧，人有点娘娘腔，别人都叫他梅梅，但是对我们特别好，他技术也十分厉害，离开的时候和师父聊天，他说在公司是P3和P4的技术双认证，是一个技术架构师，自己带着团队几个月为公司写了一个云测试平台，现在更到3.x版本了吧。我个人是十分佩服我师父的，为人低调，技术够强，还有好人缘。\n&nbsp;&nbsp;&nbsp;&nbsp;在公司的那段时候里，我主要做的是一个以课题形式展示的数据分析平台，用到的技术无非就是大学里学的那些，那个时候和另外一个同事还吹牛逼说咱也是架构师了，这仅仅是因为自己画了个水的一逼的图\n![这里写图片描述](http://img.blog.csdn.net/20161220230511202?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n&nbsp;&nbsp;&nbsp;&nbsp;哈哈，如果这幅图出自架构师之手，就是系统架构了，可是出自我们这等毛小子之手就是闹着玩了吧。就好比以国家的名义去挖墓，就是考古，以个人名义去挖墓就是盗墓了。\n\n- 昌平线，煎熬\n&nbsp;&nbsp;&nbsp;&nbsp;西二旗是中国最堵得一个地铁站了，大家都说后村厂路堵车十分钟，中国互联网经济停滞2小时。\n&nbsp;&nbsp;&nbsp;&nbsp;昌平线是北漂人的聚集地了，不是因为别的，是因为这条线路上的房租便宜 ，那个时候我就盘踞在沙河高教园旁边的东沙屯村里，每月800元的房租还是负担的起的，除了交通不便之外，一切还都可以接受，毕竟你是在北漂。\n\n----\n## **2016-别了流年，是现在的我**\n&nbsp;&nbsp;&nbsp;&nbsp;九月末我面试了现在所在的公司，离开了广联达，不是因为它不优秀，它不好，只是因为那里现在还不适合我，在我的棱角被磨平之前，我想出去闯一闯。\n&nbsp;&nbsp;&nbsp;&nbsp;可能是我所在部门的原因，我觉得特别懒散，感觉大家都是在混日子，每天改那么点bug，每天更新一点小功能，或者这就是大公司的尴尬，或者说转型之中的公司的短板吧，大家都沉浸在以前的辉煌之中，没有创造力，没有新奇的想法，没有交流的冲动，没有那种干劲。于是我选择了离开，我想先让我去经历一番我想要的工作与生活，等我累了，说不定我就会想念这种状态了。\n&nbsp;&nbsp;&nbsp;&nbsp;现在所在的是一个创业公司，像我想象中一样，大家窝在一个不大的办公司，交流与合作，为了梦想一起努力着，很开心。\n&nbsp;&nbsp;&nbsp;&nbsp;在这里我接触到了更多知识，技术的，做人的，交流的，至今我脑海中还清晰的记着那天赵总的一句话：读书要有收获，至少要涨气场。\n&nbsp;&nbsp;&nbsp;&nbsp;新的环境里我接触学习了Docker，ELK，重新学习了一些机器学习的算法知识。于是在我的CSDN博客中创建了两个技术专栏，由于刚刚接触，写的也不够深入，不过我会努力的。\n&nbsp;&nbsp;&nbsp;&nbsp;Docker江湖：http://blog.csdn.net/column/details/13159.html\n &nbsp;&nbsp;&nbsp;&nbsp;ELK从入门到放弃：http://blog.csdn.net/column/details/13079.html\n \n&nbsp;&nbsp;&nbsp;&nbsp;认认真真经历才能好好成长。\n\n---\n## **2016-我在CSDN的收获**\n - 鲍大神\n&nbsp;&nbsp;&nbsp;&nbsp;开始在CSDN上写博客是大一的时候，是一个牛逼的学长带我走上了这条\"不归路\"，谢谢<a href=\"http://blog.csdn.net/baolibin528\">鲍大神</a>这一路的指导与传授，一直以来，他都是我的榜样。我也努力赶上他，只可惜看到的永远都是背影。\n\n- 梦姐姐\n&nbsp;&nbsp;&nbsp;&nbsp;八月份的时候偶然的机会认识梦姐姐，做了博乐，后来也申请并通过了CSDN博客专家。\n\n- 结识技术爱好者\n&nbsp;&nbsp;&nbsp;&nbsp;其实相比这些更重要的是通过CSDN所认识的每一个技术爱好者，可以说CSDN是国内的程序员的社交平台了。感觉那些给我留言提问我的人，可能有些疑问还是没有帮你们解决，只是我个人能力有限，不像郭神，鸿洋大神技术功底深厚。在这个平台之上，我也认识到了自己的许多不足和技术缺点，在阅读博客的过程中，也学到了不少东西。\n\n&nbsp;&nbsp;&nbsp;&nbsp;谢谢你一路陪我成长，你若不离，我定不弃。\n\n----\n## **2016-开始commit我的github**\n&nbsp;&nbsp;&nbsp;&nbsp;有人说开源垃圾，有人说开源缩减了开发的成本和时间，不管怎样，开源是一种趋势，而且势头不会减弱，很荣幸我也投入了开源的大军，即使现在我还是一个蝼蚁。\n&nbsp;&nbsp;&nbsp;&nbsp;我的github：https://github.com/thinkgamer\n![这里写图片描述](http://img.blog.csdn.net/20161221000236962?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n----\n## **2016-杂乱无章**\n&nbsp;&nbsp;&nbsp;&nbsp;这一年，从一个初出茅庐的蝼蚁一步步成长，一个个经历，我给交了一份70分的答卷，我没有让我的父母和亲人失望，我没有让我的老师失望，我没有让我喜欢的人失望，我也没有让曾经看不起我的人失望，只是我让自己失望了。\n&nbsp;&nbsp;&nbsp;&nbsp;有些东西我没有去争取，有些机会我没有把握，有些冲动我失了控。但正是这些完美的不完美的，才让你有更大的劲头去前进。\n\n----\n## **2017-下一个自己**\n&nbsp;&nbsp;&nbsp;&nbsp;时间不会因为你的遗憾而停留，我们能做的就是把每一天都当成最后一天来过。\n&nbsp;&nbsp;&nbsp;&nbsp;2017，我要完成：\n\n -  一个安卓APP和对应的Web \n -  小说《这夏未眠》\n - 发表社区划分论文\n - 深入学习Scala和Spark\n - 掌握一个深度学习框架（eg：Caffe）\n - 跟进研究Hadoop家族的最近版本，并形成文档\n - 换一台Mackbook Pro\n - 攒够100K+\n\n&nbsp;&nbsp;&nbsp;&nbsp;感谢这一路有你，加油！\n\n----\n个人微信公众号，欢迎关注\n![个人微信公众号，欢迎关注](http://img.blog.csdn.net/20161221002809051?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)","source":"_posts/随手记/别了青春与流年，遇见下一个自己.md","raw":"---\ntitle: 别了青春与流年，遇见下一个自己\ndate: 2016-12-21 00:35\ntags: [随手记]\ncategories: 随手记\t\t\n---\n\n<font size=3>如果说岁月是年轮，我们便是推行者，如果说成长是一场华丽的蜕变，我们便是领舞者。一路走来，太多不易，告别青春的年少轻狂，我们成了岁月里被磨平的棱角，静静的守在属于自己的一亩三分地。</font>\n\n\n<!--More-->\n\n## **2016-时间是长了脚的妖怪，跑的飞快**\n\n&nbsp;&nbsp;&nbsp;&nbsp;四年时光，匆匆而过，沈阳占据了我23岁之前的太多第一次，第一次一个人一包行李，第一次21个小时的硬座，第一次坐地铁，第一次谈恋爱，第一次分手，第一次旅行，第一次坐摩天轮，第一次吃棉花糖，第一次看电影，第一次接触电脑，第一次......\n&nbsp;&nbsp;&nbsp;&nbsp;时间是长了脚的妖怪，跑的飞快，只是好像后来我们都离开，各自生活在喧嚣未来，当时的遗憾在回忆肆虐的某些时段，重新打开，又好象我们同时都在。\n&nbsp;&nbsp;&nbsp;&nbsp;有人说，谈过恋爱，分过手，挂过科，拿过奖学金，当过学生干部的大学才是完美的，那么我想我还是比较幸运的，回顾我的大学生活，除了两件我极力想做的事情没有完成之外，经历了太多，谈过恋爱，分过手，当过学生干部，拿过奖学金，参加过各种志愿活动，也做过校园代理，被坑过，被骗过，也干过兼职，做过外包，送过外卖，发过传单，在这整个过程中，认识了不少人，见过不少事，看明白了不少的社会道理，看清了多少人的虚情假意，感谢那一路让我经历成长的人。  \n&nbsp;&nbsp;&nbsp;&nbsp;大一是我大学生活里最快乐的一年，那时的我们很单纯。只是后来，大家都变了。\n\n![这里写图片描述](http://img.blog.csdn.net/20161220223409601)\n\n-----\n\n## **2016-剑未配好，已出江湖，来一场说走就走的北漂**\n&nbsp;&nbsp;&nbsp;&nbsp;该到来的还是会到来，虽然对于工作我是做好了准备，但是还是有点措手不及。\n\n- 七月，别了流年\n&nbsp;&nbsp;&nbsp;&nbsp;那是七月，我的心情迫切的像火辣辣的太阳，拉着行李，从大学的门前离开，没有回头，虽然这里有我牵挂的人，有我念着的事，但我还是把更多的希望寄托在充满魔性的首都，因为我相信这里是梦会是开始的地方，于是在朋友的帮助下，我开启了我的北漂生活。\n\n- 广联达\n&nbsp;&nbsp;&nbsp;&nbsp;我来北京的第一家公司是广联达，建筑行业国内算是龙头老大了，虽然在互联网行业不是太牛逼，但对于一个初出茅庐的我还是够我学习和经历了，而且凑巧的是公司是我一个八几年的校友创立的，只是这和我没有半毛钱关系，在那的三个月里，我连个人影都没有见过。\n&nbsp;&nbsp;&nbsp;&nbsp;后来的后来我选择了离开，不是公司不好，不是带我的师父不优秀，不是同事不牛逼，只是我感觉那里不适合我。\n&nbsp;&nbsp;&nbsp;&nbsp;我的师父是项目组组长吧，人有点娘娘腔，别人都叫他梅梅，但是对我们特别好，他技术也十分厉害，离开的时候和师父聊天，他说在公司是P3和P4的技术双认证，是一个技术架构师，自己带着团队几个月为公司写了一个云测试平台，现在更到3.x版本了吧。我个人是十分佩服我师父的，为人低调，技术够强，还有好人缘。\n&nbsp;&nbsp;&nbsp;&nbsp;在公司的那段时候里，我主要做的是一个以课题形式展示的数据分析平台，用到的技术无非就是大学里学的那些，那个时候和另外一个同事还吹牛逼说咱也是架构师了，这仅仅是因为自己画了个水的一逼的图\n![这里写图片描述](http://img.blog.csdn.net/20161220230511202?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n&nbsp;&nbsp;&nbsp;&nbsp;哈哈，如果这幅图出自架构师之手，就是系统架构了，可是出自我们这等毛小子之手就是闹着玩了吧。就好比以国家的名义去挖墓，就是考古，以个人名义去挖墓就是盗墓了。\n\n- 昌平线，煎熬\n&nbsp;&nbsp;&nbsp;&nbsp;西二旗是中国最堵得一个地铁站了，大家都说后村厂路堵车十分钟，中国互联网经济停滞2小时。\n&nbsp;&nbsp;&nbsp;&nbsp;昌平线是北漂人的聚集地了，不是因为别的，是因为这条线路上的房租便宜 ，那个时候我就盘踞在沙河高教园旁边的东沙屯村里，每月800元的房租还是负担的起的，除了交通不便之外，一切还都可以接受，毕竟你是在北漂。\n\n----\n## **2016-别了流年，是现在的我**\n&nbsp;&nbsp;&nbsp;&nbsp;九月末我面试了现在所在的公司，离开了广联达，不是因为它不优秀，它不好，只是因为那里现在还不适合我，在我的棱角被磨平之前，我想出去闯一闯。\n&nbsp;&nbsp;&nbsp;&nbsp;可能是我所在部门的原因，我觉得特别懒散，感觉大家都是在混日子，每天改那么点bug，每天更新一点小功能，或者这就是大公司的尴尬，或者说转型之中的公司的短板吧，大家都沉浸在以前的辉煌之中，没有创造力，没有新奇的想法，没有交流的冲动，没有那种干劲。于是我选择了离开，我想先让我去经历一番我想要的工作与生活，等我累了，说不定我就会想念这种状态了。\n&nbsp;&nbsp;&nbsp;&nbsp;现在所在的是一个创业公司，像我想象中一样，大家窝在一个不大的办公司，交流与合作，为了梦想一起努力着，很开心。\n&nbsp;&nbsp;&nbsp;&nbsp;在这里我接触到了更多知识，技术的，做人的，交流的，至今我脑海中还清晰的记着那天赵总的一句话：读书要有收获，至少要涨气场。\n&nbsp;&nbsp;&nbsp;&nbsp;新的环境里我接触学习了Docker，ELK，重新学习了一些机器学习的算法知识。于是在我的CSDN博客中创建了两个技术专栏，由于刚刚接触，写的也不够深入，不过我会努力的。\n&nbsp;&nbsp;&nbsp;&nbsp;Docker江湖：http://blog.csdn.net/column/details/13159.html\n &nbsp;&nbsp;&nbsp;&nbsp;ELK从入门到放弃：http://blog.csdn.net/column/details/13079.html\n \n&nbsp;&nbsp;&nbsp;&nbsp;认认真真经历才能好好成长。\n\n---\n## **2016-我在CSDN的收获**\n - 鲍大神\n&nbsp;&nbsp;&nbsp;&nbsp;开始在CSDN上写博客是大一的时候，是一个牛逼的学长带我走上了这条\"不归路\"，谢谢<a href=\"http://blog.csdn.net/baolibin528\">鲍大神</a>这一路的指导与传授，一直以来，他都是我的榜样。我也努力赶上他，只可惜看到的永远都是背影。\n\n- 梦姐姐\n&nbsp;&nbsp;&nbsp;&nbsp;八月份的时候偶然的机会认识梦姐姐，做了博乐，后来也申请并通过了CSDN博客专家。\n\n- 结识技术爱好者\n&nbsp;&nbsp;&nbsp;&nbsp;其实相比这些更重要的是通过CSDN所认识的每一个技术爱好者，可以说CSDN是国内的程序员的社交平台了。感觉那些给我留言提问我的人，可能有些疑问还是没有帮你们解决，只是我个人能力有限，不像郭神，鸿洋大神技术功底深厚。在这个平台之上，我也认识到了自己的许多不足和技术缺点，在阅读博客的过程中，也学到了不少东西。\n\n&nbsp;&nbsp;&nbsp;&nbsp;谢谢你一路陪我成长，你若不离，我定不弃。\n\n----\n## **2016-开始commit我的github**\n&nbsp;&nbsp;&nbsp;&nbsp;有人说开源垃圾，有人说开源缩减了开发的成本和时间，不管怎样，开源是一种趋势，而且势头不会减弱，很荣幸我也投入了开源的大军，即使现在我还是一个蝼蚁。\n&nbsp;&nbsp;&nbsp;&nbsp;我的github：https://github.com/thinkgamer\n![这里写图片描述](http://img.blog.csdn.net/20161221000236962?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n----\n## **2016-杂乱无章**\n&nbsp;&nbsp;&nbsp;&nbsp;这一年，从一个初出茅庐的蝼蚁一步步成长，一个个经历，我给交了一份70分的答卷，我没有让我的父母和亲人失望，我没有让我的老师失望，我没有让我喜欢的人失望，我也没有让曾经看不起我的人失望，只是我让自己失望了。\n&nbsp;&nbsp;&nbsp;&nbsp;有些东西我没有去争取，有些机会我没有把握，有些冲动我失了控。但正是这些完美的不完美的，才让你有更大的劲头去前进。\n\n----\n## **2017-下一个自己**\n&nbsp;&nbsp;&nbsp;&nbsp;时间不会因为你的遗憾而停留，我们能做的就是把每一天都当成最后一天来过。\n&nbsp;&nbsp;&nbsp;&nbsp;2017，我要完成：\n\n -  一个安卓APP和对应的Web \n -  小说《这夏未眠》\n - 发表社区划分论文\n - 深入学习Scala和Spark\n - 掌握一个深度学习框架（eg：Caffe）\n - 跟进研究Hadoop家族的最近版本，并形成文档\n - 换一台Mackbook Pro\n - 攒够100K+\n\n&nbsp;&nbsp;&nbsp;&nbsp;感谢这一路有你，加油！\n\n----\n个人微信公众号，欢迎关注\n![个人微信公众号，欢迎关注](http://img.blog.csdn.net/20161221002809051?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)","slug":"随手记/别了青春与流年，遇见下一个自己","published":1,"updated":"2017-12-19T02:54:56.791Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjbd1r26v001jkxuwgawq5yaz","content":"<font size=\"3\">如果说岁月是年轮，我们便是推行者，如果说成长是一场华丽的蜕变，我们便是领舞者。一路走来，太多不易，告别青春的年少轻狂，我们成了岁月里被磨平的棱角，静静的守在属于自己的一亩三分地。</font>\n\n\n<a id=\"more\"></a>\n<h2 id=\"2016-时间是长了脚的妖怪，跑的飞快\"><a href=\"#2016-时间是长了脚的妖怪，跑的飞快\" class=\"headerlink\" title=\"2016-时间是长了脚的妖怪，跑的飞快\"></a><strong>2016-时间是长了脚的妖怪，跑的飞快</strong></h2><p>&nbsp;&nbsp;&nbsp;&nbsp;四年时光，匆匆而过，沈阳占据了我23岁之前的太多第一次，第一次一个人一包行李，第一次21个小时的硬座，第一次坐地铁，第一次谈恋爱，第一次分手，第一次旅行，第一次坐摩天轮，第一次吃棉花糖，第一次看电影，第一次接触电脑，第一次……<br>&nbsp;&nbsp;&nbsp;&nbsp;时间是长了脚的妖怪，跑的飞快，只是好像后来我们都离开，各自生活在喧嚣未来，当时的遗憾在回忆肆虐的某些时段，重新打开，又好象我们同时都在。<br>&nbsp;&nbsp;&nbsp;&nbsp;有人说，谈过恋爱，分过手，挂过科，拿过奖学金，当过学生干部的大学才是完美的，那么我想我还是比较幸运的，回顾我的大学生活，除了两件我极力想做的事情没有完成之外，经历了太多，谈过恋爱，分过手，当过学生干部，拿过奖学金，参加过各种志愿活动，也做过校园代理，被坑过，被骗过，也干过兼职，做过外包，送过外卖，发过传单，在这整个过程中，认识了不少人，见过不少事，看明白了不少的社会道理，看清了多少人的虚情假意，感谢那一路让我经历成长的人。<br>&nbsp;&nbsp;&nbsp;&nbsp;大一是我大学生活里最快乐的一年，那时的我们很单纯。只是后来，大家都变了。</p>\n<p><img src=\"http://img.blog.csdn.net/20161220223409601\" alt=\"这里写图片描述\"></p>\n<hr>\n<h2 id=\"2016-剑未配好，已出江湖，来一场说走就走的北漂\"><a href=\"#2016-剑未配好，已出江湖，来一场说走就走的北漂\" class=\"headerlink\" title=\"2016-剑未配好，已出江湖，来一场说走就走的北漂\"></a><strong>2016-剑未配好，已出江湖，来一场说走就走的北漂</strong></h2><p>&nbsp;&nbsp;&nbsp;&nbsp;该到来的还是会到来，虽然对于工作我是做好了准备，但是还是有点措手不及。</p>\n<ul>\n<li><p>七月，别了流年<br>&nbsp;&nbsp;&nbsp;&nbsp;那是七月，我的心情迫切的像火辣辣的太阳，拉着行李，从大学的门前离开，没有回头，虽然这里有我牵挂的人，有我念着的事，但我还是把更多的希望寄托在充满魔性的首都，因为我相信这里是梦会是开始的地方，于是在朋友的帮助下，我开启了我的北漂生活。</p>\n</li>\n<li><p>广联达<br>&nbsp;&nbsp;&nbsp;&nbsp;我来北京的第一家公司是广联达，建筑行业国内算是龙头老大了，虽然在互联网行业不是太牛逼，但对于一个初出茅庐的我还是够我学习和经历了，而且凑巧的是公司是我一个八几年的校友创立的，只是这和我没有半毛钱关系，在那的三个月里，我连个人影都没有见过。<br>&nbsp;&nbsp;&nbsp;&nbsp;后来的后来我选择了离开，不是公司不好，不是带我的师父不优秀，不是同事不牛逼，只是我感觉那里不适合我。<br>&nbsp;&nbsp;&nbsp;&nbsp;我的师父是项目组组长吧，人有点娘娘腔，别人都叫他梅梅，但是对我们特别好，他技术也十分厉害，离开的时候和师父聊天，他说在公司是P3和P4的技术双认证，是一个技术架构师，自己带着团队几个月为公司写了一个云测试平台，现在更到3.x版本了吧。我个人是十分佩服我师父的，为人低调，技术够强，还有好人缘。<br>&nbsp;&nbsp;&nbsp;&nbsp;在公司的那段时候里，我主要做的是一个以课题形式展示的数据分析平台，用到的技术无非就是大学里学的那些，那个时候和另外一个同事还吹牛逼说咱也是架构师了，这仅仅是因为自己画了个水的一逼的图<br><img src=\"http://img.blog.csdn.net/20161220230511202?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"这里写图片描述\"><br>&nbsp;&nbsp;&nbsp;&nbsp;哈哈，如果这幅图出自架构师之手，就是系统架构了，可是出自我们这等毛小子之手就是闹着玩了吧。就好比以国家的名义去挖墓，就是考古，以个人名义去挖墓就是盗墓了。</p>\n</li>\n<li><p>昌平线，煎熬<br>&nbsp;&nbsp;&nbsp;&nbsp;西二旗是中国最堵得一个地铁站了，大家都说后村厂路堵车十分钟，中国互联网经济停滞2小时。<br>&nbsp;&nbsp;&nbsp;&nbsp;昌平线是北漂人的聚集地了，不是因为别的，是因为这条线路上的房租便宜 ，那个时候我就盘踞在沙河高教园旁边的东沙屯村里，每月800元的房租还是负担的起的，除了交通不便之外，一切还都可以接受，毕竟你是在北漂。</p>\n</li>\n</ul>\n<hr>\n<h2 id=\"2016-别了流年，是现在的我\"><a href=\"#2016-别了流年，是现在的我\" class=\"headerlink\" title=\"2016-别了流年，是现在的我\"></a><strong>2016-别了流年，是现在的我</strong></h2><p>&nbsp;&nbsp;&nbsp;&nbsp;九月末我面试了现在所在的公司，离开了广联达，不是因为它不优秀，它不好，只是因为那里现在还不适合我，在我的棱角被磨平之前，我想出去闯一闯。<br>&nbsp;&nbsp;&nbsp;&nbsp;可能是我所在部门的原因，我觉得特别懒散，感觉大家都是在混日子，每天改那么点bug，每天更新一点小功能，或者这就是大公司的尴尬，或者说转型之中的公司的短板吧，大家都沉浸在以前的辉煌之中，没有创造力，没有新奇的想法，没有交流的冲动，没有那种干劲。于是我选择了离开，我想先让我去经历一番我想要的工作与生活，等我累了，说不定我就会想念这种状态了。<br>&nbsp;&nbsp;&nbsp;&nbsp;现在所在的是一个创业公司，像我想象中一样，大家窝在一个不大的办公司，交流与合作，为了梦想一起努力着，很开心。<br>&nbsp;&nbsp;&nbsp;&nbsp;在这里我接触到了更多知识，技术的，做人的，交流的，至今我脑海中还清晰的记着那天赵总的一句话：读书要有收获，至少要涨气场。<br>&nbsp;&nbsp;&nbsp;&nbsp;新的环境里我接触学习了Docker，ELK，重新学习了一些机器学习的算法知识。于是在我的CSDN博客中创建了两个技术专栏，由于刚刚接触，写的也不够深入，不过我会努力的。<br>&nbsp;&nbsp;&nbsp;&nbsp;Docker江湖：<a href=\"http://blog.csdn.net/column/details/13159.html\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/column/details/13159.html</a><br> &nbsp;&nbsp;&nbsp;&nbsp;ELK从入门到放弃：<a href=\"http://blog.csdn.net/column/details/13079.html\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/column/details/13079.html</a></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;认认真真经历才能好好成长。</p>\n<hr>\n<h2 id=\"2016-我在CSDN的收获\"><a href=\"#2016-我在CSDN的收获\" class=\"headerlink\" title=\"2016-我在CSDN的收获\"></a><strong>2016-我在CSDN的收获</strong></h2><ul>\n<li>鲍大神<br>&nbsp;&nbsp;&nbsp;&nbsp;开始在CSDN上写博客是大一的时候，是一个牛逼的学长带我走上了这条”不归路”，谢谢<a href=\"http://blog.csdn.net/baolibin528\" target=\"_blank\" rel=\"external\">鲍大神</a>这一路的指导与传授，一直以来，他都是我的榜样。我也努力赶上他，只可惜看到的永远都是背影。</li>\n</ul>\n<ul>\n<li><p>梦姐姐<br>&nbsp;&nbsp;&nbsp;&nbsp;八月份的时候偶然的机会认识梦姐姐，做了博乐，后来也申请并通过了CSDN博客专家。</p>\n</li>\n<li><p>结识技术爱好者<br>&nbsp;&nbsp;&nbsp;&nbsp;其实相比这些更重要的是通过CSDN所认识的每一个技术爱好者，可以说CSDN是国内的程序员的社交平台了。感觉那些给我留言提问我的人，可能有些疑问还是没有帮你们解决，只是我个人能力有限，不像郭神，鸿洋大神技术功底深厚。在这个平台之上，我也认识到了自己的许多不足和技术缺点，在阅读博客的过程中，也学到了不少东西。</p>\n</li>\n</ul>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;谢谢你一路陪我成长，你若不离，我定不弃。</p>\n<hr>\n<h2 id=\"2016-开始commit我的github\"><a href=\"#2016-开始commit我的github\" class=\"headerlink\" title=\"2016-开始commit我的github\"></a><strong>2016-开始commit我的github</strong></h2><p>&nbsp;&nbsp;&nbsp;&nbsp;有人说开源垃圾，有人说开源缩减了开发的成本和时间，不管怎样，开源是一种趋势，而且势头不会减弱，很荣幸我也投入了开源的大军，即使现在我还是一个蝼蚁。<br>&nbsp;&nbsp;&nbsp;&nbsp;我的github：<a href=\"https://github.com/thinkgamer\" target=\"_blank\" rel=\"external\">https://github.com/thinkgamer</a><br><img src=\"http://img.blog.csdn.net/20161221000236962?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"这里写图片描述\"></p>\n<hr>\n<h2 id=\"2016-杂乱无章\"><a href=\"#2016-杂乱无章\" class=\"headerlink\" title=\"2016-杂乱无章\"></a><strong>2016-杂乱无章</strong></h2><p>&nbsp;&nbsp;&nbsp;&nbsp;这一年，从一个初出茅庐的蝼蚁一步步成长，一个个经历，我给交了一份70分的答卷，我没有让我的父母和亲人失望，我没有让我的老师失望，我没有让我喜欢的人失望，我也没有让曾经看不起我的人失望，只是我让自己失望了。<br>&nbsp;&nbsp;&nbsp;&nbsp;有些东西我没有去争取，有些机会我没有把握，有些冲动我失了控。但正是这些完美的不完美的，才让你有更大的劲头去前进。</p>\n<hr>\n<h2 id=\"2017-下一个自己\"><a href=\"#2017-下一个自己\" class=\"headerlink\" title=\"2017-下一个自己\"></a><strong>2017-下一个自己</strong></h2><p>&nbsp;&nbsp;&nbsp;&nbsp;时间不会因为你的遗憾而停留，我们能做的就是把每一天都当成最后一天来过。<br>&nbsp;&nbsp;&nbsp;&nbsp;2017，我要完成：</p>\n<ul>\n<li>一个安卓APP和对应的Web </li>\n<li>小说《这夏未眠》</li>\n<li>发表社区划分论文</li>\n<li>深入学习Scala和Spark</li>\n<li>掌握一个深度学习框架（eg：Caffe）</li>\n<li>跟进研究Hadoop家族的最近版本，并形成文档</li>\n<li>换一台Mackbook Pro</li>\n<li>攒够100K+</li>\n</ul>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;感谢这一路有你，加油！</p>\n<hr>\n<p>个人微信公众号，欢迎关注<br><img src=\"http://img.blog.csdn.net/20161221002809051?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"个人微信公众号，欢迎关注\"></p>\n","site":{"data":{}},"excerpt":"<font size=\"3\">如果说岁月是年轮，我们便是推行者，如果说成长是一场华丽的蜕变，我们便是领舞者。一路走来，太多不易，告别青春的年少轻狂，我们成了岁月里被磨平的棱角，静静的守在属于自己的一亩三分地。</font>","more":"<h2 id=\"2016-时间是长了脚的妖怪，跑的飞快\"><a href=\"#2016-时间是长了脚的妖怪，跑的飞快\" class=\"headerlink\" title=\"2016-时间是长了脚的妖怪，跑的飞快\"></a><strong>2016-时间是长了脚的妖怪，跑的飞快</strong></h2><p>&nbsp;&nbsp;&nbsp;&nbsp;四年时光，匆匆而过，沈阳占据了我23岁之前的太多第一次，第一次一个人一包行李，第一次21个小时的硬座，第一次坐地铁，第一次谈恋爱，第一次分手，第一次旅行，第一次坐摩天轮，第一次吃棉花糖，第一次看电影，第一次接触电脑，第一次……<br>&nbsp;&nbsp;&nbsp;&nbsp;时间是长了脚的妖怪，跑的飞快，只是好像后来我们都离开，各自生活在喧嚣未来，当时的遗憾在回忆肆虐的某些时段，重新打开，又好象我们同时都在。<br>&nbsp;&nbsp;&nbsp;&nbsp;有人说，谈过恋爱，分过手，挂过科，拿过奖学金，当过学生干部的大学才是完美的，那么我想我还是比较幸运的，回顾我的大学生活，除了两件我极力想做的事情没有完成之外，经历了太多，谈过恋爱，分过手，当过学生干部，拿过奖学金，参加过各种志愿活动，也做过校园代理，被坑过，被骗过，也干过兼职，做过外包，送过外卖，发过传单，在这整个过程中，认识了不少人，见过不少事，看明白了不少的社会道理，看清了多少人的虚情假意，感谢那一路让我经历成长的人。<br>&nbsp;&nbsp;&nbsp;&nbsp;大一是我大学生活里最快乐的一年，那时的我们很单纯。只是后来，大家都变了。</p>\n<p><img src=\"http://img.blog.csdn.net/20161220223409601\" alt=\"这里写图片描述\"></p>\n<hr>\n<h2 id=\"2016-剑未配好，已出江湖，来一场说走就走的北漂\"><a href=\"#2016-剑未配好，已出江湖，来一场说走就走的北漂\" class=\"headerlink\" title=\"2016-剑未配好，已出江湖，来一场说走就走的北漂\"></a><strong>2016-剑未配好，已出江湖，来一场说走就走的北漂</strong></h2><p>&nbsp;&nbsp;&nbsp;&nbsp;该到来的还是会到来，虽然对于工作我是做好了准备，但是还是有点措手不及。</p>\n<ul>\n<li><p>七月，别了流年<br>&nbsp;&nbsp;&nbsp;&nbsp;那是七月，我的心情迫切的像火辣辣的太阳，拉着行李，从大学的门前离开，没有回头，虽然这里有我牵挂的人，有我念着的事，但我还是把更多的希望寄托在充满魔性的首都，因为我相信这里是梦会是开始的地方，于是在朋友的帮助下，我开启了我的北漂生活。</p>\n</li>\n<li><p>广联达<br>&nbsp;&nbsp;&nbsp;&nbsp;我来北京的第一家公司是广联达，建筑行业国内算是龙头老大了，虽然在互联网行业不是太牛逼，但对于一个初出茅庐的我还是够我学习和经历了，而且凑巧的是公司是我一个八几年的校友创立的，只是这和我没有半毛钱关系，在那的三个月里，我连个人影都没有见过。<br>&nbsp;&nbsp;&nbsp;&nbsp;后来的后来我选择了离开，不是公司不好，不是带我的师父不优秀，不是同事不牛逼，只是我感觉那里不适合我。<br>&nbsp;&nbsp;&nbsp;&nbsp;我的师父是项目组组长吧，人有点娘娘腔，别人都叫他梅梅，但是对我们特别好，他技术也十分厉害，离开的时候和师父聊天，他说在公司是P3和P4的技术双认证，是一个技术架构师，自己带着团队几个月为公司写了一个云测试平台，现在更到3.x版本了吧。我个人是十分佩服我师父的，为人低调，技术够强，还有好人缘。<br>&nbsp;&nbsp;&nbsp;&nbsp;在公司的那段时候里，我主要做的是一个以课题形式展示的数据分析平台，用到的技术无非就是大学里学的那些，那个时候和另外一个同事还吹牛逼说咱也是架构师了，这仅仅是因为自己画了个水的一逼的图<br><img src=\"http://img.blog.csdn.net/20161220230511202?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"这里写图片描述\"><br>&nbsp;&nbsp;&nbsp;&nbsp;哈哈，如果这幅图出自架构师之手，就是系统架构了，可是出自我们这等毛小子之手就是闹着玩了吧。就好比以国家的名义去挖墓，就是考古，以个人名义去挖墓就是盗墓了。</p>\n</li>\n<li><p>昌平线，煎熬<br>&nbsp;&nbsp;&nbsp;&nbsp;西二旗是中国最堵得一个地铁站了，大家都说后村厂路堵车十分钟，中国互联网经济停滞2小时。<br>&nbsp;&nbsp;&nbsp;&nbsp;昌平线是北漂人的聚集地了，不是因为别的，是因为这条线路上的房租便宜 ，那个时候我就盘踞在沙河高教园旁边的东沙屯村里，每月800元的房租还是负担的起的，除了交通不便之外，一切还都可以接受，毕竟你是在北漂。</p>\n</li>\n</ul>\n<hr>\n<h2 id=\"2016-别了流年，是现在的我\"><a href=\"#2016-别了流年，是现在的我\" class=\"headerlink\" title=\"2016-别了流年，是现在的我\"></a><strong>2016-别了流年，是现在的我</strong></h2><p>&nbsp;&nbsp;&nbsp;&nbsp;九月末我面试了现在所在的公司，离开了广联达，不是因为它不优秀，它不好，只是因为那里现在还不适合我，在我的棱角被磨平之前，我想出去闯一闯。<br>&nbsp;&nbsp;&nbsp;&nbsp;可能是我所在部门的原因，我觉得特别懒散，感觉大家都是在混日子，每天改那么点bug，每天更新一点小功能，或者这就是大公司的尴尬，或者说转型之中的公司的短板吧，大家都沉浸在以前的辉煌之中，没有创造力，没有新奇的想法，没有交流的冲动，没有那种干劲。于是我选择了离开，我想先让我去经历一番我想要的工作与生活，等我累了，说不定我就会想念这种状态了。<br>&nbsp;&nbsp;&nbsp;&nbsp;现在所在的是一个创业公司，像我想象中一样，大家窝在一个不大的办公司，交流与合作，为了梦想一起努力着，很开心。<br>&nbsp;&nbsp;&nbsp;&nbsp;在这里我接触到了更多知识，技术的，做人的，交流的，至今我脑海中还清晰的记着那天赵总的一句话：读书要有收获，至少要涨气场。<br>&nbsp;&nbsp;&nbsp;&nbsp;新的环境里我接触学习了Docker，ELK，重新学习了一些机器学习的算法知识。于是在我的CSDN博客中创建了两个技术专栏，由于刚刚接触，写的也不够深入，不过我会努力的。<br>&nbsp;&nbsp;&nbsp;&nbsp;Docker江湖：<a href=\"http://blog.csdn.net/column/details/13159.html\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/column/details/13159.html</a><br> &nbsp;&nbsp;&nbsp;&nbsp;ELK从入门到放弃：<a href=\"http://blog.csdn.net/column/details/13079.html\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/column/details/13079.html</a></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;认认真真经历才能好好成长。</p>\n<hr>\n<h2 id=\"2016-我在CSDN的收获\"><a href=\"#2016-我在CSDN的收获\" class=\"headerlink\" title=\"2016-我在CSDN的收获\"></a><strong>2016-我在CSDN的收获</strong></h2><ul>\n<li>鲍大神<br>&nbsp;&nbsp;&nbsp;&nbsp;开始在CSDN上写博客是大一的时候，是一个牛逼的学长带我走上了这条”不归路”，谢谢<a href=\"http://blog.csdn.net/baolibin528\" target=\"_blank\" rel=\"external\">鲍大神</a>这一路的指导与传授，一直以来，他都是我的榜样。我也努力赶上他，只可惜看到的永远都是背影。</li>\n</ul>\n<ul>\n<li><p>梦姐姐<br>&nbsp;&nbsp;&nbsp;&nbsp;八月份的时候偶然的机会认识梦姐姐，做了博乐，后来也申请并通过了CSDN博客专家。</p>\n</li>\n<li><p>结识技术爱好者<br>&nbsp;&nbsp;&nbsp;&nbsp;其实相比这些更重要的是通过CSDN所认识的每一个技术爱好者，可以说CSDN是国内的程序员的社交平台了。感觉那些给我留言提问我的人，可能有些疑问还是没有帮你们解决，只是我个人能力有限，不像郭神，鸿洋大神技术功底深厚。在这个平台之上，我也认识到了自己的许多不足和技术缺点，在阅读博客的过程中，也学到了不少东西。</p>\n</li>\n</ul>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;谢谢你一路陪我成长，你若不离，我定不弃。</p>\n<hr>\n<h2 id=\"2016-开始commit我的github\"><a href=\"#2016-开始commit我的github\" class=\"headerlink\" title=\"2016-开始commit我的github\"></a><strong>2016-开始commit我的github</strong></h2><p>&nbsp;&nbsp;&nbsp;&nbsp;有人说开源垃圾，有人说开源缩减了开发的成本和时间，不管怎样，开源是一种趋势，而且势头不会减弱，很荣幸我也投入了开源的大军，即使现在我还是一个蝼蚁。<br>&nbsp;&nbsp;&nbsp;&nbsp;我的github：<a href=\"https://github.com/thinkgamer\" target=\"_blank\" rel=\"external\">https://github.com/thinkgamer</a><br><img src=\"http://img.blog.csdn.net/20161221000236962?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"这里写图片描述\"></p>\n<hr>\n<h2 id=\"2016-杂乱无章\"><a href=\"#2016-杂乱无章\" class=\"headerlink\" title=\"2016-杂乱无章\"></a><strong>2016-杂乱无章</strong></h2><p>&nbsp;&nbsp;&nbsp;&nbsp;这一年，从一个初出茅庐的蝼蚁一步步成长，一个个经历，我给交了一份70分的答卷，我没有让我的父母和亲人失望，我没有让我的老师失望，我没有让我喜欢的人失望，我也没有让曾经看不起我的人失望，只是我让自己失望了。<br>&nbsp;&nbsp;&nbsp;&nbsp;有些东西我没有去争取，有些机会我没有把握，有些冲动我失了控。但正是这些完美的不完美的，才让你有更大的劲头去前进。</p>\n<hr>\n<h2 id=\"2017-下一个自己\"><a href=\"#2017-下一个自己\" class=\"headerlink\" title=\"2017-下一个自己\"></a><strong>2017-下一个自己</strong></h2><p>&nbsp;&nbsp;&nbsp;&nbsp;时间不会因为你的遗憾而停留，我们能做的就是把每一天都当成最后一天来过。<br>&nbsp;&nbsp;&nbsp;&nbsp;2017，我要完成：</p>\n<ul>\n<li>一个安卓APP和对应的Web </li>\n<li>小说《这夏未眠》</li>\n<li>发表社区划分论文</li>\n<li>深入学习Scala和Spark</li>\n<li>掌握一个深度学习框架（eg：Caffe）</li>\n<li>跟进研究Hadoop家族的最近版本，并形成文档</li>\n<li>换一台Mackbook Pro</li>\n<li>攒够100K+</li>\n</ul>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;感谢这一路有你，加油！</p>\n<hr>\n<p>个人微信公众号，欢迎关注<br><img src=\"http://img.blog.csdn.net/20161221002809051?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\" alt=\"个人微信公众号，欢迎关注\"></p>"}],"PostAsset":[],"PostCategory":[{"post_id":"cjbd1r25k0005kxuw9e1ibwab","category_id":"cjbd1r25r0007kxuwyjk4r7hv","_id":"cjbd1r263000hkxuwd27jsody"},{"post_id":"cjbd1r25w000bkxuwizlacbbw","category_id":"cjbd1r25r0007kxuwyjk4r7hv","_id":"cjbd1r265000kkxuwlz8zmaj0"},{"post_id":"cjbd1r25z000ekxuw8u2wpn78","category_id":"cjbd1r25r0007kxuwyjk4r7hv","_id":"cjbd1r268000nkxuw9sfamsww"},{"post_id":"cjbd1r25p0006kxuw058ndgn1","category_id":"cjbd1r25r0007kxuwyjk4r7hv","_id":"cjbd1r26b000skxuw2m9h44ro"},{"post_id":"cjbd1r261000fkxuwnyz2tgey","category_id":"cjbd1r25r0007kxuwyjk4r7hv","_id":"cjbd1r26d000ukxuwa4qpi8s2"},{"post_id":"cjbd1r265000jkxuwh92ok2ta","category_id":"cjbd1r25r0007kxuwyjk4r7hv","_id":"cjbd1r26h000xkxuwg4ksnh72"},{"post_id":"cjbd1r25t0009kxuwl8pnfjew","category_id":"cjbd1r263000gkxuwj2mhft7c","_id":"cjbd1r26j000zkxuwd9mntkv7"},{"post_id":"cjbd1r267000mkxuwc1j2j1ed","category_id":"cjbd1r25r0007kxuwyjk4r7hv","_id":"cjbd1r26l0012kxuwvtqu4lq8"},{"post_id":"cjbd1r269000rkxuw12stlgj9","category_id":"cjbd1r25r0007kxuwyjk4r7hv","_id":"cjbd1r26n0015kxuw9gupx7at"},{"post_id":"cjbd1r25v000akxuwv2ds2h7l","category_id":"cjbd1r263000gkxuwj2mhft7c","_id":"cjbd1r26q0019kxuw6vpxqedt"},{"post_id":"cjbd1r26c000tkxuwbq4jhgt4","category_id":"cjbd1r25r0007kxuwyjk4r7hv","_id":"cjbd1r26s001ckxuwa9opbwzj"},{"post_id":"cjbd1r26f000wkxuwz489l5lq","category_id":"cjbd1r25r0007kxuwyjk4r7hv","_id":"cjbd1r26t001gkxuwt51hw4o9"},{"post_id":"cjbd1r26h000ykxuwqkz87wmw","category_id":"cjbd1r25r0007kxuwyjk4r7hv","_id":"cjbd1r26w001kkxuww2lmn9vq"},{"post_id":"cjbd1r26k0011kxuw6z50bx3a","category_id":"cjbd1r25r0007kxuwyjk4r7hv","_id":"cjbd1r26x001lkxuw8jy5jm4y"},{"post_id":"cjbd1r26m0014kxuwm2uydy1d","category_id":"cjbd1r25r0007kxuwyjk4r7hv","_id":"cjbd1r26z001pkxuwib1p5buy"},{"post_id":"cjbd1r26o0018kxuw488o6vxs","category_id":"cjbd1r25r0007kxuwyjk4r7hv","_id":"cjbd1r271001qkxuwdyno6hch"},{"post_id":"cjbd1r26r001bkxuwkcn3b7fq","category_id":"cjbd1r26v001ikxuwrsxf5aaq","_id":"cjbd1r272001ukxuwvkswlc92"},{"post_id":"cjbd1r26s001fkxuwvfmohrrx","category_id":"cjbd1r26v001ikxuwrsxf5aaq","_id":"cjbd1r273001xkxuw1h49xrsx"},{"post_id":"cjbd1r26v001jkxuwgawq5yaz","category_id":"cjbd1r26v001ikxuwrsxf5aaq","_id":"cjbd1r274001zkxuwb1sy6qyg"}],"PostTag":[{"post_id":"cjbd1r25k0005kxuw9e1ibwab","tag_id":"cjbd1r25t0008kxuwkmysocmf","_id":"cjbd1r266000lkxuw2ai72tev"},{"post_id":"cjbd1r25k0005kxuw9e1ibwab","tag_id":"cjbd1r25x000dkxuw4si21ext","_id":"cjbd1r268000okxuw4cgdbx9k"},{"post_id":"cjbd1r25p0006kxuw058ndgn1","tag_id":"cjbd1r25t0008kxuwkmysocmf","_id":"cjbd1r26l0013kxuwd6bj8fiz"},{"post_id":"cjbd1r25p0006kxuw058ndgn1","tag_id":"cjbd1r269000qkxuw8f70l9wq","_id":"cjbd1r26n0016kxuw4cpad738"},{"post_id":"cjbd1r25p0006kxuw058ndgn1","tag_id":"cjbd1r25x000dkxuw4si21ext","_id":"cjbd1r26q001akxuwrrjmakph"},{"post_id":"cjbd1r25t0009kxuwl8pnfjew","tag_id":"cjbd1r26j0010kxuw41s2p0pq","_id":"cjbd1r26s001dkxuwskyzt14f"},{"post_id":"cjbd1r25v000akxuwv2ds2h7l","tag_id":"cjbd1r26j0010kxuw41s2p0pq","_id":"cjbd1r26u001hkxuw7g4ftehj"},{"post_id":"cjbd1r25w000bkxuwizlacbbw","tag_id":"cjbd1r26s001ekxuwnacgpkok","_id":"cjbd1r26y001nkxuw1y1l26yu"},{"post_id":"cjbd1r25z000ekxuw8u2wpn78","tag_id":"cjbd1r26s001ekxuwnacgpkok","_id":"cjbd1r272001skxuwwr1djxip"},{"post_id":"cjbd1r261000fkxuwnyz2tgey","tag_id":"cjbd1r26s001ekxuwnacgpkok","_id":"cjbd1r273001wkxuwr9bb0sy0"},{"post_id":"cjbd1r265000jkxuwh92ok2ta","tag_id":"cjbd1r272001vkxuwdgy9h6nn","_id":"cjbd1r2740020kxuwt8j3qt7v"},{"post_id":"cjbd1r267000mkxuwc1j2j1ed","tag_id":"cjbd1r26s001ekxuwnacgpkok","_id":"cjbd1r2750022kxuw2o0s1dbd"},{"post_id":"cjbd1r269000rkxuw12stlgj9","tag_id":"cjbd1r272001vkxuwdgy9h6nn","_id":"cjbd1r2750024kxuw1cbdeko5"},{"post_id":"cjbd1r26c000tkxuwbq4jhgt4","tag_id":"cjbd1r2750023kxuwlsw7kktp","_id":"cjbd1r2760027kxuwl8mnphsy"},{"post_id":"cjbd1r26c000tkxuwbq4jhgt4","tag_id":"cjbd1r2750025kxuwkiegcc2z","_id":"cjbd1r2760028kxuwr0pej66l"},{"post_id":"cjbd1r26f000wkxuwz489l5lq","tag_id":"cjbd1r2750023kxuwlsw7kktp","_id":"cjbd1r277002bkxuwz31mem10"},{"post_id":"cjbd1r26f000wkxuwz489l5lq","tag_id":"cjbd1r2760029kxuwwguzx0wn","_id":"cjbd1r277002ckxuwlicgcyo7"},{"post_id":"cjbd1r26h000ykxuwqkz87wmw","tag_id":"cjbd1r2750023kxuwlsw7kktp","_id":"cjbd1r277002ekxuwwkwrnmig"},{"post_id":"cjbd1r26k0011kxuw6z50bx3a","tag_id":"cjbd1r277002dkxuwl1m0khc5","_id":"cjbd1r278002hkxuw7h9j238j"},{"post_id":"cjbd1r26k0011kxuw6z50bx3a","tag_id":"cjbd1r2750025kxuwkiegcc2z","_id":"cjbd1r278002ikxuw98oyiu03"},{"post_id":"cjbd1r26m0014kxuwm2uydy1d","tag_id":"cjbd1r278002gkxuwa7620y3e","_id":"cjbd1r278002kkxuwngrf3os4"},{"post_id":"cjbd1r26o0018kxuw488o6vxs","tag_id":"cjbd1r278002gkxuwa7620y3e","_id":"cjbd1r279002mkxuwo7n4wy6b"},{"post_id":"cjbd1r26r001bkxuwkcn3b7fq","tag_id":"cjbd1r278002lkxuwqs3sn8ec","_id":"cjbd1r279002okxuw2bi90zoy"},{"post_id":"cjbd1r26s001fkxuwvfmohrrx","tag_id":"cjbd1r279002nkxuwpji9xs3b","_id":"cjbd1r27a002qkxuwovq7apaz"},{"post_id":"cjbd1r26v001jkxuwgawq5yaz","tag_id":"cjbd1r279002nkxuwpji9xs3b","_id":"cjbd1r27a002rkxuw5sk5yz14"}],"Tag":[{"name":"ELK","_id":"cjbd1r25t0008kxuwkmysocmf"},{"name":"ES","_id":"cjbd1r25x000dkxuw4si21ext"},{"name":"异常检测","_id":"cjbd1r269000qkxuw8f70l9wq"},{"name":"夏未眠","_id":"cjbd1r26j0010kxuw41s2p0pq"},{"name":"数据结构","_id":"cjbd1r26s001ekxuwnacgpkok"},{"name":"距离计算","_id":"cjbd1r272001vkxuwdgy9h6nn"},{"name":"回归分析","_id":"cjbd1r2750023kxuwlsw7kktp"},{"name":"sklearn","_id":"cjbd1r2750025kxuwkiegcc2z"},{"name":"正态分布","_id":"cjbd1r2760029kxuwwguzx0wn"},{"name":"数据归一化","_id":"cjbd1r277002dkxuwl1m0khc5"},{"name":"梯度下降","_id":"cjbd1r278002gkxuwa7620y3e"},{"name":"hexo","_id":"cjbd1r278002lkxuwqs3sn8ec"},{"name":"随手记","_id":"cjbd1r279002nkxuwpji9xs3b"}]}}